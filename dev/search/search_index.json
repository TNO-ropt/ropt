{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"<code>ropt</code>: A Python module for robust optimization","text":""},{"location":"#overview","title":"Overview","text":"<p><code>ropt</code> is a module designed for implementing and executing robust optimization workflows. In classical optimization problems, a deterministic function is optimized. However, in robust optimization, the function is expected to exhibit a stochastic nature and is represented by an ensemble of functions (realizations) for different values of some (possibly unknown) random parameters. The optimal solution is then determined by optimizing the value of a statistic, such as the mean, over the ensemble.</p> <p><code>ropt</code> can be employed to construct optimization workflows directly in Python or as a building block in optimization applications. At a minimum, the user needs to provide additional code to calculate the values for each function realization in the ensemble. This can range from simply calling a Python function that returns the objective values to initiating a long-running simulation on an HPC cluster and reading the results. Furthermore, <code>ropt</code> exposes all intermediate results of the optimization, such as objective and gradient values, but functionality to report or store any of these values must be added by the user. Optional functionality to assist with this is included with <code>ropt</code>.</p> <p><code>ropt</code> provides several features for efficiently solving complex robust optimization problems:</p> <ul> <li>Robust optimization over an ensemble of models, i.e., optimizing the average   of a set of objective functions. Alternative objectives can be implemented   using plugins, for instance, to implement risk-aware optimization, such as   Conditional Value at Risk (CVaR) or standard-deviation-based functions.</li> <li>Support for black-box optimization of arbitrary functions.</li> <li>Support for running complex optimization workflows, such as multiple runs with   different optimization settings or even different optimization methods.</li> <li>Support for nested optimization, allowing sub-sets of the variables to be   optimized by optimization workflows that run as part of the black-box function   to be optimized.</li> <li>An interface for running various continuous and discrete optimization methods.   By default, optimizers from the   <code>scipy.optimize</code>   package are included, but additional optimizers can be added via a plugin   mechanism. The most common options of these optimizers can be configured in a   uniform manner, although algorithm- or package-specific options can still be   passed.</li> <li>Efficient estimation of gradients using a Stochastic Simplex Approximate   Gradient (StoSAG) approach. Additional samplers for generating perturbed   values for gradient estimation can be added via a plugin mechanism.</li> <li>Support for linear and non-linear constraints, if supported by the chosen   optimizer.</li> <li>Flexible configuration of the optimization process using   <code>pydantic</code>.</li> <li>Support for tracking and processing optimization results generated during the   optimization process.</li> <li>Optional support for exporting results as   <code>pandas</code> data frames.</li> </ul>"},{"location":"#related-packages","title":"Related packages","text":""},{"location":"#plugins","title":"Plugins","text":"<p>Additional backend optimizers can be installed separately and used via the plugin system:</p> <ul> <li>The <code>ropt-dakota</code> plugin provides   access to algorithms from the Dakota package.</li> <li>The <code>ropt-nomad</code> plugin implements   the MADS algorithm based on the   NOMAD package.</li> <li>The <code>ropt-pymoo</code> makes the   algorithms from the <code>pymoo</code> package available to <code>ropt</code>.</li> </ul>"},{"location":"#applications","title":"Applications","text":"<p>The <code>ropt</code> package is used  by the Everest decision-making tool as its core optimization engine.</p>"},{"location":"reference/basic_optimizer/","title":"Basic Optimizer","text":""},{"location":"reference/basic_optimizer/#ropt.workflow._basic_optimizer","title":"ropt.workflow._basic_optimizer","text":"<p>This module defines a basic optimization object.</p>"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer","title":"ropt.workflow.BasicOptimizer","text":"<p>A class for executing single optimization runs.</p> <p>The <code>BasicOptimizer</code> is designed to simplify the process of setting up and executing optimization workflows that consist primarily of a single optimization run.</p> <p>This class provides a user-friendly interface for common optimization operations, including:</p> <ul> <li>Initiating a Single Optimization:  Easily start an optimization   process with a provided configuration and evaluator.</li> <li>Observing Optimization Events: Register observer functions to monitor   and react to various events that occur during the optimization, such as   the start of an evaluation or the availability of new results.</li> <li>Abort Conditions: Define a callback function that can be used to check   for abort conditions during the optimization.</li> <li>Result Reporting: Define a callback function that will be called   whenever new results become available.</li> <li>Accessing Results: After the optimization is complete, the optimal   results, corresponding variables, and the optimization's exit code are   readily accessible.</li> <li>Customizable ComputeSteps, Handlers, and Evaluators: While designed   for single runs, it allows for the addition of custom compute steps and   event handlers for more   complex scenarios.</li> </ul> <p>By encapsulating the core elements of an optimization run, the <code>BasicOptimizer</code> reduces the boilerplate code required for simple optimization tasks, allowing users to focus on defining the optimization problem and analyzing the results.</p> <p>The following example demonstrates how to find the optimum of the Rosenbrock function using a <code>BasicOptimizer</code> object, combining it with a <code>tracker</code> to store the best result.</p> Example <pre><code>import numpy as np\nfrom numpy.typing import NDArray\n\nfrom ropt.evaluator import EvaluatorContext, EvaluatorResult\nfrom ropt.workflow import BasicOptimizer\n\nDIM = 5\nCONFIG = {\n    \"variables\": {\n        \"variable_count\": DIM,\n        \"perturbation_magnitudes\": 1e-6,\n    },\n}\ninitial_values = 2 * np.arange(DIM) / DIM + 0.5\n\n\ndef rosenbrock(variables: NDArray[np.float64], _: EvaluatorContext) -&gt; EvaluatorResult:\n    objectives = np.zeros((variables.shape[0], 1), dtype=np.float64)\n    for v_idx in range(variables.shape[0]):\n        for d_idx in range(DIM - 1):\n            x, y = variables[v_idx, d_idx : d_idx + 2]\n            objectives[v_idx, 0] += (1.0 - x) ** 2 + 100 * (y - x * x) ** 2\n    return EvaluatorResult(objectives=objectives)\n\n\noptimizer = BasicOptimizer(CONFIG, rosenbrock)\noptimizer.run(initial_values)\n\nprint(f\"Optimal variables: {optimizer.results.evaluations.variables}\")\nprint(f\"Optimal objective: {optimizer.results.functions.weighted_objective}\")\n</code></pre> Customization <p>The optimization workflow executed by <code>BasicOptimizer</code> can be tailored in two main ways: by adding event handlers to the default workflow or by running an entirely different workflow:</p> <ol> <li> <p>Adding Custom Event Handlers</p> <p>This method allows for custom processing of events emitted by the default optimization workflow, without replacing the workflow itself. This is useful for tasks like custom logging or data processing.</p> <p>Event handlers can be specified in two ways, and handlers from both sources will be combined:</p> <ul> <li> <p>Environment Variable: If the <code>ROPT_HANDLERS</code> environment     variable contains a comma-separated list of event handler names,     these handlers will be added to the default optimization     workflow. Each name must correspond to a registered     <code>EventHandler</code>.</p> </li> <li> <p>JSON Configuration File: If a JSON configuration file is     found at <code>&lt;prefix&gt;/share/ropt/options.json</code> (where <code>&lt;prefix&gt;</code> is     the Python installation prefix or a system-wide data     prefix.<sup>1</sup>), <code>BasicOptimizer</code> will look for specific keys to     load additional event handlers. If this JSON file contains a     <code>basic_optimizer</code> key, and nested within it an <code>event_handlers</code>     key, the value of <code>event_handlers</code> should be a list of strings.     Each string in this list should be the name of a registered     <code>EventHandler</code>. These handlers will be added to     those found via <code>ROPT_HANDLERS</code>.</p> <p>Example <code>shared/ropt/options.json</code>:</p> <pre><code>{\n    \"basic_optimizer\": {\n        \"event_handlers\": [\"custom_logger\", \"extra/event_processor\"]\n    }\n}\n</code></pre> </li> </ul> <p>Note that if a custom optimization workflow is installed using the <code>ROPT_SCRIPT</code> environment variable (see below), these custom handlers will not be installed.</p> </li> <li> <p>Custom Workflow Execution</p> <p>If the <code>ROPT_SCRIPT</code> environment variable contains an option in the format <code>step-name=script.py</code> (where <code>script.py</code> may be any file), the named custom compute step will be executed instead of the standard optimization workflow, passing it the name of the script that defines the new optimization workflow.</p> <p>The custom compute step (<code>step-name</code>) must adhere to the following:</p> <ul> <li>It must be a registered <code>ComputeStep</code>.</li> <li>Its <code>run</code> method  must accept<ol> <li>An <code>evaluator</code> keyword argument, which will receive the      evaluator function passed to <code>BasicOptimizer</code>.</li> <li>A <code>script</code> keyword argument, which will receive the name of      script passed via <code>ROPT_SCRIPT</code>.</li> </ol> </li> <li> <p>This method must return a callable that returns an     optimization <code>ExitCode</code>.</p> <p>This callable will then be executed by <code>BasicOptimizer</code> in place of its default workflow.</p> </li> </ul> <p>As a short-cut is possible to also define <code>ROPT_SCRIPT</code> with only the name of the script (i.e. <code>ROPT_SCRIPT=script.py</code>). In this case a compute step with the name <code>run_script</code> is assumed to exists and will be used.</p> </li> </ol> <ol> <li> <p>The exact path to Python installation prefix, or the system's data prefix can be found using the Python <code>sysconfig</code> module: <pre><code>from sysconfig import get_paths\nprint(get_paths()[\"data\"])\n</code></pre> \u21a9</p> </li> </ol>"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer.results","title":"results  <code>property</code>","text":"<pre><code>results: FunctionResults | None\n</code></pre> <p>Return the optimal result found during the optimization.</p> <p>This property provides access to the best <code>FunctionResults</code> object discovered during the optimization process. It encapsulates the objective function value, constraint values, and other relevant information about the optimal solution.</p> <p>Returns:</p> Type Description <code>FunctionResults | None</code> <p>The optimal result.</p>"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    enopt_config: dict[str, Any],\n    evaluator: EvaluatorCallback,\n    *,\n    transforms: OptModelTransforms | None = None,\n    constraint_tolerance: float = 1e-10,\n) -&gt; None\n</code></pre> <p>Initialize a <code>BasicOptimizer</code> object.</p> <p>This constructor sets up the necessary components for a single optimization run. It requires an optimization configuration, an evaluator, and optional domain transform, which together define the optimization problem.</p> <p>The <code>constraint_tolerance</code> is used to check any constraints, if a constraint value is within this tolerance, it is considered satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>dict[str, Any]</code> <p>The configuration for the optimization.</p> required <code>evaluator</code> <code>EvaluatorCallback</code> <p>The evaluator object.</p> required <code>transforms</code> <code>OptModelTransforms | None</code> <p>Optional transforms to apply to the model.</p> <code>None</code> <code>constraint_tolerance</code> <code>float</code> <p>The constraint violation tolerance.</p> <code>1e-10</code>"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer.run","title":"run","text":"<pre><code>run(initial_values: ArrayLike) -&gt; ExitCode\n</code></pre> <p>Run the optimization process.</p> <p>This method initiates and executes the optimization workflow defined by the <code>BasicOptimizer</code> object. It manages the optimization, result handling, and event processing. After the optimization is complete, the optimal results, variables, and exit code can be accessed via the corresponding properties.</p> <p>Returns:</p> Type Description <code>ExitCode</code> <p>The exit code returned by the optimization workflow.</p>"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer.set_abort_callback","title":"set_abort_callback","text":"<pre><code>set_abort_callback(callback: Callable[[], bool]) -&gt; None\n</code></pre> <p>Set a callback to check for abort conditions.</p> <p>The provided callback function will be invoked repeatedly during the optimization process. If the callback returns <code>True</code>, the optimization will be aborted, and the <code>BasicOptimizer</code> will exit with an <code>ExitCode.USER_ABORT</code>.</p> <p>The callback function should have no arguments and return a boolean value.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[], bool]</code> <p>The callable to check for abort conditions.</p> required"},{"location":"reference/basic_optimizer/#ropt.workflow.BasicOptimizer.set_results_callback","title":"set_results_callback","text":"<pre><code>set_results_callback(callback: Callable[..., None]) -&gt; None\n</code></pre> <p>Set a callback to report new results.</p> <p>The provided callback function will be invoked whenever new results become available during the optimization process. This allows for real-time monitoring and analysis of the optimization's progress.</p> <p>The required signature of the callback function should be:</p> <pre><code>def callback(results: tuple[FunctionResults, ...]) -&gt; None:\n    ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[..., None]</code> <p>The callable that will be invoked to report new results.</p> required"},{"location":"reference/compute_steps_plugins/","title":"Compute Steps","text":""},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStepPlugin","title":"ropt.plugins.compute_step.base.ComputeStepPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract base class for plugins that create ComputeStep instances.</p> <p>This class defines the interface for plugins that act as factories for <code>ComputeStep</code> objects.</p>"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStepPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(name: str, **kwargs: Any) -&gt; ComputeStep\n</code></pre> <p>Create a ComputeStep instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>ComputeStep</code> objects. Plugin implementations must override this method to return an instance of their specific <code>ComputeStep</code> subclass.</p> <p>The <code>name</code> argument specifies the requested compute step, potentially in the format <code>\"plugin-name/method-name\"</code> or just <code>\"method-name\"</code>. Implementations can use this <code>name</code> to vary the created compute step if the plugin supports multiple compute step types.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The requested compute step name (potentially plugin-specific).</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments for custom configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ComputeStep</code> <p>An initialized instance of a <code>ComputeStep</code> subclass.</p>"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStep","title":"ropt.plugins.compute_step.base.ComputeStep","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for optimization compute steps.</p> <p>This class defines the fundamental interface for all executable compute steps within an optimization workflow. Concrete implementations, which perform specific actions like running an optimizer or evaluating functions, must inherit from this base class.</p> <p><code>ComputeStep</code> instances are typically created using the <code>create_compute_step</code> function.</p>"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStep.event_handlers","title":"event_handlers  <code>property</code>","text":"<pre><code>event_handlers: list[EventHandler]\n</code></pre> <p>Get the event handlers attached to this compute step.</p> <p>Returns:</p> Type Description <code>list[EventHandler]</code> <p>A list of handlers.</p>"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStep.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the ComputeStep.</p>"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStep.add_event_handler","title":"add_event_handler","text":"<pre><code>add_event_handler(handler: EventHandler) -&gt; None\n</code></pre> <p>Add an event handler.</p> <p>Compute steps emit <code>events</code> to report on the calculations they perform. These events are processed by independently created <code>event handlers</code>. Use the <code>add_event_handler</code> method to attach these handlers to the compute step.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>EventHandler</code> <p>The handler to add.</p> required"},{"location":"reference/compute_steps_plugins/#ropt.plugins.compute_step.base.ComputeStep.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Execute the logic defined by this compute step.</p> <p>This abstract method must be implemented by concrete <code>ComputeStep</code> subclasses to define the specific action the compute step performs within the optimization workflow.</p> <p>The return value and type can vary depending on the specific implementation.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Any</code> <p>Positional arguments.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the execution, if any.</p>"},{"location":"reference/default_compute_steps_plugins/","title":"Compute Steps","text":""},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.default","title":"ropt.plugins.compute_step.default","text":"<p>This module provides the default plugin implementations for compute steps.</p> <p>Supported Components:</p> <ul> <li><code>ensemble_evaluator</code>: Performs ensemble evaluations     (<code>DefaultEnsembleEvaluatorComputeStep</code>).</li> <li><code>optimizer</code>: Runs an optimization algorithm using a configured optimizer     plugin     (<code>DefaultOptimizerComputeStep</code>).</li> </ul>"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.default.DefaultComputeStepPlugin","title":"ropt.plugins.compute_step.default.DefaultComputeStepPlugin","text":"<p>               Bases: <code>ComputeStepPlugin</code></p> <p>The default plugin for creating compute_steps.</p> <p>This plugin acts as a factory for the standard <code>ComputeStep</code> implementations provided by <code>ropt</code>.</p> <p>Supported Compute Steps:</p> <ul> <li><code>ensemble_evaluator</code>: Creates a     <code>DefaultEnsembleEvaluatorComputeStep</code>     instance, which performs ensemble evaluations.</li> <li><code>optimizer</code>: Creates a     <code>DefaultOptimizerComputeStep</code>     instance, which runs an optimization algorithm using a configured     optimizer plugin.</li> </ul>"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.ensemble_evaluator.DefaultEnsembleEvaluatorComputeStep","title":"ropt.plugins.compute_step.ensemble_evaluator.DefaultEnsembleEvaluatorComputeStep","text":"<p>               Bases: <code>ComputeStep</code></p> <p>The default ensemble evaluator compute step for optimization workflows.</p> <p>This compute step performs one or more ensemble evaluations based on the provided <code>variables</code>. It yields a tuple of <code>FunctionResults</code> objects, one for each input variable vector evaluated.</p> <p>The compute step emits the following events:</p> <ul> <li><code>START_ENSEMBLE_EVALUATOR</code>:   Emitted before the evaluation process begins.</li> <li><code>START_EVALUATION</code>: Emitted   just before the underlying ensemble evaluation is called.</li> <li><code>FINISHED_EVALUATION</code>: Emitted   after the evaluation completes, carrying the generated <code>FunctionResults</code>   in its <code>data</code> dictionary under the key <code>\"results\"</code>. Event handlers   typically listen for this event.</li> <li><code>FINISHED_ENSEMBLE_EVALUATOR</code>:   Emitted after the entire compute step, including result emission, is finished.</li> </ul>"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.ensemble_evaluator.DefaultEnsembleEvaluatorComputeStep.__init__","title":"__init__","text":"<pre><code>__init__(*, evaluator: Evaluator) -&gt; None\n</code></pre> <p>Initialize a default evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>Evaluator</code> <p>The evaluator object to run function evaluations.</p> required"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.ensemble_evaluator.DefaultEnsembleEvaluatorComputeStep.run","title":"run","text":"<pre><code>run(\n    config: EnOptConfig,\n    variables: ArrayLike,\n    *,\n    transforms: OptModelTransforms | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; ExitCode\n</code></pre> <p>Run the ensemble evaluator.</p> <p>This method executes the core logic of the ensemble evaluator. It requires an optimizer configuration (<code>EnOptConfig</code>) and optionally accepts specific variable vectors to evaluate.</p> <p>If <code>metadata</code> is provided, it is attached to the <code>Results</code> objects emitted via the <code>FINISHED_EVALUATION</code> event.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>Optimizer configuration.</p> required <code>variables</code> <code>ArrayLike</code> <p>Variable vector(s) to evaluate.</p> required <code>transforms</code> <code>OptModelTransforms | None</code> <p>Optional transforms to apply to the variables,         objectives, and constraints.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary to attach to emitted <code>FunctionResults</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ExitCode</code> <p>An <code>ExitCode</code> indicating the outcome.</p>"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.optimizer.DefaultOptimizerComputeStep","title":"ropt.plugins.compute_step.optimizer.DefaultOptimizerComputeStep","text":"<p>               Bases: <code>ComputeStep</code></p> <p>The default optimizer compute step.</p> <p>This compute step executes an optimization algorithm based on a provided configuration (<code>EnOptConfig</code> or a compatible dictionary). It iteratively performs function and potentially gradient evaluations, yielding a sequence of <code>FunctionResults</code> and <code>GradientResults</code> objects.</p> <p>While initial variable values are typically specified in the configuration, they can be overridden by passing them directly to the <code>run</code> method.</p> <p>The following events are emitted during execution:</p> <ul> <li><code>START_OPTIMIZER</code>:   Emitted just before the optimization process begins.</li> <li><code>START_EVALUATION</code>: Emitted   immediately before an ensemble evaluation (for functions or gradients)   is requested from the underlying optimizer.</li> <li><code>FINISHED_EVALUATION</code>: Emitted   after an evaluation completes. This event carries the generated   <code>Results</code> object(s) in its <code>data</code> dictionary   under the key <code>\"results\"</code>. Event handlers typically listen for this event   to process or track optimization progress.</li> <li><code>FINISHED_OPTIMIZER</code>:   Emitted after the entire optimization process concludes (successfully,   or due to termination conditions or errors).</li> </ul> <p>This compute step also supports nested optimization. If a <code>nested_optimization</code> function is provided to the <code>run</code> method, the optimizer will execute a nested optimization at as part of each function evaluation. The <code>nested_optimization</code> function is expected to return a single <code>FunctionResults</code> object and a flag that indicates whether the optimization was aborted by the user.</p>"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.optimizer.DefaultOptimizerComputeStep.__init__","title":"__init__","text":"<pre><code>__init__(*, evaluator: Evaluator) -&gt; None\n</code></pre> <p>Initialize a default optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>Evaluator</code> <p>The evaluator object to run function evaluations.</p> required"},{"location":"reference/default_compute_steps_plugins/#ropt.plugins.compute_step.optimizer.DefaultOptimizerComputeStep.run","title":"run","text":"<pre><code>run(\n    config: EnOptConfig,\n    variables: ArrayLike,\n    *,\n    transforms: OptModelTransforms | None = None,\n    nested_optimization: NestedOptimizationCallable\n    | None = None,\n    metadata: dict[str, Any] | None = None,\n) -&gt; ExitCode\n</code></pre> <p>Run the compute step to perform an optimization.</p> <p>This method executes the core logic of the optimizer compute step. It requires an optimizer configuration (<code>EnOptConfig</code>) and optionally accepts specific initial variable vectors, and/or a nested optimization workflow, and metadata.</p> <p>If <code>variables</code> are not provided, the initial values specified in the <code>config</code> are used. If <code>variables</code> are provided, they override the config's initial values.</p> <p>If <code>metadata</code> is provided, it is attached to the <code>Results</code> objects emitted via the <code>FINISHED_EVALUATION</code> event.</p> <p>If a <code>nested_optimization</code> callable is provided, a callable will be called passing the initial variables to use. The callable should return a a single <code>FunctionResults</code> object that should contain the results of the nested optimization, and a flag indicating whether the optimization was aborted by the user.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>Optimizer configuration.</p> required <code>transforms</code> <code>OptModelTransforms | None</code> <p>Optional transforms to apply to the variables,                  objectives, and constraints.</p> <code>None</code> <code>variables</code> <code>ArrayLike</code> <p>Optional initial variable vector(s) to start from.</p> required <code>nested_optimization</code> <code>NestedOptimizationCallable | None</code> <p>Optional callable to run a nested optimization.</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional dictionary to attach to emitted <code>Results</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ExitCode</code> <p>An exit code indicating the outcome of the optimization.</p>"},{"location":"reference/default_evaluator_plugins/","title":"Evaluators","text":""},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.default.DefaultEvaluatorPlugin","title":"ropt.plugins.evaluator.default.DefaultEvaluatorPlugin","text":"<p>               Bases: <code>EvaluatorPlugin</code></p> <p>The default plugin for creating evaluators.</p> <p>This plugin acts as a factory for the standard evaluator implementations provided by <code>ropt</code>.</p> <p>Supported Evaluators:</p> <ul> <li><code>function_evaluator</code>: Creates a     <code>DefaultFunctionEvaluator</code>     instance, which uses function calls to calculated individual objectives     and constraints.</li> </ul>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator._function_evaluator.DefaultFunctionEvaluator","title":"ropt.plugins.evaluator._function_evaluator.DefaultFunctionEvaluator","text":"<p>               Bases: <code>Evaluator</code></p> <p>An evaluator that forwards calls to an evaluator function.</p> <p>This class acts as an adapter, allowing a standard Python callable (which matches the signature of the <code>eval</code> method) to be used as an <code>Evaluator</code> within an optimization workflow.</p> <p>It is initialized with an <code>evaluator</code> callable. When the <code>eval</code> method of this class is invoked, it simply delegates the call, along with all arguments, to the wrapped <code>evaluator</code> function.</p>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator._function_evaluator.DefaultFunctionEvaluator.__init__","title":"__init__","text":"<pre><code>__init__(*, callback: EvaluatorCallback) -&gt; None\n</code></pre> <p>Initialize the DefaultFunctionEvaluator.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>EvaluatorCallback</code> <p>The callable that will perform the actual evaluation.</p> required"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator._function_evaluator.DefaultFunctionEvaluator.eval","title":"eval","text":"<pre><code>eval(\n    variables: NDArray[float64], context: EvaluatorContext\n) -&gt; EvaluatorResult\n</code></pre> <p>Forward the evaluation call to the wrapped evaluator function.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The matrix of variables to evaluate.</p> required <code>context</code> <code>EvaluatorContext</code> <p>The evaluation context.</p> required <p>Returns:</p> Type Description <code>EvaluatorResult</code> <p>The result of calling the wrapped evaluator function.</p>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator","title":"ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator","text":"<p>               Bases: <code>Evaluator</code></p> <p>An evaluator that caches results to avoid redundant computations.</p> <p>This evaluator attempts to retrieve previously computed function results from a cache before delegating to another evaluator. The cache is populated from <code>FunctionResults</code> objects stored by <code>EventHandler</code> instances specified as <code>sources</code>.</p> <p>When an evaluation is requested, for each variable vector and its corresponding realization, this evaluator searches through the <code>results</code> attribute of its <code>sources</code>. If a <code>FunctionResults</code> object is found where the <code>variables</code> match the input (within a small tolerance) and the <code>realization</code> also matches, the cached <code>objectives</code> and <code>constraints</code> from that <code>FunctionResults</code> object are used.</p> <p>If some, but not all, requested evaluations are found in the cache, this evaluator will mark the cached ones as inactive for the next evaluator in the chain and then call that evaluator to compute only the missing results. The final combined results (cached and newly computed) are then returned.</p> <p>This is particularly useful in scenarios where the same variable sets might be evaluated multiple times, for example, in iterative optimization algorithms or when restarting optimizations.</p>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    evaluator: Evaluator,\n    sources: list[EventHandler] | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the DefaultCachedEvaluator.</p> <p>The <code>sources</code> argument should be a sequence of <code>EventHandler</code> instances. These handlers are expected to store <code>FunctionResults</code> in their <code>[\"results\"]</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>Evaluator</code> <p>The evaluator to cache.</p> required <code>sources</code> <code>list[EventHandler] | None</code> <p><code>EventHandler</code> instances for retrieving cached results.</p> <code>None</code>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator.eval_cached","title":"eval_cached","text":"<pre><code>eval_cached(\n    variables: NDArray[float64], context: EvaluatorContext\n) -&gt; tuple[\n    EvaluatorResult, dict[int, tuple[int, FunctionResults]]\n]\n</code></pre> <p>Evaluate objective and constraint functions, utilizing a cache.</p> <p>This method implements the core evaluation logic. It returns not only the evaluation results but also the function results that were retrieved from cache. The <code>eval</code> method in this class does not utilize these indices. However, derived classes can overload <code>eval</code> and use this information to add further details to the results, such as populating the <code>evaluation_info</code> attribute of an <code>EvaluatorResult</code>.</p> <p>The cache hits that are returned consists of a dictionary where the keys are the indices of the variable vectors that were found in the cache, and the values are tuples containing the realization index of the cached vectors and the corresponding <code>FunctionResults</code> object. This allows the caller to know which evaluations were retrieved from cache and which were computed anew. The cached evaluations can then be retrieved from the <code>FunctionResults</code> object, using the realization index.</p> Note <p>If the optimization was initialized with realization names in the configuration, these are used to match the realizations of the requested evaluations, with those in the cached results. This means that the results may originate from a different optimization run, as long as the realization names are still valid. However, in this case the results used for finding cached values must also store the realization names, otherwise the cached results will not be found.</p> <p>If the configuration does not contain realization names, the realization indices are used to match the realizations of the requested evaluations. In this case the indices of the realizations in the cached results must match those of the requested evaluations, i.e. they must have been specified in the same order in the respective configurations.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>Matrix of variables to evaluate (each row is a vector).</p> required <code>context</code> <code>EvaluatorContext</code> <p>The evaluation context.</p> required <p>Returns:</p> Type Description <code>tuple[EvaluatorResult, dict[int, tuple[int, FunctionResults]]]</code> <p>An <code>EvaluatorResult</code> and the cache hits.</p>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator.eval","title":"eval","text":"<pre><code>eval(\n    variables: NDArray[float64], context: EvaluatorContext\n) -&gt; EvaluatorResult\n</code></pre> <p>Evaluate objective and constraint functions, utilizing a cache.</p> <p>For each input variable vector and its realization, this method first attempts to find a matching result in the cache provided by its <code>sources</code>.</p> <p>If a result is found in the cache, it's used directly. If not, the evaluation is delegated to the stored evaluator. The <code>context.active</code> array is updated to indicate to the subsequent evaluator which evaluations are still pending; evaluations found in the cache are marked as inactive.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>Matrix of variables to evaluate (each row is a vector).</p> required <code>context</code> <code>EvaluatorContext</code> <p>The evaluation context.</p> required <p>Returns:</p> Type Description <code>EvaluatorResult</code> <p>An <code>EvaluatorResult</code> with calculated or cached values.</p>"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator.add_sources","title":"add_sources","text":"<pre><code>add_sources(\n    sources: EventHandler | Sequence[EventHandler],\n) -&gt; None\n</code></pre> <p>Add one or more <code>EventHandler</code> sources to the evaluator.</p> <p>This method allows adding additional sources of cached results to the evaluator. The sources are expected to be <code>EventHandler</code> instances that store <code>FunctionResults</code>.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>EventHandler | Sequence[EventHandler]</code> <p><code>EventHandler</code> instances to add as a source.</p> required"},{"location":"reference/default_evaluator_plugins/#ropt.plugins.evaluator.cached_evaluator.DefaultCachedEvaluator.remove_sources","title":"remove_sources","text":"<pre><code>remove_sources(\n    sources: EventHandler | Sequence[EventHandler],\n) -&gt; None\n</code></pre> <p>Remove one or more <code>EventHandler</code> sources from the evaluator.</p> <p>This method allows removing previously added sources of cached results from the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>EventHandler | Sequence[EventHandler]</code> <p><code>EventHandler</code> instances to remove as a source.</p> required"},{"location":"reference/default_event_handler_plugins/","title":"Event Handlers","text":""},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler.default","title":"ropt.plugins.event_handler.default","text":"<p>This module provides the default plugin implementations for event handlers.</p> <p>Supported handlers:</p> <ul> <li><code>tracker</code>: Tracks the 'best' or 'last' valid result based on objective       value and constraints       (<code>DefaultTrackerHandler</code>).</li> <li><code>store</code>: Accumulates all results from specified sources       (<code>DefaultStoreHandler</code>).</li> <li><code>observer</code>: Listens for events from specified sources, and calls a       callback for each event       (<code>DefaultObserverHandler</code>).</li> </ul>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler.default.DefaultEventHandlerPlugin","title":"ropt.plugins.event_handler.default.DefaultEventHandlerPlugin","text":"<p>               Bases: <code>EventHandlerPlugin</code></p> <p>The default plugin for creating built-in event handlers.</p> <p>This plugin acts as a factory for the standard <code>EventHandler</code> implementations provided by <code>ropt</code>.</p> <p>Supported Handlers:</p> <ul> <li><code>tracker</code>: Creates a     <code>DefaultTrackerHandler</code>     instance, which tracks either the 'best' or 'last' valid result based on     objective value and constraints.</li> <li><code>store</code>: Creates a     <code>DefaultStoreHandler</code>     instance, which accumulates all results received from specified sources.</li> <li><code>observer</code>: Creates a     <code>DefaultObserverHandler</code>     instance, which calls a callback for each event received from specified     sources.</li> </ul>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._tracker.DefaultTrackerHandler","title":"ropt.plugins.event_handler._tracker.DefaultTrackerHandler","text":"<p>               Bases: <code>EventHandler</code></p> <p>The default event handler for tracking optimization results.</p> <p>This event handler listens for <code>FINISHED_EVALUATION</code> events emitted from within an optimization workflow. It processes the <code>Results</code> objects contained within these events and selects a single <code>FunctionResults</code> object to retain based on defined criteria.</p> <p>The criteria for selection are:</p> <ul> <li><code>what='best'</code> (default): Tracks the result with the lowest weighted   objective value encountered so far.</li> <li><code>what='last'</code>: Tracks the most recently received valid result.</li> </ul> <p>Optionally, results can be filtered based on constraint violations using the <code>constraint_tolerance</code> parameter. If provided, any result violating constraints beyond this tolerance is ignored.</p> <p>The selected result (in the optimizer domain) is stored internally. The result accessible via dictionary access (<code>handler[\"results\"]</code>) is the selected result, potentially transformed to the user domain.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._tracker.DefaultTrackerHandler.event_types","title":"event_types  <code>property</code>","text":"<pre><code>event_types: set[EventType]\n</code></pre> <p>Return the event types that are handled.</p> <p>Returns:</p> Type Description <code>set[EventType]</code> <p>A set of event types that are handled.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._tracker.DefaultTrackerHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    what: Literal[\"best\", \"last\"] = \"best\",\n    constraint_tolerance: float | None = None,\n) -&gt; None\n</code></pre> <p>Initialize a default tracker event handler.</p> <p>This event handler monitors <code>Results</code> objects and selects a single <code>FunctionResults</code> object to retain based on the <code>what</code> criterion ('best' or 'last').</p> <p>The 'best' result is the one with the lowest weighted objective value encountered so far. The 'last' result is the most recently received valid result. Results can optionally be filtered by <code>constraint_tolerance</code> to ignore those violating constraints beyond the specified threshold.</p> <p>Tracking logic (comparing 'best' or selecting 'last') operates on the results in the optimizer's domain. However, the final selected result that is made accessible via dictionary access (<code>handler[\"results\"]</code>) is transformed to the user's domain.</p> <p>Parameters:</p> Name Type Description Default <code>what</code> <code>Literal['best', 'last']</code> <p>Criterion for selecting results ('best' or 'last').</p> <code>'best'</code> <code>constraint_tolerance</code> <code>float | None</code> <p>Optional threshold for filtering constraint violations.</p> <code>None</code>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._tracker.DefaultTrackerHandler.handle_event","title":"handle_event","text":"<pre><code>handle_event(event: Event) -&gt; None\n</code></pre> <p>Handle incoming events.</p> <p>This method processes incoming <code>FINISHED_EVALUATION</code> events.</p> <p>If a relevant event containing results is received, this method updates the tracked result (<code>self[\"results\"]</code>) based on the <code>what</code> criterion ('best' or 'last') and the optional <code>constraint_tolerance</code>.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event object.</p> required"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._store.DefaultStoreHandler","title":"ropt.plugins.event_handler._store.DefaultStoreHandler","text":"<p>               Bases: <code>EventHandler</code></p> <p>The default event handler for storing optimization results.</p> <p>This event handler listens for <code>FINISHED_EVALUATION</code> events emitted by specified compute steps from within an optimization workflow. It collects all <code>Results</code> objects contained within these events and stores them sequentially in memory.</p> <p>The accumulated results are stored as a tuple and can be accessed via dictionary access using the key <code>\"results\"</code> (e.g., <code>handler[\"results\"]</code>). Each time new results are received from a valid source, they are appended to this tuple.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._store.DefaultStoreHandler.event_types","title":"event_types  <code>property</code>","text":"<pre><code>event_types: set[EventType]\n</code></pre> <p>Return the event types that are handled.</p> <p>Returns:</p> Type Description <code>set[EventType]</code> <p>A set of event types that are handled.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._store.DefaultStoreHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize a default store event handler.</p> <p>This event handler collects and stores all <code>Results</code> objects it receives. It listens for <code>FINISHED_EVALUATION</code> events and appends the results contained within them to an internal tuple.</p> <p>The results are converted from the optimizer domain to the user domain before being stored. The accumulated results are stored as a tuple and can be accessed via dictionary access using the key <code>\"results\"</code> (e.g., <code>handler[\"results\"]</code>). Initially, <code>handler[\"results\"]</code> is <code>None</code>.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._store.DefaultStoreHandler.handle_event","title":"handle_event","text":"<pre><code>handle_event(event: Event) -&gt; None\n</code></pre> <p>Handle incoming events.</p> <p>This method processes events it receives. It specifically listens for <code>FINISHED_EVALUATION</code> events.</p> <p>If a relevant event containing results is received, this method retrieves the results, optionally transforms them to the user domain and appends them to the tuple stored in <code>self[\"results\"]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event object.</p> required"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._observer.DefaultObserverHandler","title":"ropt.plugins.event_handler._observer.DefaultObserverHandler","text":"<p>               Bases: <code>EventHandler</code></p> <p>The default event handler for observing events.</p> <p>This event handler listens for events and forwards them to one or more callback functions.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._observer.DefaultObserverHandler.event_types","title":"event_types  <code>property</code>","text":"<pre><code>event_types: set[EventType]\n</code></pre> <p>Return the event types that are handled.</p> <p>Returns:</p> Type Description <code>set[EventType]</code> <p>A set of event types that are handled.</p>"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._observer.DefaultObserverHandler.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    event_types: set[EventType],\n    callback: Callable[[Event], None],\n) -&gt; None\n</code></pre> <p>Initialize a default event handler.</p> <p>This event handler responds to events by calling <code>callback</code> if the event type matches <code>event_types</code>.</p> <p>Parameters:</p> Name Type Description Default <code>event_types</code> <code>set[EventType]</code> <p>The set of event types  to respond to.</p> required <code>callback</code> <code>Callable[[Event], None]</code> <p>The callable to call.</p> required"},{"location":"reference/default_event_handler_plugins/#ropt.plugins.event_handler._observer.DefaultObserverHandler.handle_event","title":"handle_event","text":"<pre><code>handle_event(event: Event) -&gt; None\n</code></pre> <p>Handle incoming events.</p> <p>This method processes events emitted from within the workflow.</p> <p>If a event containing results is received, and its type equals the stored event type, the stored callback is called.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event object emitted from the workflow.</p> required"},{"location":"reference/default_function_estimator_plugin/","title":"Default Function Estimators","text":""},{"location":"reference/default_function_estimator_plugin/#ropt.plugins.function_estimator.default.DefaultFunctionEstimator","title":"ropt.plugins.function_estimator.default.DefaultFunctionEstimator","text":"<p>               Bases: <code>FunctionEstimator</code></p> <p>The default implementation for function estimation strategies.</p> <p>This class provides methods for combining objective function values and gradients from an ensemble of realizations into a single representative value or gradient. The specific method is configured via the <code>FunctionEstimatorConfig</code> in the main <code>EnOptConfig</code>.</p> <p>Supported Methods:</p> <ul> <li> <p><code>mean</code> (or <code>default</code>):     Calculates the combined function value as the weighted mean of the     individual realization function values. The combined gradient is     calculated as the weighted mean of the individual realization gradients     (unless <code>merge_realizations</code> is true, in which case the pre-merged     gradient is used directly).</p> </li> <li> <p><code>stddev</code>:     Calculates the combined function value as the weighted standard     deviation of the individual realization function values. The combined     gradient is calculated using the chain rule based on the standard     deviation formula. This method requires at least two realizations with     non-zero weights and is incompatible with <code>merge_realizations=True</code>     for gradient calculation.</p> </li> </ul>"},{"location":"reference/default_realization_filter_plugin/","title":"Default Realization Filters","text":""},{"location":"reference/default_realization_filter_plugin/#ropt.plugins.realization_filter.default.DefaultRealizationFilter","title":"ropt.plugins.realization_filter.default.DefaultRealizationFilter","text":"<p>               Bases: <code>RealizationFilter</code></p> <p>The default implementation for realization filtering strategies.</p> <p>This class provides several methods for calculating realization weights based on objective or constraint values. The specific method and its parameters are configured via the <code>RealizationFilterConfig</code> in the main <code>EnOptConfig</code>.</p> <p>Supported Methods:</p> <ul> <li> <p><code>sort-objective</code>:     Sorts realizations based on a weighted sum of specified objective     function values. It then assigns zero weights to realizations outside of     a defined index range (<code>first</code> to <code>last</code>) in the sorted list. Requires     options defined by     <code>SortObjectiveOptions</code>.</p> </li> <li> <p><code>sort-constraint</code>:     Sorts realizations based on the value of a single specified constraint     function. It assigns zero weights to realizations outside of a defined     index range (<code>first</code> to <code>last</code>) in the sorted list. Requires options     defined by     <code>SortConstraintOptions</code>.</p> </li> <li> <p><code>cvar-objective</code>:     Calculates realization weights using the Conditional Value-at-Risk (CVaR)     method applied to a weighted sum of specified objective function values.     Weights are assigned based on a specified <code>percentile</code> of the worst-performing     realizations (highest objective values for minimization). Interpolation is     used if the percentile boundary falls between realizations.     Requires options defined by     <code>CVaRObjectiveOptions</code>.</p> </li> <li> <p><code>cvar-constraint</code>:     Calculates realization weights using the CVaR method applied to the     value of a single specified constraint function. Weights are assigned     based on a specified <code>percentile</code> of the worst-performing realizations     (definition of \"worst\" depends on the constraint type: LE, GE, or EQ).     Interpolation is used if the percentile boundary falls between     realizations.     Requires options defined by     <code>CVaRConstraintOptions</code>.</p> </li> </ul>"},{"location":"reference/default_realization_filter_plugin/#ropt.plugins.realization_filter.default.SortObjectiveOptions","title":"ropt.plugins.realization_filter.default.SortObjectiveOptions","text":"<p>               Bases: <code>_ConfigBaseModel</code></p> <p>Configuration settings for the <code>sort-objective</code> realization filter.</p> <p>This method sorts realizations based on a weighted sum of objective function values and assigns weights only to those within a specified rank range.</p> <p>How it works:</p> <ol> <li>A weighted sum is calculated for each realization using the objective    values specified by the <code>sort</code> indices and the corresponding weights from    the main <code>EnOptConfig</code>. If only one objective    index is provided in <code>sort</code>, no weighting is applied.</li> <li>Realizations are sorted based on this calculated value (ascending).</li> <li>Realizations whose rank falls within the range [<code>first</code>, <code>last</code>]    (inclusive) are selected.</li> <li>The original weights (from <code>EnOptConfig.realizations.weights</code>) of the    selected realizations are retained; all other realizations receive a    weight of zero. Failed realizations (NaN objective values) are effectively    given the lowest rank and are excluded before selection.</li> </ol> <p>Attributes:</p> Name Type Description <code>sort</code> <code>tuple[NonNegativeInt]</code> <p>List of objective function indices to use for sorting.</p> <code>first</code> <code>NonNegativeInt</code> <p>The starting rank (0-based index) of realizations to select after sorting.</p> <code>last</code> <code>NonNegativeInt</code> <p>The ending rank (0-based index) of realizations to select after sorting.</p>"},{"location":"reference/default_realization_filter_plugin/#ropt.plugins.realization_filter.default.SortConstraintOptions","title":"ropt.plugins.realization_filter.default.SortConstraintOptions","text":"<p>               Bases: <code>_ConfigBaseModel</code></p> <p>Configuration settings for the <code>sort-constraint</code> realization filter.</p> <p>This method sorts realizations based on the value of a single constraint function and assigns weights only to those within a specified rank range.</p> <p>How it works:</p> <ol> <li>The values of the constraint function specified by the <code>sort</code> index are    retrieved for each realization.</li> <li>Realizations are sorted based on these constraint values (ascending).</li> <li>Realizations whose rank falls within the range [<code>first</code>, <code>last</code>]    (inclusive) are selected.</li> <li>The original weights (from <code>EnOptConfig.realizations.weights</code>) of the    selected realizations are retained; all other realizations receive a    weight of zero. Failed realizations (NaN constraint values) are effectively    given the lowest rank and are excluded before selection.</li> </ol> <p>Attributes:</p> Name Type Description <code>sort</code> <code>NonNegativeInt</code> <p>The index of the constraint function to use for sorting.</p> <code>first</code> <code>NonNegativeInt</code> <p>The starting rank (0-based index) of realizations to select after sorting.</p> <code>last</code> <code>NonNegativeInt</code> <p>The ending rank (0-based index) of realizations to select after sorting.</p>"},{"location":"reference/default_realization_filter_plugin/#ropt.plugins.realization_filter.default.CVaRObjectiveOptions","title":"ropt.plugins.realization_filter.default.CVaRObjectiveOptions","text":"<p>               Bases: <code>_ConfigBaseModel</code></p> <p>Configuration settings for the <code>cvar-objective</code> realization filter.</p> <p>This method calculates realization weights using the Conditional Value-at-Risk (CVaR) approach applied to a weighted sum of objective function values. It focuses on the \"tail\" of the distribution representing the worst-performing realizations.</p> <p>How it works:</p> <ol> <li>A weighted sum is calculated for each realization using the objective    values specified by the <code>sort</code> indices and the corresponding weights    from the main <code>EnOptConfig</code>. If only    one objective index is provided in <code>sort</code>, no weighting is applied.</li> <li>Realizations are conceptually sorted based on this calculated value    (ascending, assuming minimization).</li> <li>The method identifies the subset of realizations corresponding to the    <code>percentile</code> worst outcomes (i.e., the highest weighted objective values).</li> <li>Weights are assigned to these worst-performing realizations based on the    CVaR calculation. If the <code>percentile</code> boundary falls between two    realizations, interpolation is used to assign partial weights. All other    realizations receive a weight of zero.</li> <li>Failed realizations (NaN objective values) are effectively excluded from    the CVaR calculation.</li> </ol> <p>Attributes:</p> Name Type Description <code>sort</code> <code>tuple[NonNegativeInt]</code> <p>List of objective function indices to use for the weighted sum.</p> <code>percentile</code> <code>Annotated[float, Field(gt=0.0, le=1.0)]</code> <p>The CVaR percentile (0.0 to 1.0) defining the portion of         worst realizations to consider. Defaults to 0.5.</p>"},{"location":"reference/default_realization_filter_plugin/#ropt.plugins.realization_filter.default.CVaRConstraintOptions","title":"ropt.plugins.realization_filter.default.CVaRConstraintOptions","text":"<p>               Bases: <code>_ConfigBaseModel</code></p> <p>Configuration settings for the <code>cvar-constraint</code> realization filter.</p> <p>This method calculates realization weights using the Conditional Value-at-Risk (CVaR) approach applied to the values of a single constraint function. It focuses on the \"tail\" of the distribution representing the realizations that most severely violate or are furthest from satisfying the constraint.</p> <p>How it works:</p> <ol> <li>The values of the constraint function specified by the <code>sort</code> index are    retrieved for each realization. These values typically represent the    constraint function evaluated minus its right-hand-side value (e.g.,    <code>g(x) - rhs</code>).</li> <li>Realizations are conceptually sorted based on how \"badly\" they perform    with respect to the constraint type:<ul> <li>LE (<code>&lt;=</code>) constraints: Realizations with the largest positive   values (most violated) are considered the worst.</li> <li>GE (<code>&gt;=</code>) constraints: Realizations with the smallest negative   values (most violated) are considered the worst.</li> <li>EQ (<code>==</code>) constraints: Realizations with the largest absolute   values (furthest from zero) are considered the worst.</li> </ul> </li> <li>The method identifies the subset of realizations corresponding to the    <code>percentile</code> worst outcomes based on the sorting defined above.</li> <li>Weights are assigned to these worst-performing realizations based on the    CVaR calculation. If the <code>percentile</code> boundary falls between two    realizations, interpolation is used to assign partial weights. All other    realizations receive a weight of zero.</li> <li>Failed realizations (NaN constraint values) are effectively excluded from    the CVaR calculation.</li> </ol> <p>Attributes:</p> Name Type Description <code>sort</code> <code>NonNegativeInt</code> <p>The index of the constraint function to use.</p> <code>percentile</code> <code>Annotated[float, Field(gt=0.0, le=1.0)]</code> <p>The CVaR percentile (0.0 to 1.0) defining the portion of         worst realizations to consider. Defaults to 0.5.</p>"},{"location":"reference/domain_transforms/","title":"Domain Transforms","text":""},{"location":"reference/domain_transforms/#ropt.transforms","title":"ropt.transforms","text":"<p>Domain Transformation Framework.</p> <p>This module provides a flexible framework for transforming optimization variables, objectives, and constraints between user-defined domains and the domains used internally by the optimizer. These transformations are essential for:</p> <ul> <li>Improving Optimizer Performance: Scaling, shifting, and other   transformations can significantly enhance the efficiency, stability, and   convergence of optimization algorithms.</li> <li>Implementing Custom Mappings:  Beyond simple scaling, this framework   supports complex, user-defined mappings between domains, allowing for   tailored problem representations.</li> <li>Handling Diverse Units and Scales: Transformations enable the optimizer   to work with variables and functions that may have vastly different units   or scales, improving numerical stability.</li> </ul> <p>Key Components:</p> <ul> <li>Abstract Base Classes: Transform classes derive from abstract base classes   that define the specific mapping logic between domains.<ul> <li><code>VariableTransform</code>:   Defines the interface for transforming variables between user and   optimizer domains.</li> <li><code>ObjectiveTransform</code>:   Defines the interface for transforming objective values between user   and optimizer domains.</li> <li><code>NonLinearConstraintTransform</code>:   Defines the interface for transforming non-linear constraint values   between user and optimizer domains.</li> </ul> </li> <li><code>OptModelTransforms</code>:   A container class for conveniently grouping and   passing multiple transformation objects (variable, objective, and   nonlinear constraint).</li> </ul> <p>Workflow and Integration:</p> <ol> <li>Configuration: Transformation objects are passed to the     <code>EnOptConfig</code> during configuration validation,     using an <code>OptModelTransforms</code>     instance. This ensures that the entire optimization process is aware of and     configured for the transformed space. The transformation objects are stored     in the configuration object.</li> <li>Optimization Workflow: The same transformation objects are passed to the     relevant optimization steps via the configuration object. (See, for example,     the default implementation of an optimizer compute step in     <code>DefaultOptimizerComputeStep.run</code>).</li> <li>Evaluation: When the optimizer requests an evaluation of a variable     vector, the following occurs:<ul> <li>Transformation to the User Domain: The variable vector is      transformed from the optimizer       domain back to the user domain using the <code>from_optimizer</code> method of       the <code>VariableTransform</code>.</li> <li>Function Evaluation: Objective and constraint values are calculated       in the user domain.</li> <li>Transformation to the Optimizer Domain: The resulting objective and      constraint values are       transformed to the optimizer domain using the <code>to_optimizer</code> methods       of the <code>ObjectiveTransform</code> and <code>NonLinearConstraintTransform</code>.</li> </ul> </li> <li>Optimization: The optimizer proceeds using the transformed values.</li> <li>Results: The <code>Results</code> objects produced during     optimization hold values in the optimizer domain. To obtain results in the     user domain, the     <code>transform_from_optimizer</code>     method is used to create new <code>Results</code> objects with the transformed values.     For example,     <code>DefaultOptimizerComputeStep.run</code>     emits events that include a dictionary with a <code>\"results\"</code> key That contains     <code>Results</code> objects in the  optimizer domain. To obtain results in the user     domain they must be converted using the     <code>transform_from_optimizer</code>     method.</li> </ol>"},{"location":"reference/domain_transforms/#ropt.transforms.OptModelTransforms","title":"ropt.transforms.OptModelTransforms  <code>dataclass</code>","text":"<p>A container for optimization model transformers.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.OptModelTransforms.variables","title":"variables  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>variables: VariableTransform | None = None\n</code></pre> <p>A <code>VariableTransform</code> object that defines the transformation for variables.</p> <p>If <code>None</code>, no transformation is applied to variables.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.OptModelTransforms.objectives","title":"objectives  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>objectives: ObjectiveTransform | None = None\n</code></pre> <p>An <code>ObjectiveTransform</code> object that defines the transformation for objectives.</p> <p>If <code>None</code>, no transformation is applied to objectives.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.OptModelTransforms.nonlinear_constraints","title":"nonlinear_constraints  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nonlinear_constraints: (\n    NonLinearConstraintTransform | None\n) = None\n</code></pre> <p>A <code>NonLinearConstraintTransform</code> object that defines the transformation for nonlinear constraints.</p> <p>If <code>None</code>, no transformation is applied to nonlinear constraints.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform","title":"ropt.transforms.base.VariableTransform","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for variable transformations.</p> <p>This class defines the interface for transforming variables between the user-defined domain and the optimizer's internal domain. Concrete implementations of this class handle the specific logic for each type of transformation.</p> <p>When implementing a variable transformation, the following aspects must be considered:</p> <ul> <li>Variable Value Transformation: Mapping variable values between the   user and optimizer domains. This is achieved by overriding the   <code>to_optimizer</code>   and   <code>from_optimizer</code>   methods.</li> <li>Perturbation Magnitude Transformation: Stochastic gradient-based   algorithms use perturbations with specified magnitudes (see   <code>perturbation_magnitudes</code>). These magnitudes   are typically defined in the user domain and must be transformed to the   optimizer domain using the   <code>magnitudes_to_optimizer</code>   method.</li> <li>Bound Constraint Difference Transformation: To report violations of   variable bounds, the differences between variable values and their   lower/upper bounds must be transformed from the optimizer domain back   to the user domain. This is done using the   <code>bound_constraint_diffs_from_optimizer</code>   method.</li> <li>Linear Constraint Transformation: Linear constraints are generally   defined by coefficients and right-hand-side values in the user domain.   These must be transformed to the optimizer domain using the   <code>linear_constraints_to_optimizer</code>   method.</li> <li>Linear Constraint Difference Transformation: To report violations of   linear constraints, the differences between the linear constraint   values and their right-hand-side values must be transformed back to the   user domain. This is done using the   <code>linear_constraints_diffs_from_optimizer</code>   method.</li> </ul>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.to_optimizer","title":"to_optimizer  <code>abstractmethod</code>","text":"<pre><code>to_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform values from the user domain to the optimizer domain.</p> <p>This method maps variable values from the user-defined domain to the optimizer's internal domain. This transformation might involve scaling, shifting, or other operations to improve the optimizer's performance.</p> <p>The input <code>values</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the variable values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The variable values in the user domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed variable values in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.from_optimizer","title":"from_optimizer  <code>abstractmethod</code>","text":"<pre><code>from_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform values from the optimizer domain to the user domain.</p> <p>This method maps variable values from the optimizer's internal domain back to the user-defined domain. This transformation reverses any scaling, shifting, or other operations that were applied to improve the optimizer's performance.</p> <p>The input <code>values</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the variable values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The variable values in the optimizer domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed variable values in the user domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.magnitudes_to_optimizer","title":"magnitudes_to_optimizer  <code>abstractmethod</code>","text":"<pre><code>magnitudes_to_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform perturbation magnitudes to the optimizer domain.</p> <p>This method transforms perturbation magnitudes, typically used in stochastic gradient-based algorithms, from the user-defined domain to the optimizer's internal domain. The transformation ensures that the perturbations are applied correctly in the optimizer's space, which may have different scaling or units than the user domain.</p> <p>For example, if variables are scaled down in the optimizer domain, the perturbation magnitudes should also be scaled down proportionally.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The perturbation magnitudes in the user domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed perturbation magnitudes in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.bound_constraint_diffs_from_optimizer","title":"bound_constraint_diffs_from_optimizer  <code>abstractmethod</code>","text":"<pre><code>bound_constraint_diffs_from_optimizer(\n    lower_diffs: NDArray[float64],\n    upper_diffs: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform bound constraint differences to the user domain.</p> <p>This method transforms the differences between variable values and their lower/upper bounds from the optimizer's internal domain back to the user-defined domain. These differences are used to report constraint violations.</p> <p>For example, if variables are scaled in the optimizer domain, the differences between the variables and their bounds must be scaled back to the user domain to accurately reflect the constraint violations in the user's original units.</p> <p>Parameters:</p> Name Type Description Default <code>lower_diffs</code> <code>NDArray[float64]</code> <p>The differences between the variable values and their lower bounds in the optimizer domain.</p> required <code>upper_diffs</code> <code>NDArray[float64]</code> <p>The differences between the variable values and their upper bounds in the optimizer domain.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed differences.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.linear_constraints_to_optimizer","title":"linear_constraints_to_optimizer","text":"<pre><code>linear_constraints_to_optimizer(\n    coefficients: NDArray[float64],\n    lower_bounds: NDArray[float64],\n    upper_bounds: NDArray[float64],\n) -&gt; tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]\n</code></pre> <p>Transform linear constraints from the user domain to the optimizer domain.</p> <p>This method transforms linear constraints, defined by their coefficients and right-hand-side bounds, from the user-defined domain to the optimizer's internal domain. This is essential to maintain the validity of the constraints after variable transformations.</p> <p>For instance, if variables are scaled or shifted in the optimizer domain, the coefficients and bounds of the linear constraints must be adjusted accordingly to ensure the constraints remain consistent.</p> <p>The linear constraints are defined by the equation <code>A * x = b</code>, where <code>A</code> is the coefficient matrix, <code>x</code> is the variable vector, and <code>b</code> represents the right-hand-side bounds.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>NDArray[float64]</code> <p>The coefficient matrix.</p> required <code>lower_bounds</code> <code>NDArray[float64]</code> <p>The lower bounds on the right-hand-side values.</p> required <code>upper_bounds</code> <code>NDArray[float64]</code> <p>The upper bounds on the right-hand-side values.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed coefficient matrix and bounds.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.VariableTransform.linear_constraints_diffs_from_optimizer","title":"linear_constraints_diffs_from_optimizer","text":"<pre><code>linear_constraints_diffs_from_optimizer(\n    lower_diffs: NDArray[float64],\n    upper_diffs: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform linear constraint differences to the user domain.</p> <p>This method transforms the differences between linear constraint values and their lower/upper bounds from the optimizer's internal domain back to the user-defined domain. These differences are used to report constraint violations.</p> <p>For example, if linear constraints are scaled in the optimizer domain, the differences between the constraint values and their bounds must be scaled back to the user domain to accurately reflect the constraint violations in the user's original units.</p> <p>Parameters:</p> Name Type Description Default <code>lower_diffs</code> <code>NDArray[float64]</code> <p>The differences between the linear constraint values and their lower bounds.</p> required <code>upper_diffs</code> <code>NDArray[float64]</code> <p>The differences between the linear constraint values and their upper bounds.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed lower and upper differences.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.ObjectiveTransform","title":"ropt.transforms.base.ObjectiveTransform","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for objective transformations.</p> <p>This class defines the interface for transforming objective values between the user-defined domain and the optimizer's internal domain. Concrete implementations of this class handle the specific logic for each type of objective transformation. This is achieved by overriding the <code>to_optimizer</code> and <code>from_optimizer</code> methods.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.ObjectiveTransform.to_optimizer","title":"to_optimizer  <code>abstractmethod</code>","text":"<pre><code>to_optimizer(\n    objectives: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform objective values to the optimizer domain.</p> <p>This method maps objective values from the user-defined domain to the optimizer's internal domain. This transformation might involve scaling, shifting, or other operations to improve the optimizer's performance.</p> <p>The input <code>objectives</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the objective values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>NDArray[float64]</code> <p>The objective values in the user domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed objective values in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.ObjectiveTransform.from_optimizer","title":"from_optimizer  <code>abstractmethod</code>","text":"<pre><code>from_optimizer(\n    objectives: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform objective values to the user domain.</p> <p>This method maps objective values from the optimizer's internal domain back to the user-defined domain. This transformation reverses any scaling, shifting, or other operations that were applied to improve the optimizer's performance.</p> <p>The input <code>objectives</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the objective values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>NDArray[float64]</code> <p>The objective values in the optimizer domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed objective values in the user domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.NonLinearConstraintTransform","title":"ropt.transforms.base.NonLinearConstraintTransform","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for nonlinear constraint transformations.</p> <p>This class defines the interface for transforming nonlinear constraint values between the user-defined domain and the optimizer's internal domain. Concrete implementations of this class handle the specific logic for each type of nonlinear constraint transformation.</p> <p>When implementing a nonlinear constraint transformation, the following aspects must be considered:</p> <ul> <li>Constraint Value Transformation: Mapping constraint values between the   user and optimizer domains. This is achieved by overriding the   <code>to_optimizer</code>   and   <code>from_optimizer</code>   methods.</li> <li>Right-Hand-Side Bound Transformation: Mapping the right-hand-side   bounds of the constraints between the user and optimizer domains. This is   achieved by overriding the   <code>bounds_to_optimizer</code>   method.</li> <li>Constraint Difference Transformation: To report violations of   nonlinear constraints, the differences between constraint values and their   lower/upper bounds must be transformed from the optimizer domain back to   the user domain. This is done using the   <code>nonlinear_constraint_diffs_from_optimizer</code>   method.</li> </ul>"},{"location":"reference/domain_transforms/#ropt.transforms.base.NonLinearConstraintTransform.to_optimizer","title":"to_optimizer  <code>abstractmethod</code>","text":"<pre><code>to_optimizer(\n    constraints: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform constraint values to the optimizer domain.</p> <p>This method maps nonlinear constraint values from the user-defined domain to the optimizer's internal domain. This transformation might involve scaling, shifting, or other operations to improve the optimizer's performance.</p> <p>The input <code>constraints</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the constraint values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>NDArray[float64]</code> <p>The nonlinear constraint values in the user domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed nonlinear constraint values in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.NonLinearConstraintTransform.from_optimizer","title":"from_optimizer  <code>abstractmethod</code>","text":"<pre><code>from_optimizer(\n    constraints: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform constraint values to the user domain.</p> <p>This method maps nonlinear constraint values from the optimizer's internal domain back to the user-defined domain. This transformation reverses any scaling, shifting, or other operations that were applied to improve the optimizer's performance.</p> <p>The input <code>constraints</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the constraint values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>NDArray[float64]</code> <p>The nonlinear constraint values in the optimizer domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed nonlinear constraint values in the user domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.NonLinearConstraintTransform.bounds_to_optimizer","title":"bounds_to_optimizer  <code>abstractmethod</code>","text":"<pre><code>bounds_to_optimizer(\n    lower_bounds: NDArray[float64],\n    upper_bounds: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform the right-hand-side bounds to the optimizer domain.</p> <p>This method transforms the lower and upper bounds of the nonlinear constraints from the user-defined domain to the optimizer's internal domain. This transformation is necessary to ensure that the constraints remain valid after the variables have been transformed.</p> <p>For example, if constraint values are scaled or shifted in the optimizer domain, the bounds must be adjusted accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bounds</code> <code>NDArray[float64]</code> <p>The lower bounds on the right-hand-side values in the user domain.</p> required <code>upper_bounds</code> <code>NDArray[float64]</code> <p>The upper bounds on the right-hand-side values in the user domain.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed bounds.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.base.NonLinearConstraintTransform.nonlinear_constraint_diffs_from_optimizer","title":"nonlinear_constraint_diffs_from_optimizer  <code>abstractmethod</code>","text":"<pre><code>nonlinear_constraint_diffs_from_optimizer(\n    lower_diffs: NDArray[float64],\n    upper_diffs: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform nonlinear constraint differences to the user domain.</p> <p>This method transforms the differences between nonlinear constraint values and their lower/upper bounds from the optimizer's internal domain back to the user-defined domain. These differences are used to report constraint violations.</p> <p>For example, if constraint values are scaled in the optimizer domain, the differences between the constraint values and their bounds must be scaled back to the user domain to accurately reflect the constraint violations in the user's original units.</p> <p>Parameters:</p> Name Type Description Default <code>lower_diffs</code> <code>NDArray[float64]</code> <p>The differences between the nonlinear constraint values and their lower bounds.</p> required <code>upper_diffs</code> <code>NDArray[float64]</code> <p>The differences between the nonlinear constraint values and their upper bounds.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed lower and upper differences.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler","title":"ropt.transforms.VariableScaler","text":"<p>               Bases: <code>VariableTransform</code></p> <p>Linearly scales and shifts variables between domains.</p> <p>This class implements a linear transformation for variables, allowing for scaling and shifting between the user-defined domain and the optimizer's internal domain. The transformation is defined by a scaling factor and an offset for each variable.</p> <p>The transformation from the user domain to the optimizer domain is given by:</p> \\[x_{opt} = \\frac{(x_{\\textrm{user}} - \\textrm{offset})}{\\textrm{scale}}\\] <p>The transformation from the optimizer domain back to the user domain is:</p> \\[x_{user} = x_{\\textrm{opt}} * {\\textrm{scale}} + {\\textrm{offset}}\\] <p>This transformation can be used to improve the performance of the optimizer by working with variables that are scaled to a more suitable range or centered around a specific value.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.__init__","title":"__init__","text":"<pre><code>__init__(\n    scales: NDArray[float64] | None,\n    offsets: NDArray[float64] | None,\n) -&gt; None\n</code></pre> <p>Initialize the variable scaler.</p> <p>This scaler applies a linear transformation to variables, defined by scaling factors and offset values.</p> <p>If both <code>scales</code> and <code>offsets</code> are provided, they are broadcasted to ensure they have the same length.</p> <p>Parameters:</p> Name Type Description Default <code>scales</code> <code>NDArray[float64] | None</code> <p>The scaling factors for each variable.</p> required <code>offsets</code> <code>NDArray[float64] | None</code> <p>The offset values for each variable.</p> required"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.to_optimizer","title":"to_optimizer","text":"<pre><code>to_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform variable values to the optimizer domain.</p> <p>This method applies the linear scaling and offset transformation to variable values, mapping them from the user-defined domain to the optimizer's internal domain.</p> <p>The transformation is defined as: <code>x_opt = (x_user - offset) / scale</code>.</p> <p>The input <code>values</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the variable values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The variable values in the user domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed variable values in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.from_optimizer","title":"from_optimizer","text":"<pre><code>from_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform variable values to the user domain.</p> <p>This method applies the inverse linear scaling and offset transformation to variable values, mapping them from the optimizer's internal domain back to the user-defined domain.</p> <p>The transformation is defined as: <code>x_user = x_opt * scale + offset</code>.</p> <p>The input <code>values</code> may be a multi-dimensional array. It is assumed that the last axis of the array represents the variable values. If this is not the case, you must adjust the order of the axes before and after calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The variable values in the optimizer domain to be transformed.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed variable values in the user domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.magnitudes_to_optimizer","title":"magnitudes_to_optimizer","text":"<pre><code>magnitudes_to_optimizer(\n    values: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Transform perturbation magnitudes to the optimizer domain.</p> <p>This method transforms perturbation magnitudes, typically used in stochastic gradient-based algorithms, from the user-defined domain to the optimizer's internal domain. The transformation ensures that the perturbations are applied correctly in the optimizer's space, which may have different scaling or units than the user domain.</p> <p>The transformation is defined as: <code>x_opt = x_user / scale</code>.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>The perturbation magnitudes in the user domain.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The transformed perturbation magnitudes in the optimizer domain.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.linear_constraints_to_optimizer","title":"linear_constraints_to_optimizer","text":"<pre><code>linear_constraints_to_optimizer(\n    coefficients: NDArray[float64],\n    lower_bounds: NDArray[float64],\n    upper_bounds: NDArray[float64],\n) -&gt; tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]\n</code></pre> <p>Transform linear constraints to the optimizer domain.</p> <p>This method transforms linear constraints, defined by their coefficients and right-hand-side bounds, from the user-defined domain to the optimizer's internal domain. This transformation accounts for the scaling and shifting applied to the variables and ensures that the constraints remain valid in the optimizer's space.</p> <p>The set of linear constraints can be represented by a matrix equation: \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\).</p> <p>When linearly transforming variables to the optimizer domain, the coefficients (\\(\\mathbf{A}\\)) and right-hand-side values (\\(\\mathbf{b}\\)) must be converted to remain valid (see also the configuration for linear constraints). If the linear transformation of the variables to the optimizer domain is given by:</p> \\[ \\hat{\\mathbf{x}} = \\mathbf{S} \\mathbf{x} + \\mathbf{o}\\] <p>then the coefficients and right-hand-side values must be transformed as follows:</p> \\[ \\begin{align}     \\hat{\\mathbf{A}} &amp;= \\mathbf{A} \\mathbf{S}^{-1} \\\\ \\hat{\\mathbf{b}}     &amp;= \\mathbf{b} + \\mathbf{A}\\mathbf{S}^{-1}\\mathbf{o} \\end{align}\\] <p>where \\(S\\) is a diagonal matrix with scaling factors on the diagonal and \\(o\\) are the offsets.</p> <p>The resulting equations are further scaled by dividing them by maximum of the absolute values of the coefficients in each equation.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>NDArray[float64]</code> <p>The coefficient matrix of the linear constraints.</p> required <code>lower_bounds</code> <code>NDArray[float64]</code> <p>The lower bounds on the right-hand-side values.</p> required <code>upper_bounds</code> <code>NDArray[float64]</code> <p>The upper bounds on the right-hand-side values.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed coefficient matrix and bounds.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.bound_constraint_diffs_from_optimizer","title":"bound_constraint_diffs_from_optimizer","text":"<pre><code>bound_constraint_diffs_from_optimizer(\n    lower_diffs: NDArray[float64],\n    upper_diffs: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform bound constraint differences to the user domain.</p> <p>This method transforms the differences between variable values and their lower/upper bounds from the optimizer's internal domain back to the user-defined domain. These differences are used to report constraint violations.</p> <p>For example, if variables are scaled in the optimizer domain, the differences between the variables and their bounds must be scaled back to the user domain to accurately reflect the constraint violations in the user's original units.</p> <p>The transformation is defined as: <code>x_user = x_opt * scale</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lower_diffs</code> <code>NDArray[float64]</code> <p>The differences between the variable values and their lower bounds.</p> required <code>upper_diffs</code> <code>NDArray[float64]</code> <p>The differences between the variable values and their upper bounds.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed lower and upper differences.</p>"},{"location":"reference/domain_transforms/#ropt.transforms.VariableScaler.linear_constraints_diffs_from_optimizer","title":"linear_constraints_diffs_from_optimizer","text":"<pre><code>linear_constraints_diffs_from_optimizer(\n    lower_diffs: NDArray[float64],\n    upper_diffs: NDArray[float64],\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <p>Transform linear constraint differences to the user domain.</p> <p>This method transforms the differences between linear constraint values and their lower/upper bounds from the optimizer's internal domain back to the user-defined domain. These differences are used to report constraint violations.</p> <p>This is implemented by re-scaling the equations with the weights that were determined and stored by the <code>linear_constraints_to_optimizer</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>lower_diffs</code> <code>NDArray[float64]</code> <p>The differences between the linear constraint values and their lower bounds.</p> required <code>upper_diffs</code> <code>NDArray[float64]</code> <p>The differences between the linear constraint values and their upper bounds.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64]]</code> <p>A tuple containing the transformed lower and upper differences.</p>"},{"location":"reference/enopt_config/","title":"Configuration","text":""},{"location":"reference/enopt_config/#ropt.config","title":"ropt.config","text":"<p>The <code>ropt.config</code> module provides configuration classes for optimization workflows.</p> <p>This module defines a set of classes that are used to configure various aspects of an optimization process, including variables, objectives, constraints, realizations, samplers, and more.</p> <p>The central configuration class for optimization is <code>EnOptConfig</code>, which encapsulates the complete configuration for a single optimization run. It is designed to be flexible and extensible, allowing users to customize the optimization process to their specific needs.</p> <p>These configuration classes are built using <code>pydantic</code>, which provides robust data validation and parsing capabilities. This ensures that the configuration data is consistent and adheres to the expected structure.</p> <p>Configuration objects are typically created from dictionaries of configuration values using the <code>model_validate</code> method provided by <code>pydantic</code>.</p> <p>Key Features:</p> <ul> <li>Modular Design: The configuration is broken down into smaller, manageable   components, each represented by a dedicated class.</li> <li>Validation: <code>pydantic</code> ensures that the configuration data is valid and   consistent.</li> <li>Extensibility: The modular design allows for easy extension and   customization of the optimization process.</li> <li>Centralized Configuration: The   <code>EnOptConfig</code> class provides a single point   of entry for configuring an optimization run.</li> </ul> <p>Parsing and Validation</p> <p>The configuration classes are built using <code>pydantic</code>, which provides robust data validation. The primary configuration class is <code>EnOptConfig</code>, and it contains nested configuration classes for various aspects of the optimization. To parse a configuration from a dictionary, use the <code>model_validate</code> class method. ```</p>"},{"location":"reference/enopt_config/#ropt.config.EnOptConfig","title":"EnOptConfig","text":"<p>The primary configuration class for an optimization run.</p> <p><code>EnOptConfig</code> orchestrates the configuration of an entire optimization workflow. It contains nested configuration classes that define specific aspects of the optimization, such as variables, objectives, constraints, realizations, and the optimizer itself.</p> <p><code>realization_filters</code>, <code>function_estimators</code>, and <code>samplers</code> are configured as tuples. Other configuration fields reference these objects by their index within the tuples. This makes it possible to share these objects between entities. For example, <code>VariablesConfig</code> has a <code>samplers</code> field, which is an array of indices specifying the sampler to use for each variable. If only a single sampler is needed, the <code>samplers</code> field in <code>EnOptConfig</code> should contain a single sampler configuration, and the <code>samplers</code> field in the <code>VariablesConfig</code> configuration contains only zeros to specify that each variable should use this entry. In case of multiple samplers, multiple sampler configurations are defined, and each entry in <code>samplers</code> array in <code>VariablesConfig</code> points to desired sampler.</p> <p>The optional <code>names</code> attribute is a dictionary that stores the names of the various entities, such as variables, objectives, and constraints. The supported name types are defined in the <code>AxisName</code> enumeration. This information is optional, as it is not strictly necessary for the optimization, but it can be useful for labeling and interpreting results. For instance, when present, it is used to create a multi-index results that are exported as data frames.</p> Info <p>Many nested configuration classes use <code>numpy</code> arrays. These arrays typically have a size determined by a configured property (e.g., the number of variables) or a size of one. In the latter case, the single value is broadcasted to all relevant elements. For example, <code>VariablesConfig</code> defines properties like initial values and bounds as <code>numpy</code> arrays, which must either match the number of variables or have a size of one.</p> Warning <p><code>EnOptConfig</code> objects are immutable and hold the in-memory configuration for an optimization run. For persistence, do not serialize the object itself. Instead, store the original dictionary used for its creation.</p> <p>Round-trip serialization (e.g., to/from JSON) is not a supported use case and may lead to data loss due to the complex types it contains, such as <code>numpy</code> arrays, or to unexpected behavior because of the transformations that are applied to the input upon creation.</p> <p>Attributes:</p> Name Type Description <code>variables</code> <code>VariablesConfig</code> <p>Configuration for the optimization variables.</p> <code>objectives</code> <code>ObjectiveFunctionsConfig</code> <p>Configuration for the objective functions.</p> <code>linear_constraints</code> <code>LinearConstraintsConfig | None</code> <p>Configuration for linear constraints.</p> <code>nonlinear_constraints</code> <code>NonlinearConstraintsConfig | None</code> <p>Configuration for non-linear constraints.</p> <code>realizations</code> <code>RealizationsConfig</code> <p>Configuration for the realizations.</p> <code>optimizer</code> <code>OptimizerConfig</code> <p>Configuration for the optimization algorithm.</p> <code>gradient</code> <code>GradientConfig</code> <p>Configuration for gradient calculations.</p> <code>realization_filters</code> <code>tuple[RealizationFilterConfig, ...]</code> <p>Configuration for realization filters.</p> <code>function_estimators</code> <code>tuple[FunctionEstimatorConfig, ...]</code> <p>Configuration for function estimators.</p> <code>samplers</code> <code>tuple[SamplerConfig, ...]</code> <p>Configuration for samplers.</p> <code>names</code> <code>dict[str, tuple[str | int, ...]]</code> <p>Optional mapping of axis types to names.</p>"},{"location":"reference/enopt_config/#ropt.config.VariablesConfig","title":"VariablesConfig","text":"<p>Configuration class for optimization variables.</p> <p>This class, <code>VariablesConfig</code>, defines the configuration for optimization variables. It is used in an <code>EnOptConfig</code> object to specify the initial values, bounds, types, and an optional mask for the variables.</p> <p>The <code>variables</code> field is required and determines the number of variables, including both free and fixed variables.</p> <p>The <code>lower_bounds</code> and <code>upper_bounds</code> fields define the bounds for each variable. These are also <code>numpy</code> arrays and are broadcasted to match the number of variables. By default, they are set to negative and positive infinity, respectively. <code>numpy.nan</code> values in these arrays indicate unbounded variables and are converted to <code>numpy.inf</code> with the appropriate sign.</p> <p>The optional <code>types</code> field allows assigning a <code>VariableType</code> to each variable. If not provided, all variables are assumed to be continuous real-valued (<code>VariableType.REAL</code>).</p> <p>The optional <code>mask</code> field is a boolean <code>numpy</code> array that indicates which variables are free to change during optimization. <code>True</code> values in the mask indicate that the corresponding variable is free, while <code>False</code> indicates a fixed variable.</p> <p>Variable perturbations</p> <p>The <code>VariablesConfig</code> class also stores information that is needed to generate perturbed variables, for instance to calculate stochastic gradients.</p> <p>Perturbations are generated by sampler objects that are configured separately as a tuple of <code>SamplerConfig</code> objects in the configuration object used by an optimization step. <code>EnOptConfig</code> object defines the available samplers in its <code>samplers</code> field. The <code>samplers</code> field of the <code>VariablesConfig</code> object specifies, for each variable, the the sampler to use by its index into the <code>samplers</code> tuple. A random number generator is created to support samplers that require random numbers.</p> <p>The generated perturbation values are scaled by the values of the <code>perturbation_magnitudes</code> field and can be modified based on the <code>perturbation_types</code>. See <code>PerturbationType</code> for details on available perturbation types.</p> <p>Perturbed variables may violate the defined variable bounds. The <code>boundary_types</code> field specifies how to handle such violations. See <code>BoundaryType</code> for details on available boundary handling methods.</p> <p>The <code>perturbation_types</code> and <code>boundary_types</code> fields use values from the <code>PerturbationType</code> and <code>BoundaryType</code> enumerations, respectively.</p> Seed for Samplers <p>The <code>seed</code> value ensures consistent results across repeated runs with the same configuration. To obtain unique results for each optimization run, modify the seed. A common approach is to use a tuple with a unique ID as the first element, ensuring reproducibility across nested and parallel evaluations.</p> <p>Attributes:</p> Name Type Description <code>types</code> <code>ArrayEnum</code> <p>Optional variable types.</p> <code>variable_count</code> <code>int</code> <p>Number of variables.</p> <code>lower_bounds</code> <code>Array1D</code> <p>Lower bounds for the variables (default: \\(-\\infty\\)).</p> <code>upper_bounds</code> <code>Array1D</code> <p>Upper bounds for the variables (default: \\(+\\infty\\)).</p> <code>mask</code> <code>Array1DBool</code> <p>Optional boolean mask indicating free variables.</p> <code>perturbation_magnitudes</code> <code>Array1D</code> <p>Magnitudes of the perturbations for each variable (default: <code>DEFAULT_PERTURBATION_MAGNITUDE</code>).</p> <code>perturbation_types</code> <code>ArrayEnum</code> <p>Type of perturbation for each variable (see <code>PerturbationType</code>, default: <code>DEFAULT_PERTURBATION_TYPE</code>).</p> <code>boundary_types</code> <code>ArrayEnum</code> <p>How to handle perturbations that violate boundary conditions (see <code>BoundaryType</code>, default: <code>DEFAULT_PERTURBATION_BOUNDARY_TYPE</code>).</p> <code>samplers</code> <code>Array1DInt</code> <p>Indices of the samplers to use for each variable.</p> <code>seed</code> <code>ItemOrTuple[int]</code> <p>Seed for the random number generator used by the samplers.</p>"},{"location":"reference/enopt_config/#ropt.config.ObjectiveFunctionsConfig","title":"ObjectiveFunctionsConfig","text":"<p>Configuration class for objective functions.</p> <p>This class, <code>ObjectiveFunctionsConfig</code>, defines the configuration for objective functions. for instance, as part of an <code>EnOptConfig</code> object.</p> <p><code>ropt</code> supports multi-objective optimization. Multiple objectives are combined into a single value by summing them after weighting. The <code>weights</code> field, a <code>numpy</code> array, determines the weight of each objective function. The length of this array defines the number of objective functions. The weights are automatically normalized to sum to 1 (e.g., <code>[1, 1]</code> becomes <code>[0.5, 0.5]</code>).</p> <p>Objective functions can optionally be processed using <code>realization filters</code> and <code>function estimators</code>.The <code>realization_filters</code> and <code>function_estimators</code> attributes, if provided, must be arrays of integer indices. Each index in the <code>realization_filters</code> array corresponds to a objective (by position) and specifies which filter to use. The available filters must be defined elsewhere as a tuple of realization filter configurations. For instance, for optimization these are defined in the <code>EnOptConfig.realization_filters</code> configuration class. The same logic applies to the <code>function_estimators</code> array . If an index is invalid (e.g., out of bounds for the corresponding object tuple), no filter or estimator is applied to that specific objective. If these attributes are not provided (<code>None</code>), no filters or estimators are applied at all.</p> <p>Attributes:</p> Name Type Description <code>weights</code> <code>Array1D</code> <p>Weights for the objective functions (default: 1.0).</p> <code>realization_filters</code> <code>Array1DInt</code> <p>Optional indices of realization filters.</p> <code>function_estimators</code> <code>Array1DInt</code> <p>Optional indices of function estimators.</p>"},{"location":"reference/enopt_config/#ropt.config.LinearConstraintsConfig","title":"LinearConstraintsConfig","text":"<p>Configuration class for linear constraints.</p> <p>This class, <code>LinearConstraintsConfig</code>, defines linear constraints used in an optimization, for instance as part of an <code>EnOptConfig</code> object.</p> <p>Linear constraints are defined by a set of linear equations involving the optimization variables. These equations can represent equality or inequality constraints. The <code>coefficients</code> field is a 2D <code>numpy</code> array where each row represents a constraint, and each column corresponds to a variable.</p> <p>The <code>lower_bounds</code> and <code>upper_bounds</code> fields specify the bounds on the right-hand side of each constraint equation. These fields are converted and broadcasted to <code>numpy</code> arrays with a length equal to the number of constraint equations.</p> <p>Less-than and greater-than inequality constraints can be specified by setting the lower bounds to \\(-\\infty\\), or the upper bounds to \\(+\\infty\\), respectively. Equality constraints are specified by setting the lower bounds equal to the upper bounds.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>Array2D</code> <p>Matrix of coefficients for the linear constraints.</p> <code>lower_bounds</code> <code>Array1D</code> <p>Lower bounds for the right-hand side of the constraint equations.</p> <code>upper_bounds</code> <code>Array1D</code> <p>Upper bounds for the right-hand side of the constraint equations.</p> Linear transformation of variables. <p>The set of linear constraints can be represented by a matrix equation: \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\).</p> <p>When linearly transforming variables to the optimizer domain, the coefficients (\\(\\mathbf{A}\\)) and right-hand-side values (\\(\\mathbf{b}\\)) must be converted to remain valid. If the linear transformation of the variables to the optimizer domain is given by:</p> \\[ \\hat{\\mathbf{x}} = \\mathbf{S} \\mathbf{x} + \\mathbf{o}\\] <p>then the coefficients and right-hand-side values must be transformed as follows:</p> \\[ \\begin{align}     \\hat{\\mathbf{A}} &amp;= \\mathbf{A} \\mathbf{S}^{-1} \\\\ \\hat{\\mathbf{b}}     &amp;= \\mathbf{b} + \\mathbf{A}\\mathbf{S}^{-1}\\mathbf{o} \\end{align}\\]"},{"location":"reference/enopt_config/#ropt.config.NonlinearConstraintsConfig","title":"NonlinearConstraintsConfig","text":"<p>Configuration class for non-linear constraints.</p> <p>This class, <code>NonlinearConstraintsConfig</code>, defines non-linear constraints , for instance as part of an <code>EnOptConfig</code> object.</p> <p>Non-linear constraints are defined by comparing a constraint function to a right-hand-side value, allowing for equality or inequality constraints. The <code>lower_bounds</code> and <code>upper_bounds</code> fields, which are <code>numpy</code> arrays, specify the bounds on these right-hand-side values. The length of these arrays determines the number of constraint functions.</p> <p>Less-than and greater-than inequality constraints can be specified by setting the lower bounds to \\(-\\infty\\), or the upper bounds to \\(+\\infty\\), respectively. Equality constraints are specified by setting the lower bounds equal to the upper bounds.</p> <p>Non-linear constraints can optionally be processed using <code>realization filters</code> and <code>function estimators</code>.The <code>realization_filters</code> and <code>function_estimators</code> attributes, if provided, must be arrays of integer indices. Each index in the <code>realization_filters</code> array corresponds to a constraint function (by position) and specifies which filter to use. The available filters must be defined elsewhere as a tuple of realization filter configurations. For instance, for optimization these are defined in the <code>EnOptConfig.realization_filters</code> configuration class. The same logic applies to the <code>function_estimators</code> array . If an index is invalid (e.g., out of bounds for the corresponding object tuple), no filter or estimator is applied to that specific constraint function. If these attributes are not provided (<code>None</code>), no filters or estimators are applied at all.</p> <p>Attributes:</p> Name Type Description <code>lower_bounds</code> <code>Array1D</code> <p>Lower bounds for the right-hand-side values.</p> <code>upper_bounds</code> <code>Array1D</code> <p>Upper bounds for the right-hand-side values.</p> <code>realization_filters</code> <code>Array1DInt</code> <p>Optional indices of realization filters.</p> <code>function_estimators</code> <code>Array1DInt</code> <p>Optional indices of function estimators.</p>"},{"location":"reference/enopt_config/#ropt.config.RealizationsConfig","title":"RealizationsConfig","text":"<p>Configuration class for realizations.</p> <p>This class, <code>RealizationsConfig</code>, defines the configuration for realizations used when calculating objectives and constraints.</p> <p>To optimize an ensemble of functions, a set of realizations is defined. When the optimizer requests a function value or a gradient, these are calculated for each realization and then combined into a single value. Typically, this combination is a weighted sum, but other methods are possible.</p> <p>The <code>weights</code> field, a <code>numpy</code> array, determines the weight of each realization. The length of this array defines the number of realizations. The weights are automatically normalized to sum to 1 (e.g., <code>[1, 1]</code> becomes <code>[0.5, 0.5]</code>).</p> <p>If function value calculations for some realizations fail (e.g., due to a simulation error), the total function and gradient values can still be calculated by excluding the missing values. However, a minimum number of successful realizations may be required. The <code>realization_min_success</code> field specifies this minimum. By default, it is set equal to the number of realizations, meaning no missing values are allowed.</p> Note <p>Setting <code>realization_min_success</code> to zero allows the optimization to proceed even if all realizations fail. While some optimizers can handle this, most will treat it as if the value were one, requiring at least one successful realization.</p> <p>Attributes:</p> Name Type Description <code>weights</code> <code>Array1D</code> <p>Weights for the realizations (default: 1.0).</p> <code>realization_min_success</code> <code>NonNegativeInt | None</code> <p>Minimum number of successful realizations (default:                     equal to the number of realizations).</p>"},{"location":"reference/enopt_config/#ropt.config.OptimizerConfig","title":"OptimizerConfig","text":"<p>Configuration class for the optimization algorithm.</p> <p>This class, <code>OptimizerConfig</code>, defines the configuration for the optimization algorithm used in an <code>EnOptConfig</code> object.</p> <p>While optimization methods can have diverse parameters, this class provides a standardized set of settings that are commonly used and forwarded to the optimizer:</p> <ul> <li><code>max_iterations</code>: The maximum number of iterations allowed. The   optimizer may choose to ignore this.</li> <li><code>max_functions</code>: The maximum number of function evaluations allowed.</li> <li><code>max_batches</code>: The maximum number of evaluations batches allowed. The   optimizer callback may ask to evaluate a batch of multiple functions and   gradients at once. This setting will limit the number of those calls.</li> <li><code>tolerance</code>: The convergence tolerance used as a stopping criterion.   The exact definition depends on the optimizer, and it may be ignored.</li> <li><code>parallel</code>: If <code>True</code>, allows the optimizer to use parallelized   function evaluations. This typically applies to gradient-free methods and   may be ignored.</li> <li><code>output_dir</code>: An optional output directory where the optimizer can   store files.</li> <li><code>options</code>: A dictionary or list of strings for generic optimizer   options. The required format and interpretation depend on the specific   optimization method.</li> <li><code>stdout</code>: Redirect optimizer standard output to the given file.</li> <li><code>stderr</code>: Redirect optimizer standard error to the given file.</li> </ul> Differences between <code>max_iterations</code>, <code>max_functions</code>, and <code>max_batches</code> <p>These three parameters provide different ways to limit the duration or computational cost of the optimization process:</p> <ul> <li> <p><code>max_iterations</code>: This limit is passed directly to the backend   optimization algorithm. Many optimizers define an \"iteration\" as a   distinct step in their process, which might involve one or more   function or gradient evaluations. The interpretation of <code>max_iterations</code>   depends on the specific backend optimizer; it typically caps the number   of these internal iterations. Some backends might ignore this setting if   they don't have a clear concept of iterations.</p> </li> <li> <p><code>max_batches</code>: This limit restricts the total number of calls made   to the evaluation function provided to <code>ropt</code>. An optimizer might request   a batch containing multiple function and/or gradient evaluations within   a single call. <code>max_batches</code> limits how many such batch requests are   processed sequentially. This is particularly useful for managing resource   usage when batches are evaluated in parallel (e.g., on an HPC cluster),   as it controls the number of sequential submission steps. The number of   batches does not necessarily correspond directly to the number of   optimizer iterations, especially if function and gradient evaluations   occur in separate batches.</p> </li> <li> <p><code>max_functions</code>: This imposes a hard limit on the total number of   individual objective function evaluations performed across all batches.   Since a single batch evaluation (limited by <code>max_batches</code>) can involve   multiple function evaluations, setting <code>max_functions</code> provides more   granular control over the total computational effort spent on function   calls. It can serve as an alternative stopping criterion if the backend   optimizer doesn't support <code>max_iterations</code> or if you need to strictly   limit the function evaluation count. Note that exceeding this limit might   cause the optimization to terminate mid-batch, potentially earlier than   a corresponding <code>max_batches</code> limit would.</p> </li> </ul> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>Name of the optimization method.</p> <code>max_iterations</code> <code>PositiveInt | None</code> <p>Maximum number of iterations (optional).</p> <code>max_functions</code> <code>PositiveInt | None</code> <p>Maximum number of function evaluations (optional).</p> <code>max_batches</code> <code>PositiveInt | None</code> <p>Maximum number of batch evaluations (optional).</p> <code>tolerance</code> <code>NonNegativeFloat | None</code> <p>Convergence tolerance (optional).</p> <code>parallel</code> <code>bool</code> <p>Allow parallelized function evaluations (default: <code>False</code>).</p> <code>output_dir</code> <code>Path | None</code> <p>Output directory for the optimizer (optional).</p> <code>options</code> <code>dict[str, Any] | list[str] | None</code> <p>Generic options for the optimizer (optional).</p> <code>stdout</code> <code>Path | None</code> <p>File to redirect optimizer standard output (optional).</p> <code>stderr</code> <code>Path | None</code> <p>File to redirect optimizer standard error (optional).</p>"},{"location":"reference/enopt_config/#ropt.config.GradientConfig","title":"GradientConfig","text":"<p>Configuration class for gradient calculations.</p> <p>This class, <code>GradientConfig</code>, defines the configuration of gradient calculations. It is used in an <code>EnOptConfig</code> object as the <code>gradient</code> field to specify how gradients are calculated in gradient-based optimizers.</p> <p>Gradients are estimated using function values calculated from perturbed variables and the unperturbed variables. The <code>number_of_perturbations</code> field determines the number of perturbed variables used, which must be at least one.</p> <p>If function evaluations for some perturbed variables fail, the gradient may still be estimated as long as a minimum number of evaluations succeed. The <code>perturbation_min_success</code> field specifies this minimum. By default, it equals <code>number_of_perturbations</code>.</p> <p>Gradients are calculated for each realization individually and then combined into a total gradient. If <code>number_of_perturbations</code> is low, or even just one, individual gradient calculations may be unreliable. In this case, setting <code>merge_realizations</code> to <code>True</code> directs the optimizer to combine the results of all realizations directly into a single gradient estimate.</p> <p>The <code>evaluation_policy</code> option controls how and when objective functions and gradients are calculated. It accepts one of three string values:</p> <ul> <li><code>\"speculative\"</code>: Evaluate the gradient whenever the objective function     is requested, even if the optimizer hasn't explicitly asked for the     gradient at that point. This approach can potentially improve load     balancing on HPC clusters by initiating gradient work earlier, though     its effectiveness depends on whether the optimizer can utilize these     speculatively computed gradients.</li> <li><code>\"separate\"</code>: Always launch function and gradient evaluations as     distinct operations, even if the optimizer requests both simultaneously.     This is particularly useful when employing realization filters (see     <code>RealizationFilterConfig</code>) that     might disable certain realizations, as it can potentially reduce the     number of gradient evaluations needed.</li> <li><code>\"auto\"</code>: Evaluate functions and/or gradients strictly according to the     optimizer's requests. Calculations are performed only when the     optimization algorithm explicitly requires them.</li> </ul> <p>Attributes:</p> Name Type Description <code>number_of_perturbations</code> <code>PositiveInt</code> <p>Number of perturbations (default: <code>DEFAULT_NUMBER_OF_PERTURBATIONS</code>).</p> <code>perturbation_min_success</code> <code>PositiveInt | None</code> <p>Minimum number of successful function evaluations for perturbed variables (default: equal to <code>number_of_perturbations</code>).</p> <code>merge_realizations</code> <code>bool</code> <p>Merge all realizations for the final gradient calculation (default: <code>False</code>).</p> <code>evaluation_policy</code> <code>Literal['speculative', 'separate', 'auto']</code> <p>How to evaluate functions and gradients.</p>"},{"location":"reference/enopt_config/#ropt.config.FunctionEstimatorConfig","title":"FunctionEstimatorConfig","text":"<p>Configuration class for function estimators.</p> <p>This class, <code>FunctionEstimatorConfig</code>, defines the configuration for function estimators. Function estimators are generally configured as a tuple of <code>FunctionEstimatorConfig</code> objects in a configuration class of an optimization step. For instance, <code>function_estimators</code> field of the <code>EnOptConfig</code> defines the available estimators for the optimization.</p> <p>By default, objective and constraint functions, as well as their gradients, are calculated from individual realizations using a weighted sum. Function estimators provide a way to modify this default calculation.</p> <p>The <code>method</code> field specifies the function estimator method to use for combining the individual realizations. The <code>options</code> field allows passing a dictionary of key-value pairs to further configure the chosen method. The interpretation of these options depends on the selected method.</p> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>Name of the function estimator method.</p> <code>options</code> <code>dict[str, Any]</code> <p>Dictionary of options for the function estimator.</p>"},{"location":"reference/enopt_config/#ropt.config.RealizationFilterConfig","title":"RealizationFilterConfig","text":"<p>Configuration class for realization filters.</p> <p>This class, <code>RealizationFilterConfig</code>, defines the configuration for realization filters. Realization filters are generally configured as a tuple in another configuration object. For instance, the <code>realization_filters</code> field of the <code>EnOptConfig</code> defines the available filters for the optimization.</p> <p>By default, objective and constraint functions, as well as their gradients, are calculated as a weighted function of all realizations. Realization filters provide a way to modify the weights of individual realizations. For example, they can be used to select a subset of realizations for calculating the final objective and constraint functions and their gradients by setting the weights of the other realizations to zero.</p> <p>The <code>method</code> field specifies the realization filter method to use for adjusting the weights. The <code>options</code> field allows passing a dictionary of key-value pairs to further configure the chosen method. The interpretation of these options depends on the selected method.</p> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>Name of the realization filter method.</p> <code>options</code> <code>dict[str, Any]</code> <p>Dictionary of options for the realization filter.</p>"},{"location":"reference/enopt_config/#ropt.config.SamplerConfig","title":"SamplerConfig","text":"<p>Configuration class for samplers.</p> <p>This class, <code>SamplerConfig</code>, defines the configuration for samplers used in an <code>EnOptConfig</code> object. Samplers are configured as a tuple in the <code>samplers</code> field of the <code>EnOptConfig</code>, defining the available samplers for the optimization. The <code>samplers</code> field in the <code>GradientConfig</code> specifies the index of the sampler to use for each variable.</p> <p>Samplers generate perturbations added to variables for gradient calculations. These perturbations can be deterministic or stochastic.</p> <p>The <code>method</code> field specifies the sampler method to use for generating perturbations. The <code>options</code> field allows passing a dictionary of key-value pairs to further configure the chosen method. The interpretation of these options depends on the selected method.</p> <p>By default, each realization uses a different set of perturbed variables. Setting the <code>shared</code> flag to <code>True</code> directs the sampler to use the same set of perturbed values for all realizations.</p> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>Name of the sampler method.</p> <code>options</code> <code>dict[str, Any]</code> <p>Dictionary of options for the sampler.</p> <code>shared</code> <code>bool</code> <p>Whether to share perturbation values between realizations (default: <code>False</code>).</p>"},{"location":"reference/enopt_config/#ropt.config.constants","title":"ropt.config.constants","text":"<p>Default values used by the configuration classes.</p>"},{"location":"reference/enopt_config/#ropt.config.constants.DEFAULT_SEED","title":"DEFAULT_SEED  <code>module-attribute</code>","text":"<pre><code>DEFAULT_SEED: Final = 1\n</code></pre> <p>Default seed for random number generators.</p> <p>The seed is used as the base value for random number generators within various components of the optimization process, such as samplers. Using a consistent seed ensures reproducibility across multiple runs with the same configuration. To obtain unique results for each optimization run, modify this seed.</p>"},{"location":"reference/enopt_config/#ropt.config.constants.DEFAULT_NUMBER_OF_PERTURBATIONS","title":"DEFAULT_NUMBER_OF_PERTURBATIONS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_NUMBER_OF_PERTURBATIONS: Final = 5\n</code></pre> <p>Default number of perturbations for gradient estimation.</p> <p>This value defines the default number of perturbed variables used to estimate gradients. A higher number of perturbations can lead to more accurate gradient estimates but also increases the number of function evaluations required.</p>"},{"location":"reference/enopt_config/#ropt.config.constants.DEFAULT_PERTURBATION_MAGNITUDE","title":"DEFAULT_PERTURBATION_MAGNITUDE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PERTURBATION_MAGNITUDE: Final = 0.005\n</code></pre> <p>Default magnitude for variable perturbations.</p> <p>This value specifies the default value of the scaling factor applied to the perturbation values generated by samplers. The magnitude can be interpreted as an absolute value or as a relative value, depending on the selected perturbation type.</p> <p>See also: <code>PerturbationType</code>.</p>"},{"location":"reference/enopt_config/#ropt.config.constants.DEFAULT_PERTURBATION_BOUNDARY_TYPE","title":"DEFAULT_PERTURBATION_BOUNDARY_TYPE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PERTURBATION_BOUNDARY_TYPE: Final = MIRROR_BOTH\n</code></pre> <p>Default perturbation boundary handling type.</p> <p>This value determines how perturbations that violate the defined variable bounds are handled. The default, <code>BoundaryType.MIRROR_BOTH</code>, mirrors perturbations back into the valid range if they exceed either the lower or upper bound.</p> <p>See also: <code>BoundaryType</code>.</p>"},{"location":"reference/enopt_config/#ropt.config.constants.DEFAULT_PERTURBATION_TYPE","title":"DEFAULT_PERTURBATION_TYPE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_PERTURBATION_TYPE: Final = ABSOLUTE\n</code></pre> <p>Default perturbation type.</p> <p>This value determines how the perturbation magnitude is interpreted. The default, <code>PerturbationType.ABSOLUTE</code>, means that the perturbation magnitude is added directly to the variable value. Other options, such as <code>PerturbationType.RELATIVE</code>, scale the perturbation magnitude based on the variable's bounds.</p> <p>See also: <code>PerturbationType</code>.</p>"},{"location":"reference/enopt_config/#ropt.config.options","title":"ropt.config.options","text":"<p>This module defines utilities for validating plugin options.</p> <p>This module provides classes and functions to define and validate options for plugins. It uses Pydantic to create models that represent the schema of plugin options, allowing for structured and type-safe configuration.</p> <p>Classes:</p> Name Description <code>OptionsSchemaModel</code> <p>Represents the overall schema for plugin options.</p> <code>MethodSchemaModel</code> <p>Represents the schema for a specific method within a plugin, including its name and options.</p>"},{"location":"reference/enopt_config/#ropt.config.options.OptionsSchemaModel","title":"OptionsSchemaModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the overall schema for plugin options.</p> <p>This class defines the structure for describing the methods and options available for a plugin. The methods are described in a list of [<code>MethodSchemaModel][ropt.config.options.MethodSchemaModel</code>] objects, each describing a method supported by the plugin.</p> <p>Attributes:</p> Name Type Description <code>methods</code> <code>dict[str, MethodSchemaModel[Any]]</code> <p>A list of method schemas.</p> <p>Example: <pre><code>from ropt.config.options import OptionsSchemaModel\n\nschema = OptionsSchemaModel.model_validate(\n    {\n        \"methods\": [\n            {\n                \"options\": {\"a\": float}\n            },\n            {\n                \"options\": {\"b\": int | str},\n            },\n        ]\n    }\n)\n\noptions = schema.get_options_model(\"method\")\nprint(options.model_validate({\"a\": 1.0, \"b\": 1}))  # a=1.0 b=1\n</code></pre></p>"},{"location":"reference/enopt_config/#ropt.config.options.OptionsSchemaModel.get_options_model","title":"get_options_model","text":"<pre><code>get_options_model(method: str) -&gt; type[BaseModel]\n</code></pre> <p>Creates a Pydantic model for validating options of a specific method.</p> <p>This method dynamically generates a Pydantic model tailored to validate the options associated with a given method. It iterates through the defined methods, collecting option schemas from those matching the specified <code>method</code> name. The resulting model can then be used to validate dictionaries of options against the defined schema.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The name of the method for which to create the options model.</p> required <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>A Pydantic model class capable of validating options for the specified method.</p>"},{"location":"reference/enopt_config/#ropt.config.options.MethodSchemaModel","title":"MethodSchemaModel","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Represents the schema for a specific method within a plugin.</p> <p>This class defines the structure for describing one or more methods supported by a plugin. It contains a dictionary describing an option for this method.</p> <p>Attributes:</p> Name Type Description <code>options</code> <code>dict[str, T]</code> <p>A list of option dictionaries.</p> <code>url</code> <code>HttpUrl | None</code> <p>An optional URL for the plugin.</p>"},{"location":"reference/enopt_config/#ropt.config.options.gen_options_table","title":"gen_options_table","text":"<pre><code>gen_options_table(schema: dict[str, Any]) -&gt; str\n</code></pre> <p>Generates a Markdown table documenting plugin options.</p> <p>This function takes a schema dictionary, validates it against the <code>OptionsSchemaModel</code>, and then generates a Markdown table that summarizes the available methods and their options. Each row in the table represents a method, and the columns list the method's name and its configurable options. If a URL is provided for a method, the method name will be hyperlinked to that URL in the table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict[str, Any]</code> <p>A dictionary representing the schema of plugin options.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing a Markdown table that documents the plugin options.</p>"},{"location":"reference/enums/","title":"Enumerations","text":""},{"location":"reference/enums/#ropt.enums","title":"ropt.enums","text":"<p>Enumerations used within the <code>ropt</code> library.</p>"},{"location":"reference/enums/#ropt.enums.VariableType","title":"VariableType","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumerates the types of optimization variables.</p> <p>Specified in <code>VariablesConfig</code>, this information allows optimization backends to adapt their behavior.</p>"},{"location":"reference/enums/#ropt.enums.VariableType.REAL","title":"REAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REAL = 1\n</code></pre> <p>Continuous variables represented by real values.</p>"},{"location":"reference/enums/#ropt.enums.VariableType.INTEGER","title":"INTEGER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>INTEGER = 2\n</code></pre> <p>Discrete variables represented by integer values.</p>"},{"location":"reference/enums/#ropt.enums.BoundaryType","title":"BoundaryType","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumerates strategies for handling variable boundary violations.</p> <p>When variables are perturbed during optimization, their values might fall outside the defined lower and upper bounds. This enumeration defines different methods to adjust these perturbed values back within the valid range. The chosen strategy is configured in the <code>GradientConfig</code>.</p>"},{"location":"reference/enums/#ropt.enums.BoundaryType.NONE","title":"NONE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONE = 1\n</code></pre> <p>Do not modify the value.</p>"},{"location":"reference/enums/#ropt.enums.BoundaryType.TRUNCATE_BOTH","title":"TRUNCATE_BOTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TRUNCATE_BOTH = 2\n</code></pre> <p>Truncate the value \\(v_i\\) at the lower or upper boundary (\\(l_i\\), \\(u_i\\)):</p> \\[ \\hat{v_i} = \\begin{cases}     l_i &amp; \\text{if $v_i &lt; l_i$}, \\\\     b_i &amp; \\text{if $v_i &gt; b_i$}, \\\\     v_i &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"reference/enums/#ropt.enums.BoundaryType.MIRROR_BOTH","title":"MIRROR_BOTH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIRROR_BOTH = 3\n</code></pre> <p>Mirror the value \\(v_i\\) at the lower or upper boundary (\\(l_i\\), \\(u_i\\)):</p> \\[ \\hat{v_i} = \\begin{cases}     2l_i - v_i &amp; \\text{if $v_i &lt; l_i$}, \\\\     2b_i - v_i &amp; \\text{if $v_i &gt; b_i$}, \\\\     v_i        &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"reference/enums/#ropt.enums.PerturbationType","title":"PerturbationType","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumerates methods for scaling perturbation samples.</p> <p>Before a generated perturbation sample is added to a variable's current value (during gradient estimation, for example), it can be scaled. This enumeration defines the available scaling methods, configured in the <code>GradientConfig</code>.</p>"},{"location":"reference/enums/#ropt.enums.PerturbationType.ABSOLUTE","title":"ABSOLUTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ABSOLUTE = 1\n</code></pre> <p>Use the perturbation value as is.</p>"},{"location":"reference/enums/#ropt.enums.PerturbationType.RELATIVE","title":"RELATIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RELATIVE = 2\n</code></pre> <p>Multiply the perturbation value \\(p_i\\) by the range defined by the bounds of the variables \\(c_i\\): \\(\\hat{p}_i = (c_{i,\\text{max}} - c_{i,\\text{min}}) \\times p_i\\). The bounds will generally be defined in the configuration for the variables (see <code>VariablesConfig</code>).</p>"},{"location":"reference/enums/#ropt.enums.EventType","title":"EventType","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumerates the types of events emitted during optimization workflow execution.</p> <p>Events signal significant occurrences within the optimization process, such as the start or end of an optimization or an evaluation. Callbacks can be registered to listen for specific event types.</p> <p>When an event occurs, registered callbacks receive an <code>Event</code> object containing:</p> <ul> <li><code>event_type</code>: The type of the event (a value from this enumeration).</li> <li><code>data</code>: A dictionary containing event-specific data, such as   <code>Results</code> objects.</li> </ul>"},{"location":"reference/enums/#ropt.enums.EventType.START_EVALUATION","title":"START_EVALUATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>START_EVALUATION = 1\n</code></pre> <p>Emitted before evaluating new functions.</p>"},{"location":"reference/enums/#ropt.enums.EventType.FINISHED_EVALUATION","title":"FINISHED_EVALUATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FINISHED_EVALUATION = 2\n</code></pre> <p>Emitted after finishing the evaluation.</p>"},{"location":"reference/enums/#ropt.enums.EventType.START_OPTIMIZER","title":"START_OPTIMIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>START_OPTIMIZER = 3\n</code></pre> <p>Emitted just before starting an optimizer.</p>"},{"location":"reference/enums/#ropt.enums.EventType.FINISHED_OPTIMIZER","title":"FINISHED_OPTIMIZER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FINISHED_OPTIMIZER = 4\n</code></pre> <p>Emitted immediately after an optimizer finishes.</p>"},{"location":"reference/enums/#ropt.enums.EventType.START_ENSEMBLE_EVALUATOR","title":"START_ENSEMBLE_EVALUATOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>START_ENSEMBLE_EVALUATOR = 5\n</code></pre> <p>Emitted just before starting an evaluation.</p>"},{"location":"reference/enums/#ropt.enums.EventType.FINISHED_ENSEMBLE_EVALUATOR","title":"FINISHED_ENSEMBLE_EVALUATOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FINISHED_ENSEMBLE_EVALUATOR = 6\n</code></pre> <p>Emitted immediately after an evaluation finishes.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode","title":"ExitCode","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumerates the reasons for terminating an optimization.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = 0\n</code></pre> <p>Unknown cause of termination.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.TOO_FEW_REALIZATIONS","title":"TOO_FEW_REALIZATIONS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TOO_FEW_REALIZATIONS = 1\n</code></pre> <p>Returned when too few realizations are evaluated successfully.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.MAX_FUNCTIONS_REACHED","title":"MAX_FUNCTIONS_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_FUNCTIONS_REACHED = 2\n</code></pre> <p>Returned when the maximum number of function evaluations is reached.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.MAX_BATCHES_REACHED","title":"MAX_BATCHES_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MAX_BATCHES_REACHED = 3\n</code></pre> <p>Returned when the maximum number of evaluation batches is reached.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.NESTED_OPTIMIZER_FAILED","title":"NESTED_OPTIMIZER_FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NESTED_OPTIMIZER_FAILED = 4\n</code></pre> <p>Returned when a nested optimization fails to find an optimal value.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.USER_ABORT","title":"USER_ABORT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER_ABORT = 5\n</code></pre> <p>Returned when the optimization is aborted by the user.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.OPTIMIZER_FINISHED","title":"OPTIMIZER_FINISHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPTIMIZER_FINISHED = 6\n</code></pre> <p>Returned when an optimization step terminates normally.</p>"},{"location":"reference/enums/#ropt.enums.ExitCode.ENSEMBLE_EVALUATOR_FINISHED","title":"ENSEMBLE_EVALUATOR_FINISHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENSEMBLE_EVALUATOR_FINISHED = 7\n</code></pre> <p>Returned when an evaluation step terminates normally.</p>"},{"location":"reference/enums/#ropt.enums.AxisName","title":"AxisName","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enumerates the semantic meaning of axes in data arrays.</p> <p>The optimization workflow includes variables, objectives, constraints, realizations, and the optimizer. Each of these components can have multiple instances, leading to multidimensional data arrays. In particular, the <code>Results</code> objects store optimization data (like variable values, objective function values, constraint values, etc.) in multidimensional NumPy arrays.</p> <p>The <code>AxisName</code> enumeration  provides standardized labels to identify what each dimension (axis) of these arrays represents. For example, an array might have dimensions corresponding to different realizations, different objective functions, or different variables.</p> <p>This information is stored as metadata within the <code>Results</code> object and can be accessed using methods like <code>get_axes</code> on result fields. It is used internally, for instance, during data export to correctly label axes or retrieve associated names (like variable names) from the configuration.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.VARIABLE","title":"VARIABLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VARIABLE = 'variable'\n</code></pre> <p>The axis index corresponds to the index of the variable.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.OBJECTIVE","title":"OBJECTIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OBJECTIVE = 'objective'\n</code></pre> <p>The axis index corresponds to the index of the objective function.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.LINEAR_CONSTRAINT","title":"LINEAR_CONSTRAINT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LINEAR_CONSTRAINT = 'linear_constraint'\n</code></pre> <p>The axis index corresponds to the index of the linear constraint.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.NONLINEAR_CONSTRAINT","title":"NONLINEAR_CONSTRAINT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NONLINEAR_CONSTRAINT = 'nonlinear_constraint'\n</code></pre> <p>The axis index corresponds to the index of the constraint function.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.REALIZATION","title":"REALIZATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REALIZATION = 'realization'\n</code></pre> <p>The axis index corresponds to the index of the realization.</p>"},{"location":"reference/enums/#ropt.enums.AxisName.PERTURBATION","title":"PERTURBATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PERTURBATION = 'perturbation'\n</code></pre> <p>The axis index corresponds to the index of the perturbation.</p>"},{"location":"reference/evaluator/","title":"Function Evaluations","text":""},{"location":"reference/evaluator/#ropt.ensemble_evaluator.EnsembleEvaluator","title":"ropt.ensemble_evaluator.EnsembleEvaluator","text":"<p>Construct functions and gradients from an ensemble of functions.</p> <p>The <code>EnsembleEvaluator</code> class is responsible for calculating functions and gradients from an ensemble of functions. It leverages the settings defined in an <code>EnOptConfig</code> configuration object to guide the calculations.</p> <p>The core functionality relies on an evaluator callable, (usually provided by an <code>Evaluator</code> object), which is used to evaluate the individual functions within the ensemble. The evaluator provides the raw function values, which are then processed by the <code>EnsembleEvaluator</code> to produce the final function and gradient estimates.</p>"},{"location":"reference/evaluator/#ropt.ensemble_evaluator.EnsembleEvaluator.__init__","title":"__init__","text":"<pre><code>__init__(\n    config: EnOptConfig,\n    transforms: OptModelTransforms | None,\n    evaluator: EvaluatorCallback,\n) -&gt; None\n</code></pre> <p>Initialize the EnsembleEvaluator.</p> <p>This method sets up the <code>EnsembleEvaluator</code> with the necessary configuration, evaluator, and plugins.</p> <p>The <code>config</code> object contains all the settings required for the ensemble evaluation, such as the number of realizations, the function estimators, and the gradient settings. The <code>transforms</code> object defines the domain transforms that should be applied to variables, objectives and constraints. The <code>evaluator</code> callable is usually provide by a <code>Evaluator</code> object. The <code>plugin_manager</code> is used to load the realization filters, function estimators, and samplers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The configuration object.</p> required <code>transforms</code> <code>OptModelTransforms | None</code> <p>The domain transforms to apply.</p> required <code>evaluator</code> <code>EvaluatorCallback</code> <p>The callable for evaluating individual functions.</p> required"},{"location":"reference/evaluator/#ropt.ensemble_evaluator.EnsembleEvaluator.calculate","title":"calculate","text":"<pre><code>calculate(\n    variables: NDArray[float64],\n    *,\n    compute_functions: bool,\n    compute_gradients: bool,\n) -&gt; tuple[Results, ...]\n</code></pre> <p>Evaluate the given variable vectors.</p> <p>This method calculates functions, gradients, or both, based on the provided variable vectors and the specified flags.</p> <p>The <code>variables</code> argument can be a single vector or a matrix where each row is a variable vector.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The variable vectors to evaluate.</p> required <code>compute_functions</code> <code>bool</code> <p>Whether to calculate functions.</p> required <code>compute_gradients</code> <code>bool</code> <p>Whether to calculate gradients.</p> required <p>Returns:</p> Type Description <code>tuple[Results, ...]</code> <p>The results for function evaluations and/or gradient evaluations.</p>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorContext","title":"ropt.evaluator.EvaluatorContext  <code>dataclass</code>","text":"<p>Capture additional details for the function evaluator.</p> <p>Function evaluators (see <code>Evaluator</code>) primarily receive variable vectors to evaluate objective and constraint functions. However, they may also benefit from additional information to optimize their calculations. This <code>EvaluatorContext</code> object provides that supplementary information.</p> <p>Specifically, it provides:</p> <ul> <li>The configuration object for the current optimization run.</li> <li>A boolean vector (<code>active</code>) indicating which variable rows require   evaluation.</li> <li>The realization index for each variable vector. This can be used to   determine the correct function from an ensemble to use with each variable   vector.</li> <li>The perturbation index for each variable vector (if applicable). A value   less than 0 indicates that the vector is not a perturbation.</li> </ul> <p>Attributes:</p> Name Type Description <code>config</code> <code>EnOptConfig</code> <p>Configuration of the optimizer.</p> <code>active</code> <code>NDArray[bool_]</code> <p>Indicates which variable rows require evaluation.</p> <code>realizations</code> <code>NDArray[intc]</code> <p>Realization numbers for each requested evaluation.</p> <code>perturbations</code> <code>NDArray[intc] | None</code> <p>Perturbation numbers for each requested evaluation.                 A value less than 0 indicates that the vector is                 not a perturbation.</p>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorContext.get_active_evaluations","title":"get_active_evaluations","text":"<pre><code>get_active_evaluations(array: NDArray[T]) -&gt; NDArray[T]\n</code></pre> <p>Filter an array based on the active property.</p> <p>This is a utility method, which can be used if only the active property is used to exclude variable rows that are inactive, i.e. where none of the objects or constraints are needed.</p> <p>This method filters a one- or two-dimensional array by retaining only those entries or rows that correspond to active. The activity of realizations is determined by the <code>self.active</code> boolean array (where <code>True</code> indicates active).</p> <p>If <code>self.active</code> is <code>None</code> (indicating that all variable rows are to be considered active), no filtering is applied, and the original input is returned.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray[T]</code> <p>The array to filter.</p> required <p>Returns:</p> Type Description <code>NDArray[T]</code> <p>The filtered results.</p>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorContext.insert_inactive_results","title":"insert_inactive_results","text":"<pre><code>insert_inactive_results(\n    array: NDArray[T], *, fill_value: float = 0.0\n) -&gt; NDArray[T]\n</code></pre> <p>Expand an array by inserting fill values for inactive variables.</p> <p>This is a utility method, which can be used if only the active property is used to exclude variable rows that are fully inactive.</p> <p>This method takes an array and expands it to its original dimensions by inserting a specified <code>fill_value</code> at positions corresponding to inactive rows. If the array is one-dimensional, zero entries are inserted, if it is two-dimensional rows of zero values are inserted.</p> <p>If <code>self.active</code> is <code>None</code> (implying all rows were considered active or no filtering was applied), the input <code>array</code> is returned unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray[T]</code> <p>The array to expand.</p> required <code>fill_value</code> <code>float</code> <p>The value to insert for inactive entries.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>NDArray[T]</code> <p>An expanded array matching the original number of variables.</p>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorResult","title":"ropt.evaluator.EvaluatorResult  <code>dataclass</code>","text":"<p>Store the results of a function evaluation.</p> <p>This class stores the results of evaluating objective and constraint functions for a set of variable vectors.</p> <p>The <code>objectives</code> and <code>constraints</code> are stored as matrices. Each column in these matrices corresponds to a specific objective or constraint, and each row corresponds to a variable vector.</p> <p>When the evaluator is asked to evaluate functions, some variable vectors may be marked as inactive. The results for these inactive vectors should be set to zero. All active variable vectors should be evaluated. If an evaluation fails for any reason, the corresponding values should be set to <code>numpy.nan</code>.</p> <p>A <code>batch_id</code> can be set to identify this specific set of evaluation results.</p> <p>The <code>evaluation_info</code> dictionary can store additional metadata for each evaluation. This information is not used internally by <code>ropt</code> and can have an arbitrary structure, to be interpreted by the application. This can be used, for example, to uniquely identify the results calculated for each variable vector, allowing them to be linked back to their corresponding input vectors.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>NDArray[float64]</code> <p>The calculated objective values.</p> required <code>constraints</code> <code>NDArray[float64] | None</code> <p>Optional calculated constraint values.</p> <code>None</code> <code>batch_id</code> <code>int | None</code> <p>Optional batch ID to identify this set of results.</p> <code>None</code> <code>evaluation_info</code> <code>dict[str, NDArray[Any]]</code> <p>Optional info for each evaluation.</p> <code>dict()</code>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorCallback","title":"ropt.evaluator.EvaluatorCallback","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the call signature for evaluator callbacks.</p> <p>Events handlers define a callback to handler events emitted during the optimization process.</p>"},{"location":"reference/evaluator/#ropt.evaluator.EvaluatorCallback.__call__","title":"__call__","text":"<pre><code>__call__(\n    variables: NDArray[float64], context: EvaluatorContext\n) -&gt; EvaluatorResult\n</code></pre> <p>Handle an event.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The variables to pass to the evaluation function.</p> required <code>context</code> <code>EvaluatorContext</code> <p>The evaluator context to pass.</p> required <p>Returns:</p> Type Description <code>EvaluatorResult</code> <p>The results of the evaluation.</p>"},{"location":"reference/evaluator_plugins/","title":"Evaluators","text":""},{"location":"reference/evaluator_plugins/#ropt.plugins.evaluator.base.EvaluatorPlugin","title":"ropt.plugins.evaluator.base.EvaluatorPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract base class for evaluator plugins.</p> <p>This class defines the interface for plugins responsible for creating <code>Evaluator</code> instances within an optimization workflow.</p>"},{"location":"reference/evaluator_plugins/#ropt.plugins.evaluator.base.EvaluatorPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(name: str, **kwargs: Any) -&gt; Evaluator\n</code></pre> <p>Create an Evaluator instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>Evaluator</code> objects. Plugin implementations must override this method to return an instance of their specific <code>Evaluator</code> subclass.</p> <p>The <code>PluginManager</code> calls this method when an evaluator provided by this plugin is requested.</p> <p>The <code>name</code> argument specifies the requested evaluator, potentially in the format <code>\"plugin-name/method-name\"</code> or just <code>\"method-name\"</code>. Implementations can use this <code>name</code> to vary the created evaluator if the plugin supports multiple evaluator types.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The requested evaluator name (potentially plugin-specific).</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments for custom configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Evaluator</code> <p>An initialized instance of an <code>Evaluator</code> subclass.</p>"},{"location":"reference/evaluator_plugins/#ropt.plugins.evaluator.base.Evaluator","title":"ropt.plugins.evaluator.base.Evaluator","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for evaluator components within an optimization workflow.</p> <p>Subclasses must implement the abstract <code>eval</code> method, which is responsible for performing the actual evaluation of variables using an <code>EvaluatorContext</code> and returning an <code>EvaluatorResult</code>.</p> <p><code>Evaluator</code> instances are typically created using the <code>create_evaluator</code> function.</p>"},{"location":"reference/evaluator_plugins/#ropt.plugins.evaluator.base.Evaluator.eval","title":"eval  <code>abstractmethod</code>","text":"<pre><code>eval(\n    variables: NDArray[float64], context: EvaluatorContext\n) -&gt; EvaluatorResult\n</code></pre> <p>Evaluate objective and constraint functions for given variables.</p> <p>This method defines function evaluator callback, which calculates objective and constraint functions for a set of variable vectors, potentially for a subset of realizations and perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The matrix of variables to evaluate. Each row represents        a variable vector.</p> required <code>context</code> <code>EvaluatorContext</code> <p>The evaluation context, providing additional information        about the evaluation.</p> required <p>Returns:</p> Type Description <code>EvaluatorResult</code> <p>An evaluation results object containing the calculated values.</p> Reusing Objectives and Constraints <p>When defining multiple objectives, there may be a need to reuse the same objective or constraint value multiple times. For instance, a total objective could consist of the mean of the objectives for each realization, plus the standard deviation of the same values. This can be implemented by defining two objectives: the first calculated as the mean of the realizations, and the second using a function estimator to compute the standard deviations. The optimizer is unaware that both objectives use the same set of realizations. To prevent redundant calculations, the evaluator should compute the results of the realizations once and return them for both objectives.</p>"},{"location":"reference/event_handler_plugins/","title":"Event Handlers","text":""},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandlerPlugin","title":"ropt.plugins.event_handler.base.EventHandlerPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract base class for event handler plugins.</p> <p>This class defines the interface for plugins responsible for creating <code>EventHandler</code> instances within an optimization workflow. Event handlers respond to events after being connected to a compute step by the <code>add_event_handler</code> method. For each event emitted by the compute step, the <code>handle_event</code> method of connected handlers is called with the event.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandlerPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(name: str, **kwargs: Any) -&gt; EventHandler\n</code></pre> <p>Create a EventHandler instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>EventHandler</code> objects. Plugin implementations must override this method to return an instance of their specific <code>EventHandler</code> subclass.</p> <p>The <code>name</code> argument specifies the requested event handler, potentially in the format <code>\"plugin-name/method-name\"</code> or just <code>\"method-name\"</code>. Implementations can use this <code>name</code> to vary the created event handler if the plugin supports multiple event handler types.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The requested event handler name (potentially plugin-specific).</p> required <code>kwargs</code> <code>Any</code> <p>Additional arguments for custom configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EventHandler</code> <p>An initialized instance of a <code>EventHandler</code> subclass.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler","title":"ropt.plugins.event_handler.base.EventHandler","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for event handlers.</p> <p>This class defines the fundamental interface for all event handlers within an optimization workflow. Concrete handler implementations, (e.g., tracking results, storing data, logging), must inherit from this base class.</p> <p>Handlers may store state using dictionary-like access (<code>[]</code>), allowing them to accumulate information or make data available to other components in an optimization workflow.</p> <p>Subclasses must implement the abstract <code>handle_event</code> method to define their specific event processing logic.</p> <p><code>EventHandler</code> instances are typically created using the <code>create_event_handler</code> function.</p> <p>Event handlers are attached to a <code>ComputeStep</code> using its <code>add_event_handler</code> method. When the compute step emits an event, the <code>handle_event</code> method of each attached handler is invoked, allowing it to process the event.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler.event_types","title":"event_types  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>event_types: set[EventType]\n</code></pre> <p>Return the event types that are handled.</p> <p>Returns:</p> Type Description <code>set[EventType]</code> <p>A set of event types that are handled.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the EventHandler.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler.handle_event","title":"handle_event  <code>abstractmethod</code>","text":"<pre><code>handle_event(event: Event) -&gt; None\n</code></pre> <p>Process an event.</p> <p>This abstract method must be implemented by concrete <code>EventHandler</code> subclasses. It defines the event handler's core logic for reacting to <code>Event</code> objects emitted in the optimization workflow.</p> <p>Implementations should inspect the <code>event</code> object (its <code>event_type</code> and <code>data</code>) and perform computations accordingly, such as storing results, logging information, or updating internal state.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event object.</p> required"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Any\n</code></pre> <p>Retrieve a value from the event handler's internal state.</p> <p>This method enables dictionary-like access (<code>handler[key]</code>) to the values stored within the event handler's internal state dictionary. This allows handlers to store and retrieve data accumulated during workflow execution.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key identifying the value to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The value associated with the specified key.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the provided <code>key</code> does not exist in the             event handler's stored values.</p>"},{"location":"reference/event_handler_plugins/#ropt.plugins.event_handler.base.EventHandler.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: str, value: Any) -&gt; None\n</code></pre> <p>Store or update a value in the event handler's internal state.</p> <p>This method enables dictionary-like assignment (<code>handler[key] = value</code>) to store arbitrary data within the event handler's internal state dictionary. This allows event handlers to accumulate information or make data available to other components of the workflow.</p> <p>The key must be a valid Python identifier.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The string key identifying the value to store (must be an identifier).</p> required <code>value</code> <code>Any</code> <p>The value to associate with the key.</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the provided <code>key</code> is not a valid identifier.</p>"},{"location":"reference/exceptions/","title":"Exceptions","text":""},{"location":"reference/exceptions/#ropt.exceptions","title":"ropt.exceptions","text":"<p>Exceptions raised within the <code>ropt</code> library.</p>"},{"location":"reference/exceptions/#ropt.exceptions.ComputeStepAborted","title":"ComputeStepAborted","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a compute step is aborted prematurely.</p> <p>This exception signals that an optimization or another compute step could not complete its intended task due to a specific condition (e.g., insufficient valid realizations, user request).</p> <p>It must be initialized with an <code>ExitCode</code> indicating the reason for the abortion.</p>"},{"location":"reference/exceptions/#ropt.exceptions.ComputeStepAborted.__init__","title":"__init__","text":"<pre><code>__init__(exit_code: ExitCode) -&gt; None\n</code></pre> <p>Initialize the ComputeStepAborted exception.</p> <p>Stores the reason for the abortion, which can be accessed via the <code>exit_code</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>exit_code</code> <code>ExitCode</code> <p>The code indicating why the compute step was aborted.</p> required"},{"location":"reference/external_optimizer_plugin/","title":"External Optimizers","text":""},{"location":"reference/external_optimizer_plugin/#ropt.plugins.optimizer.external.ExternalOptimizer","title":"ropt.plugins.optimizer.external.ExternalOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Plugin class for optimization using an external process.</p> <p>This class enables optimization via an external process, which performs the optimization independently and communicates with this class over pipes to request function evaluations, report optimizer states, and handle any errors.</p> <p>Typically, the optimizer is specified within an <code>OptimizerConfig</code> via the <code>method</code> field, which either provides the algorithm name directly or follows the form <code>plugin-name/method-name</code>. In the first case, <code>ropt</code> searches among all available optimizer plugins to find the specified method. In the second case, it checks if the plugin identified by <code>plugin-name</code> contains <code>method-name</code> and, if so, uses it. Both of these are not supported by the external optimizer class. Instead, it requires that the <code>method</code> field includes both the plugin and method names in the format <code>external/plugin-name/method-name</code> or <code>external/method-name</code>. This ensures the external optimizer can identify and launch the specified optimization method <code>method-name</code> and launch it as an external process.</p>"},{"location":"reference/function_estimator_plugins/","title":"Function estimators","text":""},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator","title":"ropt.plugins.function_estimator","text":"<p>Provides plugin functionality for adding function estimators.</p> <p>Function estimators are used by the optimization process to combine the results (objective function values and gradients) from a set of realizations into a single representative value. This module allows for the extension of <code>ropt</code> with custom strategies for aggregating ensemble results.</p> <p>Core Concepts:</p> <ul> <li>Plugin Interface: Function estimator plugins must inherit from the   <code>FunctionEstimatorPlugin</code>   base class. This class acts as a factory, defining a <code>create</code> method to   instantiate estimator objects.</li> <li>Estimator Implementation: The actual aggregation logic resides in classes   that inherit from the   <code>FunctionEstimator</code>   abstract base class. These classes are initialized with the optimization   configuration (<code>EnOptConfig</code>) and the index of the   specific estimator configuration to use (<code>estimator_index</code>). The core   functionality is provided by the <code>calculate_function</code> and <code>calculate_gradient</code>   methods, which combine the function values and gradients from multiple   realizations, respectively, using realization weights.</li> <li>Discovery: The <code>PluginManager</code> discovers   available <code>FunctionEstimatorPlugin</code> implementations (typically via entry   points) and uses them to create <code>FunctionEstimator</code> instances as needed   during workflow execution.</li> </ul> <p>Built-in Function Estimator Plugins:</p> <p>The default <code>DefaultFunctionEstimator</code> class provides methods for calculating the weighted mean (<code>mean</code>) and standard deviation (<code>stddev</code>) of the realization results.</p>"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimatorPlugin","title":"ropt.plugins.function_estimator.base.FunctionEstimatorPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base Class for Function Estimator Plugins (Factories).</p> <p>This class defines the interface for plugins responsible for creating <code>FunctionEstimator</code> instances. These plugins act as factories for specific function estimation strategies.</p> <p>During optimization execution, the <code>PluginManager</code> identifies the appropriate function estimator plugin based on the configuration and uses its <code>create</code> class method to instantiate the actual <code>FunctionEstimator</code> object that will perform the aggregation of ensemble results (function values and gradients).</p>"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimatorPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(\n    enopt_config: EnOptConfig, estimator_index: int\n) -&gt; FunctionEstimator\n</code></pre> <p>Factory method to create a concrete FunctionEstimator instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>FunctionEstimator</code> objects. Plugin implementations must override this method to return an instance of their specific <code>FunctionEstimator</code> subclass.</p> <p>The <code>PluginManager</code> calls this method when the optimization requires a function estimator provided by this plugin.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The main EnOpt configuration object.</p> required <code>estimator_index</code> <code>int</code> <p>Index into <code>enopt_config.function_estimators</code> for              this estimator.</p> required <p>Returns:</p> Type Description <code>FunctionEstimator</code> <p>An initialized FunctionEstimator object ready for use.</p>"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimator","title":"ropt.plugins.function_estimator.base.FunctionEstimator","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for Function Estimator Implementations.</p> <p>This class defines the fundamental interface for all concrete function estimator implementations within the <code>ropt</code> framework. Function estimator plugins provide classes derived from <code>FunctionEstimator</code> that encapsulate the logic for combining the objective function values and gradients from an ensemble of realizations into a single representative value. This aggregated value is then used by the core optimization algorithm.</p> <p>Instances of <code>FunctionEstimator</code> subclasses are created by their corresponding <code>FunctionEstimatorPlugin</code> factories. They are initialized with an <code>EnOptConfig</code> object detailing the optimization setup and the <code>estimator_index</code> identifying the specific estimator configuration to use from the config.</p> <p>The core functionality involves combining results using realization weights, performed by the <code>calculate_function</code> and <code>calculate_gradient</code> methods, which must be implemented by subclasses.</p> <p>Subclasses must implement:</p> <ul> <li><code>__init__</code>: To accept the configuration and index.</li> <li><code>calculate_function</code>: To combine function values from realizations.</li> <li><code>calculate_gradient</code>: To combine gradient values from realizations.</li> </ul>"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimator.__init__","title":"__init__","text":"<pre><code>__init__(\n    enopt_config: EnOptConfig, estimator_index: int\n) -&gt; None\n</code></pre> <p>Initialize the function estimator object.</p> <p>The <code>function_estimators</code> field in the <code>enopt_config</code> is a tuple of estimator configurations (<code>FunctionEstimatorConfig</code>). The <code>estimator_index</code> identifies which configuration from this tuple should be used to initialize this specific estimator instance.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The configuration of the optimizer.</p> required <code>estimator_index</code> <code>int</code> <p>The index of the estimator configuration to use.</p> required"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimator.calculate_function","title":"calculate_function  <code>abstractmethod</code>","text":"<pre><code>calculate_function(\n    functions: NDArray[float64], weights: NDArray[float64]\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Combine function values from realizations into an expected value.</p> <p>This method takes the function (objective or constraint) values evaluated for each realization in the ensemble and combines them into a single representative value or vector of values, using the provided realization weights.</p> <p>Parameters:</p> Name Type Description Default <code>functions</code> <code>NDArray[float64]</code> <p>The function values for each realization.</p> required <code>weights</code> <code>NDArray[float64]</code> <p>The weight for each realization.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>A scalar or 1D array representing the combined function value(s).</p>"},{"location":"reference/function_estimator_plugins/#ropt.plugins.function_estimator.base.FunctionEstimator.calculate_gradient","title":"calculate_gradient  <code>abstractmethod</code>","text":"<pre><code>calculate_gradient(\n    functions: NDArray[float64],\n    gradient: NDArray[float64],\n    weights: NDArray[float64],\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Combine gradients from realizations into an expected gradient.</p> <p>This method takes the gradients evaluated for each realization and combines them into a single representative gradient vector or matrix, using the provided realization weights and potentially the function values themselves (e.g., for estimators like standard deviation where the chain rule applies).</p> Interaction with <code>merge_realizations</code> <p>The <code>merge_realizations</code> flag in the <code>GradientConfig</code> determines how the initial gradient estimate(s) are computed by <code>ropt</code> before being passed to this <code>calculate_gradient</code> method.</p> <ul> <li>If <code>False</code> (default): <code>ropt</code> estimates a separate gradient for each   realization that has a non-zero weight. The implementation   must then combine these gradients using the provided <code>weights</code>.</li> <li>If <code>True</code>: <code>ropt</code> computes a single, merged gradient estimate by   treating all perturbations across all realizations collectively.   The implementation must handle this input appropriately. For simple   averaging estimators, this might involve returning the input gradient   unchanged.</li> </ul> <p>The <code>merge_realizations=True</code> option allows gradient estimation even with a low number of perturbations (potentially just one) but is generally only suitable for estimators performing averaging-like operations. Estimator implementations should check this flag during initialization (<code>__init__</code>) and raise a <code>ValueError</code> if <code>merge_realizations=True</code> is incompatible with the estimator's logic (e.g., standard deviation).</p> <p>Parameters:</p> Name Type Description Default <code>functions</code> <code>NDArray[float64]</code> <p>The functions for each realization.</p> required <code>gradient</code> <code>NDArray[float64]</code> <p>The gradient for each realization.</p> required <code>weights</code> <code>NDArray[float64]</code> <p>The weight of each realization.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>The expected gradients.</p>"},{"location":"reference/optimization/","title":"Ensemble Optimization","text":""},{"location":"reference/optimization/#ropt.optimization.EnsembleOptimizer","title":"ropt.optimization.EnsembleOptimizer","text":"<p>Optimizer for ensemble-based optimizations.</p> <p>The <code>EnsembleOptimizer</code> class provides the core functionality for running ensemble-based optimizations. Direct use of this class is generally discouraged. Instead, use the <code>BasicOptimizer</code> class or build a workflow containing the optimization steps.</p>"},{"location":"reference/optimization/#ropt.optimization.EnsembleOptimizer.is_parallel","title":"is_parallel  <code>property</code>","text":"<pre><code>is_parallel: bool\n</code></pre> <p>Determine if the optimization supports parallel evaluations.</p> <p>The underlying optimization algorithm may request function evaluations via a callback. Parallel optimization, in this context, means that the algorithm may request multiple function evaluations in a single callback.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the optimization supports parallel evaluations, <code>False</code></p> <code>bool</code> <p>otherwise.</p>"},{"location":"reference/optimization/#ropt.optimization.EnsembleOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    enopt_config: EnOptConfig,\n    transforms: OptModelTransforms | None,\n    ensemble_evaluator: EnsembleEvaluator,\n    signal_evaluation: SignalEvaluationCallback\n    | None = None,\n    nested_optimizer: NestedOptimizerCallback | None = None,\n) -&gt; None\n</code></pre> <p>Initialize the EnsembleOptimizer.</p> <p>This class orchestrates ensemble-based optimizations. It requires an optimization configuration, an evaluator, and a plugin manager to function.</p> <p>The <code>EnsembleOptimizer</code> needs the following to define a single optimization run:</p> <ol> <li>An <code>EnOptConfig</code> object: This contains     all configuration settings for the optimization.</li> <li>A <code>OptModelTransforms</code> object:     This handles the transforms to apply to the variables, objectives and     constraints.</li> <li>An <code>EnsembleEvaluator</code>     object: This object is responsible for evaluating functions.</li> <li>A <code>PluginManager</code> object: This object     provides access to optimizer plugins.</li> </ol> <p>Additionally, two optional callbacks can be provided to extend the functionality:</p> <ol> <li>A     <code>SignalEvaluationCallback</code>:     This callback is invoked before and after each function evaluation.</li> <li>A     <code>NestedOptimizerCallback</code>:     This callback is invoked at each function evaluation to run a nested     optimization.</li> </ol> <p>The optimizer plugins are used by the ensemble optimizer to implement the actual optimization process. The <code>EnsembleOptimizer</code> class provides the callback function to these plugins needed (see OptimizerCallback)</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The ensemble optimization configuration.</p> required <code>transforms</code> <code>OptModelTransforms | None</code> <p>The transforms to apply to the model.</p> required <code>ensemble_evaluator</code> <code>EnsembleEvaluator</code> <p>The evaluator for function evaluations.</p> required <code>signal_evaluation</code> <code>SignalEvaluationCallback | None</code> <p>Optional callback to signal evaluations.</p> <code>None</code> <code>nested_optimizer</code> <code>NestedOptimizerCallback | None</code> <p>Optional callback for nested optimizations.</p> <code>None</code>"},{"location":"reference/optimization/#ropt.optimization.EnsembleOptimizer.start","title":"start","text":"<pre><code>start(variables: NDArray[float64]) -&gt; ExitCode\n</code></pre> <p>Start the optimization process.</p> <p>This method initiates the optimization process using the provided initial variables. The optimization will continue until a stopping criterion is met or an error occurs.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The initial variables for the optimization.</p> required <p>Returns:</p> Type Description <code>ExitCode</code> <p>An <code>ExitCode</code> indicating the reason for</p> <code>ExitCode</code> <p>termination.</p>"},{"location":"reference/optimization/#ropt.optimization.SignalEvaluationCallback","title":"ropt.optimization.SignalEvaluationCallback","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a callback to signal the start and end of an evaluation.</p> <p>This callback is invoked before and after each evaluation, allowing for custom handling or tracking of evaluation events.</p>"},{"location":"reference/optimization/#ropt.optimization.SignalEvaluationCallback.__call__","title":"__call__","text":"<pre><code>__call__(\n    results: tuple[Results, ...] | None = None,\n) -&gt; None\n</code></pre> <p>Callback protocol for signaling the start and end of evaluations.</p> <p>This callback is invoked by the ensemble optimizer before and after each evaluation. Before the evaluation starts, the callback is called with <code>results</code> set to <code>None</code>. After the evaluation completes, the callback is called again, this time with <code>results</code> containing the output of the evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>tuple[Results, ...] | None</code> <p>The results produced by the evaluation, or <code>None</code> if the      evaluation has not yet started.</p> <code>None</code>"},{"location":"reference/optimization/#ropt.optimization.NestedOptimizerCallback","title":"ropt.optimization.NestedOptimizerCallback","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for functions that start a nested optimization.</p>"},{"location":"reference/optimization/#ropt.optimization.NestedOptimizerCallback.__call__","title":"__call__","text":"<pre><code>__call__(\n    variables: NDArray[float64],\n) -&gt; tuple[FunctionResults | None, bool]\n</code></pre> <p>Callback protocol for executing a nested optimization.</p> <p>This function is invoked by the ensemble optimizer during each function evaluation to initiate a nested optimization process. It receives the current variables as input and is expected to perform a nested optimization using these variables as a starting point. The function should return a tuple containing the result of the nested optimization and a boolean indicating whether the nested optimization was aborted by the user. The result of the nested optimization should be a <code>FunctionResults</code> object, or <code>None</code> if no result is available.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>The variables to use as the starting point.</p> required <p>Returns:</p> Type Description <code>tuple[FunctionResults | None, bool]</code> <p>The result and a boolean indicating if the user aborted.</p>"},{"location":"reference/optimization/#ropt.optimization.OptimizerCallback","title":"ropt.optimization.OptimizerCallback","text":"<p>               Bases: <code>Protocol</code></p> <p>Defines the call signature for the optimizer evaluation callback.</p> <p>Optimizers uses this callback to request function and gradient evaluations from the <code>ropt</code> core during the optimization process.</p>"},{"location":"reference/optimization/#ropt.optimization.OptimizerCallback.__call__","title":"__call__","text":"<pre><code>__call__(\n    variables: NDArray[float64],\n    /,\n    *,\n    return_functions: bool,\n    return_gradients: bool,\n) -&gt; OptimizerCallbackResult\n</code></pre> <p>Request function and/or gradient evaluations from the <code>ropt</code> core.</p> <p>This method is called by the optimizer implementation to obtain objective function values, constraint values, and their gradients for one or more sets of variable values. In addition other update information, such as non-linear constraint bounds may be returned.</p> <p>The <code>variables</code> argument can be a 1D array (single vector) or a 2D array (matrix where each row is a variable vector). Parallel or batch-based optimizers might provide a matrix, while others typically provide a single vector.</p> <p>The <code>return_functions</code> and <code>return_gradients</code> flags control what is computed and returned in a <code>OptimizerCallbackResult</code> structure.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>NDArray[float64]</code> <p>A 1D or 2D array of variable values to evaluate.</p> required <code>return_functions</code> <code>bool</code> <p>If <code>True</code>, compute and return function/constraint values.</p> required <code>return_gradients</code> <code>bool</code> <p>If <code>True</code>, compute and return gradient values.</p> required <p>Returns:</p> Type Description <code>OptimizerCallbackResult</code> <p>A data structure with the results.</p>"},{"location":"reference/optimization/#ropt.optimization.OptimizerCallbackResult","title":"ropt.optimization.OptimizerCallbackResult  <code>dataclass</code>","text":"<p>Holds the results from an optimizer callback evaluation.</p> <p>This dataclass is used to structure the output of the <code>OptimizerCallback</code>. It bundles the objective function values, gradient values, and any updated non-linear constraint bounds that result from an evaluation request.</p> <p>The <code>functions</code> attribute will contain a NumPy array of the objective function value(s) if they were requested and successfully computed, otherwise it will be <code>None</code>. Similarly, the <code>gradients</code> attribute will hold a NumPy array of gradient values if requested and computed, and <code>None</code> otherwise.</p> <p>The <code>nonlinear_constraint_bounds</code> attribute is a tuple containing two NumPy arrays: the first for lower bounds and the second for upper bounds of any non-linear constraints. This will be <code>None</code> if there are no non-linear constraints or if their bounds were not updated during the callback.</p> <p>The <code>functions</code> and <code>gradients</code> fields must be structured as follows:</p> <ul> <li> <p>Functions Array: This array contains the objective and non-linear     constraint values. If <code>variables</code> was a vector, it's a 1D array:</p> <pre><code>[objective, constraint1, constraint2, ...]\n</code></pre> <p>If <code>variables</code> was a matrix, it's a 2D array where each row corresponds to a row in the input <code>variables</code>, with the same structure:</p> <pre><code>[\n    [obj_row1, con1_row1, ...],\n    [obj_row2, con2_row2, ...],\n    ...\n]\n</code></pre> </li> <li> <p>Gradients Array: This array contains the gradients of the objective     and non-linear constraints. It's always a 2D array where rows correspond     to the objective/constraints and columns correspond to the variables:</p> <pre><code>[\n    [grad_obj_var1,  grad_obj_var2,  ...],\n    [grad_con1_var1, grad_con1_var2, ...],\n    ...\n]\n</code></pre> </li> </ul> <p>Attributes:</p> Name Type Description <code>functions</code> <code>NDArray[float64] | None</code> <p>Objective function value(s).</p> <code>gradients</code> <code>NDArray[float64] | None</code> <p>Gradient values.</p> <code>nonlinear_constraint_bounds</code> <code>tuple[NDArray[float64], NDArray[float64]] | None</code> <p>Updated non-linear constraint lower and upper bounds.</p>"},{"location":"reference/optimizer_plugins/","title":"Optimizers","text":""},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer","title":"ropt.plugins.optimizer","text":"<p>Framework and Implementations for Optimizer Plugins.</p> <p>This module provides the necessary components for integrating optimization algorithms into <code>ropt</code> via its plugin system. Optimizer plugins allow <code>ropt</code> to utilize various optimization techniques, either built-in or provided by third-party packages.</p> <p>Core Concepts:</p> <ul> <li>Plugin Interface: Optimizer plugins must inherit from the   <code>OptimizerPlugin</code> base class.   This class acts as a factory, defining a <code>create</code> method to instantiate   optimizer objects.</li> <li>Optimizer Implementation: The actual optimization logic resides in classes   that inherit from the <code>Optimizer</code>   abstract base class. These classes are initialized with the optimization   configuration (<code>EnOptConfig</code>) and an   <code>OptimizerCallback</code>. The callback is   used by the optimizer to request function and gradient evaluations from   <code>ropt</code>. The optimization process is initiated by calling the optimizer's   <code>start</code> method.</li> <li>Discovery: The <code>PluginManager</code> discovers   available <code>OptimizerPlugin</code> implementations (typically via entry points) and   uses them to create <code>Optimizer</code> instances as needed during workflow execution.</li> </ul> <p>Utilities:</p> <p>The <code>ropt.plugins.optimizer.utils</code> module offers helper functions for common tasks within optimizer plugins, such as validating constraint support and handling normalized constraints.</p> <p>Built-in Optimizers:</p> <p><code>ropt</code> includes the following optimizers by default:</p> <ul> <li><code>SciPyOptimizer</code>: Provides   access to various algorithms from the <code>scipy.optimize</code> library.</li> <li><code>ExternalOptimizer</code>:   Enables running other optimizer plugins in a separate external process, useful   for isolation or specific execution environments.</li> </ul>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.OptimizerPlugin","title":"ropt.plugins.optimizer.base.OptimizerPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base Class for Optimizer Plugins (Factories).</p> <p>This class defines the interface for plugins responsible for creating <code>Optimizer</code> instances. These plugins act as factories for specific optimization algorithms or backends.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.OptimizerPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(\n    config: EnOptConfig,\n    optimizer_callback: OptimizerCallback,\n) -&gt; Optimizer\n</code></pre> <p>Create an Optimizer instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>Optimizer</code> objects. Plugin implementations must override this method to return an instance of their specific <code>Optimizer</code> subclass.</p> <p>The <code>PluginManager</code> calls this method when an optimization workflow requires an optimizer provided by this plugin.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The  configuration object containing the                 optimization settings.</p> required <code>optimizer_callback</code> <code>OptimizerCallback</code> <p>The callback function used by the optimizer to                 request evaluations.</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>An initialized instance of an <code>Optimizer</code> subclass.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.OptimizerPlugin.validate_options","title":"validate_options  <code>classmethod</code>","text":"<pre><code>validate_options(\n    method: str, options: dict[str, Any] | list[str] | None\n) -&gt; None\n</code></pre> <p>Validate the optimizer-specific options for a given method.</p> <p>This class method is intended to check if the <code>options</code> dictionary, typically provided in the <code>OptimizerConfig</code>, contains valid keys and values for the specified optimization <code>method</code> supported by this plugin.</p> <p>This default implementation performs no validation. Subclasses should override this method to implement validation logic specific to the methods they support, potentially using schema validation tools like Pydantic.</p> <p>The raised exception must be a ValueError, or derive from a ValueError.</p> Note <p>It is expected that the optimizer either receives a dictionary, or a list of options. This method should test if the type of the options is as expected, and raise a <code>ValueError</code> with an appropriate message if this is not the case.</p> Method name with prefix <p>The method string may be prefixed in the form \"backend/method\", take this into account when parsing the method name.</p> Handling the default method <p>The the method string may be set to \"default\", in which case it should be mapped to the correct default method of the backend.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The specific optimization method name (e.g., \"slsqp\",      \"my_optimizer/variant1\").</p> required <code>options</code> <code>dict[str, Any] | list[str] | None</code> <p>The dictionary or a list of strings of options.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided options are invalid for the specified         method.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.Optimizer","title":"ropt.plugins.optimizer.base.Optimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for Optimizer Implementations.</p> <p>This class defines the fundamental interface for all concrete optimizer implementations within the <code>ropt</code> framework. Optimizer plugins provide classes derived from <code>Optimizer</code> that encapsulate the logic of specific optimization algorithms.</p> <p>Instances of <code>Optimizer</code> subclasses are created by their corresponding <code>OptimizerPlugin</code> factories. They are initialized with an <code>EnOptConfig</code> object detailing the optimization setup and an <code>OptimizerCallback</code> function. The callback is crucial as it allows the optimizer to request function and gradient evaluations from the <code>ropt</code> core during its execution.</p> <p>The optimization process itself is initiated by calling the <code>start</code> method, which must be implemented by subclasses.</p> <p>Subclasses must implement: - <code>__init__</code>: To accept the configuration and callback. - <code>start</code>: To contain the main optimization loop.</p> <p>Subclasses can optionally override: - <code>allow_nan</code>:   To indicate if the algorithm can handle NaN function values. - <code>is_parallel</code>: To indicate if the algorithm may perform parallel evaluations.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.Optimizer.allow_nan","title":"allow_nan  <code>property</code>","text":"<pre><code>allow_nan: bool\n</code></pre> <p>Indicate whether the optimizer can handle NaN function values.</p> <p>If an optimizer algorithm can gracefully handle <code>NaN</code> (Not a Number) objective function values, its implementation should override this property to return <code>True</code>.</p> <p>This is particularly relevant in ensemble-based optimization where evaluations might fail for all realizations. When <code>allow_nan</code> is <code>True</code>, setting <code>realization_min_success</code> to zero allows the evaluation process to return <code>NaN</code> instead of raising an error, enabling the optimizer to potentially continue.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the optimizer supports NaN function values.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.Optimizer.is_parallel","title":"is_parallel  <code>property</code>","text":"<pre><code>is_parallel: bool\n</code></pre> <p>Indicate whether the optimizer alows parallel evaluations.</p> <p>If an optimizer algorithm is designed to evaluate multiple variable vectors concurrently, its implementation should override this property to return <code>True</code>.</p> <p>This information can be used by <code>ropt</code> or other components to manage resources or handle parallel execution appropriately.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the optimizer allows parallel evaluations.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.Optimizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    config: EnOptConfig,\n    optimizer_callback: OptimizerCallback,\n) -&gt; None\n</code></pre> <p>Initialize an optimizer object.</p> <p>The <code>config</code> object provides the desired configuration for the optimization process and should be used to set up the optimizer correctly before starting the optimization. The optimization will be initiated using the <code>start</code> method and will repeatedly require function and gradient evaluations at given variable vectors. The <code>optimizer_callback</code> argument provides the function that should be used to calculate the function and gradient values, which can then be forwarded to the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The optimizer configuration to used.</p> required <code>optimizer_callback</code> <code>OptimizerCallback</code> <p>The optimizer callback.</p> required"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.base.Optimizer.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(initial_values: NDArray[float64]) -&gt; None\n</code></pre> <p>Initiate the optimization process.</p> <p>This abstract method must be implemented by concrete <code>Optimizer</code> subclasses to start the optimization process. It takes the initial set of variable values as input.</p> <p>During execution, the implementation should use the <code>OptimizerCallback</code> (provided during initialization) to request necessary function and gradient evaluations from the <code>ropt</code> core.</p> <p>Parameters:</p> Name Type Description Default <code>initial_values</code> <code>NDArray[float64]</code> <p>A 1D NumPy array representing the starting variable             values for the optimization.</p> required"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils","title":"ropt.plugins.optimizer.utils","text":"<p>Utility functions for use by optimizer plugins.</p> <p>This module provides utility functions to validate supported constraints, filter linear constraints, and to retrieve the list of supported optimizers.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints","title":"NormalizedConstraints","text":"<p>Class for handling normalized constraints.</p> <p>This class can be used to normalize non-linear constraints into the form C(x) = 0, C(x) &lt;= 0, or C(x) &gt;= 0. By default this is done by subtracting the right-hand side value, and multiplying with -1, if necessary.</p> <p>The right hand sides are provided by the <code>lower_bounds</code> and <code>upper_bound</code> values. If corresponding entries in these arrays are equal (within a 1e-15 tolerance), the corresponding constraint is assumed to be a equality constraint. If they are not, they are considered inequality constraints, if one or both values are finite. If the lower bounds are finite, the constraint is added as is, after subtracting of the lower bound. If the upper bound is finite, the same is done, but the constraint is multiplied by -1. If both are finite, both constraints are added, effectively splitting a two-sided constraint into two normalized constraints.</p> <p>By default this normalizes inequality constraints to the form C(x) &gt; 0, by setting the <code>flip</code> flag, this can be changed to C(x) &lt; 0.</p> <p>Usage:</p> <ol> <li>Initialize with the lower and upper bounds.</li> <li>Before each new function/gradient evaluation with a new variable     vector, reset the normalized constraints by calling the <code>reset</code>     method.</li> <li>The constraint values are given by the <code>constraints</code> property. Before     accessing it, call the <code>set_constraints</code> with the raw constraints. If     necessary, this will calculate and cache the normalized values. Since     values are cached, calling this method and accessing <code>constraints</code>     multiple times is cheap.</li> <li>Use the same procedure for gradients, using the <code>gradients</code> property     and <code>set_gradients</code>. Raw gradients must be provided as a matrix,     where the rows are the gradients of each constraint.</li> <li>Use the <code>is_eq</code> property to retrieve a vector of boolean flags to     check which constraints are equality constraints.</li> </ol> <p>See the <code>scipy</code> optimization backend in the <code>ropt</code> source code for an example of usage.</p> Parallel evaluation. <p>The raw constraints may be a vector of constraints, or may be a matrix of constraints for multiple variables to support parallel evaluation. In the latter case, the constraints for different variables are given by the columns of the matrix. In this case, the <code>constraints</code> property will have the same structure. Note that this is only supported for the constraint values, not for the gradients. Hence, parallel evaluation of multiple gradients is not supported.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.is_eq","title":"is_eq  <code>property</code>","text":"<pre><code>is_eq: list[bool]\n</code></pre> <p>Return flags indicating which constraints are equality constraints.</p> <p>Returns:</p> Type Description <code>list[bool]</code> <p>A list of booleans, <code>True</code> for constraints that are equality constraints.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.constraints","title":"constraints  <code>property</code>","text":"<pre><code>constraints: NDArray[float64] | None\n</code></pre> <p>Return the cached normalized constraint values.</p> <p>These are the constraint values after applying the normalization logic (subtracting RHS, potential sign flipping) based on the bounds provided during initialization.</p> <p>This property should be accessed after calling <code>set_constraints</code> with the raw constraint values for the current variable vector. Returns <code>None</code> if <code>set_constraints</code> has not been called since the last <code>reset</code>.</p> <p>Returns:</p> Type Description <code>NDArray[float64] | None</code> <p>A NumPy array containing the normalized constraint values.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.gradients","title":"gradients  <code>property</code>","text":"<pre><code>gradients: NDArray[float64] | None\n</code></pre> <p>Return the cached normalized constraint gradients.</p> <p>These are the gradients of the constraints after applying the normalization logic (potential sign flipping) based on the bounds provided during initialization.</p> <p>This property should be accessed after calling <code>set_gradients</code> with the raw constraint gradients for the current variable vector. Returns <code>None</code> if <code>set_gradients</code> has not been called since the last <code>reset</code>.</p> <p>Returns:</p> Type Description <code>NDArray[float64] | None</code> <p>A 2D NumPy array containing the normalized constraint gradients.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.__init__","title":"__init__","text":"<pre><code>__init__(*, flip: bool = False) -&gt; None\n</code></pre> <p>Initialize the normalization class.</p> <p>Parameters:</p> Name Type Description Default <code>flip</code> <code>bool</code> <p>Whether to flip the sign of the constraints.</p> <code>False</code>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.set_bounds","title":"set_bounds","text":"<pre><code>set_bounds(\n    lower_bounds: NDArray[float64],\n    upper_bounds: NDArray[float64],\n) -&gt; None\n</code></pre> <p>Set the bounds of the normalization class.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bounds</code> <code>NDArray[float64]</code> <p>The lower bounds on the right hand sides.</p> required <code>upper_bounds</code> <code>NDArray[float64]</code> <p>The upper bounds on the right hand sides.</p> required"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset cached normalized constraints and gradients.</p> <p>This must be called when the stored constraints and their gradients are no longer valid. This is typically done after a new function/gradient evaluation. The <code>set_constraints</code> and <code>set_gradients</code> methods can then be called to set the new values.</p> <p>After calling this method, the <code>constraints</code> and <code>gradients</code> properties will return <code>None</code> until new values are set. This can be utilized to check if new values need to be calculated.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.set_constraints","title":"set_constraints","text":"<pre><code>set_constraints(values: NDArray[float64]) -&gt; None\n</code></pre> <p>Calculate and cache normalized constraint values.</p> <p>This method takes the raw constraint values (evaluated at the current variable vector) and applies the normalization logic defined during initialization (subtracting RHS, potential sign flipping). The results are stored internally and made available via the <code>constraints</code> property.</p> <p>This supports parallel evaluation: if <code>values</code> is a 2D array, each row is treated as the constraint values for a separate variable vector evaluation.</p> <p>If there are already cached values, this method will not overwrite them, the <code>reset</code> method must be called first.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>A 1D or 2D NumPy array of raw constraint values. If 2D,     rows represent different evaluations.</p> required"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.NormalizedConstraints.set_gradients","title":"set_gradients","text":"<pre><code>set_gradients(values: NDArray[float64]) -&gt; None\n</code></pre> <p>Calculate and cache normalized constraint gradients.</p> <p>This method takes the raw constraint gradients (evaluated at the current variable vector) and applies the normalization logic defined during initialization (potential sign flipping). The results are stored internally and made available via the <code>gradients</code> property.</p> <p>If there are already cached values, this method will not overwrite them, the <code>reset</code> method must be called first.</p> Note <p>Unlike <code>set_constraints</code>, this method does not support parallel evaluation; it expects gradients corresponding to a single variable vector.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[float64]</code> <p>A 2D NumPy array of raw constraint gradients (rows are     gradients of original constraints, columns are variables).</p> required"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.validate_supported_constraints","title":"validate_supported_constraints","text":"<pre><code>validate_supported_constraints(\n    config: EnOptConfig,\n    method: str,\n    supported_constraints: dict[str, set[str]],\n    required_constraints: dict[str, set[str]],\n) -&gt; None\n</code></pre> <p>Validate if the configured constraints are supported by the chosen method.</p> <p>This function checks if the constraints defined in the <code>config</code> object (bounds, linear, non-linear) are compatible with the specified optimization <code>method</code>. It uses dictionaries mapping constraint types to sets of methods that support or require them.</p> <p>Constraint types are identified by keys like <code>\"bounds\"</code>, <code>\"linear:eq\"</code>, <code>\"linear:ineq\"</code>, <code>\"nonlinear:eq\"</code>, and <code>\"nonlinear:ineq\"</code>.</p> <p>Example <code>supported_constraints</code> dictionary: <pre><code>{\n    \"bounds\": {\"L-BFGS-B\", \"TNC\", \"SLSQP\"},\n    \"linear:eq\": {\"SLSQP\"},\n    \"linear:ineq\": {\"SLSQP\"},\n    \"nonlinear:eq\": {\"SLSQP\"},\n    \"nonlinear:ineq\": {\"SLSQP\"},\n}\n</code></pre> A similar structure is used for <code>required_constraints</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The optimization configuration object.</p> required <code>method</code> <code>str</code> <p>The name of the optimization method being used.</p> required <code>supported_constraints</code> <code>dict[str, set[str]]</code> <p>Dict mapping constraint types to sets of methods                    that support them.</p> required <code>required_constraints</code> <code>dict[str, set[str]]</code> <p>Dict mapping constraint types to sets of methods                    that require them.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a configured constraint is not supported by the                  method, or if a required constraint is missing.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.create_output_path","title":"create_output_path","text":"<pre><code>create_output_path(\n    base_name: str,\n    base_dir: Path | None = None,\n    name: str | None = None,\n    suffix: str | None = None,\n) -&gt; Path\n</code></pre> <p>Construct a unique output path, appending an index if necessary.</p> <p>This function generates a file or directory path based on the provided components. If the resulting path already exists, it automatically appends or increments a numerical suffix (e.g., \"-001\", \"-002\") to ensure uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>base_name</code> <code>str</code> <p>The core name for the path.</p> required <code>base_dir</code> <code>Path | None</code> <p>Optional parent directory for the path.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional identifier to include in the path.</p> <code>None</code> <code>suffix</code> <code>str | None</code> <p>Optional file extension or suffix for the path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>A unique <code>pathlib.Path</code> object.</p>"},{"location":"reference/optimizer_plugins/#ropt.plugins.optimizer.utils.get_masked_linear_constraints","title":"get_masked_linear_constraints","text":"<pre><code>get_masked_linear_constraints(\n    config: EnOptConfig, initial_values: NDArray[float64]\n) -&gt; tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]\n</code></pre> <p>Adjust linear constraints based on a variable mask.</p> <p>When an optimization problem uses a variable mask (<code>config.variables.mask</code>) to optimize only a subset of variables, the linear constraints need to be adapted. This function performs that adaptation.</p> <p>It removes columns from the constraint coefficient matrix (<code>config.linear_constraints.coefficients</code>) that correspond to the masked (fixed) variables. The contribution of these fixed variables (using their <code>initial_values</code>) is then calculated and subtracted from the original lower and upper bounds (<code>config.linear_constraints.lower_bounds</code>, <code>config.linear_constraints.upper_bounds</code>) to produce adjusted bounds for the optimization involving only the active variables.</p> <p>Additionally, any constraint rows that originally involved only masked variables (i.e., all coefficients for active variables in that row are zero) are removed entirely, as they become trivial constants.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The <code>EnOptConfig</code> object             containing the variable mask and linear constraints.</p> required <code>initial_values</code> <code>NDArray[float64]</code> <p>The initial values to use.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray[float64], NDArray[float64], NDArray[float64]]</code> <p>The adjusted coefficients and bounds.</p>"},{"location":"reference/plugin_manager/","title":"Plugin Manager","text":""},{"location":"reference/plugin_manager/#ropt.plugins","title":"ropt.plugins","text":"<p>Extending <code>ropt</code> with plugins.</p> <p>The <code>ropt.plugins</code> module provides the framework for extending <code>ropt</code>'s capabilities through a plugin system. Plugins allow for the integration of custom or third-party components, installed as separate packages.</p> <p><code>ropt</code> supports several types of plugins, each addressing a specific aspect of the optimization workflow:</p> <p>Workflow related plugins:</p> <p>Workflow related plugins may directly be used by the user to create components when building optimization workflows, generally via these convenience functions:</p> <ul> <li>create_compute_step: Create     compute steps, such as optimizations, to run during workflow execution.</li> <li>create_event_handler:     Create handlers to process events generated during optimization.</li> <li>create_evaluator: Create     evaluators to for use by compute steps.</li> </ul> <p>Optimizer related plugins:</p> <p>These plugins are used to implement specific features of the ensemble optimizer:</p> <ul> <li><code>optimizer</code>: Implements optimization algorithms.</li> <li><code>sampler</code>: Generates parameter perturbations, which   are used for gradient estimation.</li> <li><code>realization_filter</code>: Selects subsets of   ensemble realizations for calculating objectives or constraints.</li> <li><code>function_estimator</code>: Computes final   objective function values and gradients from individual realization results.</li> </ul> <p>These plugins are generally not directly created by the user. Instead, the optimization algorithm will find and create the required plugins based on information passed via the optimization configuration.</p> <p>Plugin Management and Discovery</p> <p>The <code>PluginManager</code> class is central to the plugin system. It discovers and manages available plugins. Plugins are typically discovered automatically using Python's standard entry points mechanism.</p> <p>Each plugin type has a corresponding abstract base class that custom plugins must inherit from:</p> <ul> <li>Workflow plugins: <code>ComputeStepPlugin</code>,   <code>EventHandlerPlugin</code>,   <code>EvaluatorPlugin</code></li> <li>Optimizer: <code>OptimizerPlugin</code></li> <li>Sampler: <code>SamplerPlugin</code></li> <li>Realization Filter: <code>RealizationFilterPlugin</code></li> <li>Function Estimator: <code>FunctionEstimatorPlugin</code></li> </ul> <p>Using Plugins</p> <p>The <code>PluginManager.get_plugin</code> method is used internally by <code>ropt</code> to retrieve the appropriate plugin implementation based on a specified type and method name. The <code>PluginManager.get_plugin_name</code> method can be used to find the name of a plugin that supports a given method.</p> <p>Plugins can implement multiple named methods. To request a specific method (<code>method-name</code>) from a particular plugin (<code>plugin-name</code>), use the format <code>\"plugin-name/method-name\"</code>. If only a method name is provided, the plugin manager searches through all registered plugins (that allow discovery) for one that supports the method. Using <code>\"plugin-name/default\"</code> typically selects the primary or default method offered by that plugin, although specifying <code>\"default\"</code> without a plugin name is not permitted.</p> <p>Plugins retrieved by the <code>PluginManager.get_plugin</code> method generally implement a <code>create</code> factory method that will be used to instantiate the objects that implement the desired functionality. These objects must inherit from the base class for the corresponding plugin type:</p> <ul> <li>Workflow related plugins:   <code>ComputeStep</code>,   <code>EventHandler</code>,   <code>Evaluator</code></li> <li>Optimizer: <code>Optimizer</code></li> <li>Sampler: <code>Sampler</code></li> <li>Realization Filter:   <code>RealizationFilter</code></li> <li>Function Estimator:   <code>FunctionEstimator</code></li> </ul> <p>The create_evaluator, create_event_handler and create_compute_step convenience functions combine plugin discovery and object creation into a single call.</p> <p>Pre-installed Plugins Included with <code>ropt</code></p> <p><code>ropt</code> comes bundled with a set of pre-installed plugins:</p> <ul> <li>Workflow: The built-in   <code>default compute step</code>,   <code>default event handler</code>   and <code>default evaluator</code> plugins,   providing components for executing complex optimization workflows.</li> <li>Optimizer: The <code>scipy</code>   plugin, leveraging algorithms from <code>scipy.optimize</code>, and the   <code>ExternalOptimizer</code> plugin,   which is used to launch optimizers in separate processes.</li> <li>Sampler: The <code>scipy</code> plugin,   using distributions from <code>scipy.stats</code>.</li> <li>Realization Filter: The   <code>default</code>   plugin, offering filters based on ranking and for CVaR optimization.</li> <li>Function Estimator: The   <code>default</code>   plugin, supporting objectives based on mean or standard deviation.</li> </ul>"},{"location":"reference/plugin_manager/#ropt.plugins.PluginManager","title":"PluginManager","text":"<p>Manages the discovery and retrieval of <code>ropt</code> plugins.</p> <p>The <code>PluginManager</code> is responsible for finding available plugins based on Python's entry points mechanism and providing access to them. It serves as a central registry for different types of plugins used within <code>ropt</code>, such as optimizers, samplers, and workflow components.</p> <p>Upon initialization, the manager scans for entry points defined under the <code>ropt.plugins.*</code> groups (e.g., <code>ropt.plugins.optimizer</code>). Plugins found this way are loaded and stored internally, categorized by their type.</p> <p>The primary way to interact with the manager is through the <code>get_plugin</code> method, which retrieves a specific plugin class based on its type and a method name it supports. The <code>get_plugin_name</code> method can be used to find the name of a plugin that supports a given method.</p> <p>Example: Registering a Custom Optimizer Plugin</p> <p>To make a custom optimizer plugin available to <code>ropt</code>, you would typically define an entry point in your package's <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"ropt.plugins.optimizer\"]\nmy_optimizer = \"my_package.my_module:MyOptimizer\"\n</code></pre> <p>When <code>ropt</code> initializes the <code>PluginManager</code>, it will discover and load <code>MyOptimizer</code> from <code>my_package.my_module</code>, making it accessible via <code>plugin_manager.get_plugin(\"optimizer\", \"my_optimizer/some_method\")</code> or potentially <code>plugin_manager.get_plugin(\"optimizer\", \"some_method\")</code> if discovery is allowed and the method is unique.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.PluginManager.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the plugin manager.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.PluginManager.get_plugin","title":"get_plugin","text":"<pre><code>get_plugin(plugin_type: PluginType, method: str) -&gt; Any\n</code></pre> <p>Retrieve a plugin class by its type and a supported method name.</p> <p>This method finds and returns the class of a plugin that matches the specified <code>plugin_type</code> and supports the given <code>method</code>.</p> <p>The <code>method</code> argument can be specified in two ways:</p> <ol> <li>Explicit Plugin: Use the format <code>\"plugin-name/method-name\"</code>.     This directly requests the <code>method-name</code> from the plugin named     <code>plugin-name</code>.</li> <li>Implicit Plugin: Provide only the <code>method-name</code>. The manager     will search through all registered plugins of the specified     <code>plugin_type</code> that allow discovery (see     <code>Plugin.allows_discovery</code>).     It returns the first plugin found that supports the <code>method-name</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>plugin_type</code> <code>PluginType</code> <p>The category of the plugin (e.g., \"optimizer\", \"sampler\").</p> required <code>method</code> <code>str</code> <p>The name of the method the plugin must support, potentially          prefixed with the plugin name and a slash (<code>/</code>).</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The plugin class that matches the criteria.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching plugin is found for the given type and         method, or if \"default\" is used as a method name without         specifying a plugin name.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.PluginManager.get_plugin_name","title":"get_plugin_name","text":"<pre><code>get_plugin_name(\n    plugin_type: PluginType, method: str\n) -&gt; str | None\n</code></pre> <p>Return the name of the plugin that supports a given method.</p> <p>Verifies whether a plugin of the specified <code>plugin_type</code> supports the given <code>method</code>. This is useful for checking availability before attempting to retrieve a plugin with <code>get_plugin</code>.</p> <p>The <code>method</code> argument can be specified in two ways:</p> <ol> <li>Explicit Plugin: <code>\"plugin-name/method-name\"</code> checks if the specific     plugin named <code>plugin-name</code> supports <code>method-name</code>.</li> <li>Implicit Plugin: <code>\"method-name\"</code> searches through all discoverable     plugins of the given <code>plugin_type</code> to see if any support <code>method-name</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>plugin_type</code> <code>PluginType</code> <p>The category of the plugin (e.g., \"optimizer\", \"sampler\").</p> required <code>method</code> <code>str</code> <p>The name of the method to check, potentially prefixed          with the plugin name and a slash (<code>/</code>).</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The name of a matching plugin supporting the specified method, or <code>None</code>.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.PluginType","title":"PluginType  <code>module-attribute</code>","text":"<pre><code>PluginType = Literal[\n    \"optimizer\",\n    \"sampler\",\n    \"realization_filter\",\n    \"function_estimator\",\n    \"event_handler\",\n    \"compute_step\",\n    \"evaluator\",\n]\n</code></pre> <p>Represents the valid types of plugins supported by <code>ropt</code>.</p> <p>This type alias defines the string identifiers used to categorize different plugins within the <code>ropt</code> framework. Each identifier corresponds to a specific role in the optimization process:</p> <ul> <li><code>\"optimizer\"</code>: Plugins implementing optimization algorithms   (<code>OptimizerPlugin</code>).</li> <li><code>\"sampler\"</code>: Plugins for generating parameter samples   (<code>SamplerPlugin</code>).</li> <li><code>\"realization_filter\"</code>: Plugins for filtering ensemble realizations   (<code>RealizationFilterPlugin</code>).</li> <li><code>\"function_estimator\"</code>: Plugins for estimating objective functions and gradients   (<code>FunctionEstimatorPlugin</code>).</li> <li><code>\"event_handler\"</code>: Plugins that create event handlers for processing optimization   results (<code>EventHandlerPlugin</code>).</li> <li><code>\"compute_step\"</code>: Plugins that define executable steps within an optimization workflow   (<code>ComputeStepPlugin</code>).</li> <li><code>\"evaluator\"</code>: Plugins that define evaluators within an optimization workflow   (<code>EvaluatorPlugin</code>).</li> </ul>"},{"location":"reference/plugin_manager/#ropt.plugins.Plugin","title":"Plugin","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all <code>ropt</code> plugins.</p> <p>This class serves as the fundamental building block for all plugins within the <code>ropt</code> framework. Any class intended to function as a plugin (e.g., an optimizer, sampler, or event handler) must inherit from this base class.</p> <p>It defines the core interface that all plugins must adhere to, ensuring consistency and enabling the <code>PluginManager</code> to discover and manage them effectively.</p> <p>Subclasses must implement the <code>is_supported</code> class method to indicate which named methods (functionalities) they provide. They can optionally override the <code>allows_discovery</code> class method if they should not be automatically selected by the plugin manager when a method name is provided without an explicit plugin name.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.Plugin.is_supported","title":"is_supported  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>is_supported(method: str) -&gt; bool\n</code></pre> <p>Verify if this plugin supports a specific named method.</p> <p>This class method is used by the <code>PluginManager</code> (specifically its <code>get_plugin_name</code> method) to determine if this plugin class provides the functionality associated with the given <code>method</code> name.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The string identifier of the method to check for support.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the plugin supports the specified method, <code>False</code> otherwise.</p>"},{"location":"reference/plugin_manager/#ropt.plugins.Plugin.allows_discovery","title":"allows_discovery  <code>classmethod</code>","text":"<pre><code>allows_discovery() -&gt; bool\n</code></pre> <p>Determine if the plugin allows implicit discovery by method name.</p> <p>By default (<code>True</code>), plugins can be found by the <code>PluginManager</code> when a user provides only a method name (without specifying the plugin, e.g., <code>\"method-name\"</code>).</p> <p>If a plugin should only be used when explicitly named (e.g., <code>\"plugin-name/method-name\"</code>), it must override this class method to return <code>False</code>.</p> <p>For instance, the <code>external</code> optimizer plugin acts as a wrapper for other optimizers run in separate processes. It doesn't provide methods directly and must always be explicitly requested, so it overrides this method to return <code>False</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the plugin can be discovered implicitly by method name.</p>"},{"location":"reference/realization_filter_plugins/","title":"Realization Filters","text":""},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter","title":"ropt.plugins.realization_filter","text":"<p>Provides plugin functionality for adding realization filters.</p> <p>Realization filters are used by the optimization process to determine how the results from a set of realizations should be weighted when evaluating the overall objective and constraint functions. This module allows for the extension of <code>ropt</code> with custom realization filtering strategies.</p> <p>Core Concepts:</p> <ul> <li>Plugin Interface: Realization filter plugins must inherit from the   <code>RealizationFilterPlugin</code>   base class. This class acts as a factory, defining a <code>create</code> method to   instantiate filter objects.</li> <li>Filter Implementation: The actual filtering logic resides in classes that   inherit from the   <code>RealizationFilter</code>   abstract base class. These classes are initialized with the optimization   configuration (<code>EnOptConfig</code>) and the index of the   specific filter configuration to use (<code>filter_index</code>). The core functionality   is provided by the <code>get_realization_weights</code> method, which calculates and   returns weights for each realization based on their objective and constraint   values.</li> <li>Discovery: The <code>PluginManager</code> discovers   available <code>RealizationFilterPlugin</code> implementations (typically via entry   points) and uses them to create <code>RealizationFilter</code> instances as needed.</li> </ul> <p>Built-in Realization Filter Plugins:</p> <p>The default <code>DefaultRealizationFilter</code> class provides several filtering methods, including sorting by objective/constraint values and Conditional Value-at-Risk (CVaR) based weighting.</p>"},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter.base.RealizationFilterPlugin","title":"ropt.plugins.realization_filter.base.RealizationFilterPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base Class for Realization Filter Plugins (Factories).</p> <p>This class defines the interface for plugins responsible for creating <code>RealizationFilter</code> instances. These plugins act as factories for specific realization filtering strategies.</p>"},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter.base.RealizationFilterPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(\n    enopt_config: EnOptConfig, filter_index: int\n) -&gt; RealizationFilter\n</code></pre> <p>Factory method to create a concrete RealizationFilter instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>RealizationFilter</code> objects. Plugin implementations must override this method to return an instance of their specific <code>RealizationFilter</code> subclass.</p> <p>The <code>PluginManager</code> calls this method when an optimization requires realization weights calculated by this plugin.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The main EnOpt configuration object.</p> required <code>filter_index</code> <code>int</code> <p>Index into <code>enopt_config.realization_filters</code> for           this filter.</p> required <p>Returns:</p> Type Description <code>RealizationFilter</code> <p>An initialized RealizationFilter object ready for use.</p>"},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter.base.RealizationFilter","title":"ropt.plugins.realization_filter.base.RealizationFilter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for realization filter classes.</p>"},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter.base.RealizationFilter.__init__","title":"__init__","text":"<pre><code>__init__(\n    enopt_config: EnOptConfig, filter_index: int\n) -&gt; None\n</code></pre> <p>Initialize the realization filter plugin.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The configuration of the optimizer.</p> required <code>filter_index</code> <code>int</code> <p>The index of the filter to use.</p> required"},{"location":"reference/realization_filter_plugins/#ropt.plugins.realization_filter.base.RealizationFilter.get_realization_weights","title":"get_realization_weights  <code>abstractmethod</code>","text":"<pre><code>get_realization_weights(\n    objectives: NDArray[float64],\n    constraints: NDArray[float64] | None,\n) -&gt; NDArray[np.float64]\n</code></pre> <p>Return the updated weights of the realizations.</p> <p>This method is called by the optimizer with the current values of the objectives and constraints. Based on these values it must decide how much weight each realization should be given, and return those as a vector.</p> <p>The objectives and the constraints are passed as matrices, where the columns contain the values of the objectives or constraints. The index along the row axis corresponds to the number of the realization.</p> Normalization <p>The weights will be normalized to a sum of one by the optimizer before use, hence any non-negative weight value is permissable.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>NDArray[float64]</code> <p>The objectives of all realizations.</p> required <code>constraints</code> <code>NDArray[float64] | None</code> <p>The constraints for all realizations.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>A vector of weights of the realizations.</p>"},{"location":"reference/results/","title":"Optimization Results","text":""},{"location":"reference/results/#ropt.results","title":"ropt.results","text":"<p>Data classes for storing intermediate optimization results.</p> <p>During the optimization process, the calculation of functions and gradients generates data that needs to be reported. To facilitate this, new results are passed to callbacks as a sequence of <code>Results</code> objects. These objects can be instances of either the <code>FunctionResults</code> or <code>GradientResults</code> classes, which store the results of function and gradient evaluations, respectively.</p> <p>Much of the data within these result objects is multi-dimensional. For example, the <code>objectives</code> field, which is part of the nested <code>evaluations</code> object within <code>FunctionResults</code>, is a two-dimensional <code>numpy</code> array. In this array, each column represents a different objective, and each row corresponds to a specific realization number.</p> <p>To simplify exporting and reporting, the identity of the axes in these multi-dimensional arrays is stored as metadata associated with each field. These fields are derived from the <code>ResultField</code> class, which provides a <code>get_axes</code> class method for retrieving the axes. For instance, for the <code>objectives</code> field, this method would return:</p> <p><pre><code>&gt;&gt;&gt; from ropt.results import FunctionEvaluations\n&gt;&gt;&gt; FunctionEvaluations.get_axes(\"objectives\")\n(&lt;AxisName.REALIZATION: 'realization'&gt;, &lt;AxisName.OBJECTIVE: 'objective'&gt;)\n</code></pre> Given that the first axis denotes realizations and the second axis denotes objectives, each row in the array represents the set of objective values for a specific realization. This metadata provides the necessary context for exporting and reporting code to associate each element in the result matrix with its corresponding realization and objective, as specified in the optimizer configuration. The pandas exporting code, for example, utilizes this information to construct a multi-index for the output DataFrame and to transform the multi-dimensional data into multiple columns.</p> <p>The <code>AxisName</code> enumeration currently defines the following axes:</p> <ul> <li><code>AxisName.OBJECTIVE</code> The index along this   axis refers to the objective number as specified in the   <code>EnOptConfig</code> configuration.</li> <li><code>AxisName.NONLINEAR_CONSTRAINT</code>   The index along this axis corresponds to the non-linear constraint index   defined in the <code>EnOptConfig</code> configuration.</li> <li><code>AxisName.LINEAR_CONSTRAINT</code> The   index along this axis corresponds to the linear constraint index defined in   the <code>EnOptConfig</code> configuration.</li> <li><code>AxisName.VARIABLE</code> The index along this axis   refers to the variable number as specified by the   <code>EnOptConfig</code> configuration.</li> <li><code>AxisName.REALIZATION</code>: When results   involve an ensemble, this axis represents the different realizations, where   the index corresponds to the realization number.</li> <li><code>AxisName.PERTURBATION</code> For gradient   calculations, multiple variable perturbations are used. The objectives and   constraints calculated for each perturbation are reported along this axis,   which represents the perturbation index.</li> </ul> <p>Refer to the documentation of the individual result classes for the exact dimensions of each result field. The dimensionality of the data and the order of the axes are fixed and listed sequentially for every field.</p> Note <p>The dimensionality associated with a result axis is fixed. For instance, even with only a single objective, results containing objective values will still include a <code>AxisName.OBJECTIVE</code> axis of length one.</p>"},{"location":"reference/results/#ropt.results.Results","title":"ropt.results.Results  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storing optimization results.</p> <p>The <code>Results</code> class serves as a foundation for storing various types of optimization results. It is not intended to be instantiated directly but rather serves as a base for derived classes like <code>FunctionResults</code> and <code>GradientResults</code>, which hold the actual data.</p> <p>This class provides storage for the following generic information:</p> <ul> <li>Batch ID: An optional identifier, potentially generated by the     function evaluator, that uniquely identifies a group of function     evaluations passed to the evaluator by teh optimizer.</li> <li>Metadata: A dictionary for storing additional information generated     during optimization. This metadata can include various primitive values     that are not directly interpreted by the optimization code but are     useful for reporting and analysis.</li> <li>Names: The optional <code>names</code> attribute is a dictionary that stores     the names of the various entities, such as variables, objectives, and     constraints. The supported name types are defined in the     <code>AxisName</code> enumeration. This information is     optional, as it is not strictly necessary for the optimization, but it     can be useful for labeling and interpreting results. For instance, when     present, it is used to create a multi-index results that are exported as     data frames.</li> </ul> <p>The derived classes, <code>FunctionResults</code> and <code>GradientResults</code>, extend this base class with specific attributes for storing function evaluation results and gradient evaluation results, respectively. These derived classes also provide methods for exporting the stored data.</p> <p>One key method provided by the <code>Results</code> class is <code>to_dataframe</code>, which allows exporting the contents of a specific field, or a subset of its sub-fields, to a <code>pandas</code> DataFrame for further data analysis and reporting.</p> <p>Attributes:</p> Name Type Description <code>batch_id</code> <code>int | None</code> <p>The ID of the evaluation batch.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>A dictionary of metadata.</p> <code>names</code> <code>dict[str, tuple[str | int, ...]]</code> <p>Optional names of the various result axes.</p>"},{"location":"reference/results/#ropt.results.Results.transform_from_optimizer","title":"transform_from_optimizer  <code>abstractmethod</code>","text":"<pre><code>transform_from_optimizer(\n    config: EnOptConfig, transforms: OptModelTransforms\n) -&gt; Results\n</code></pre> <p>Transform results from the optimizer domain to the user domain.</p> <p>During optimization, variables, objectives, and constraints are often transformed to a different domain (the optimizer domain) to enhance the performance and stability of the optimization algorithm. The <code>Results</code> objects produced during optimization are initially in the optimizer domain. This method reverses these transformations, mapping the results back to the user-defined domain. The transformations between the user and optimizer domains are defined by the classes in the <code>ropt.transforms</code> module.</p> <p>For instance, variables might have been scaled and shifted to a range more suitable for the optimizer. This method, using the provided <code>OptModelTransforms</code> object, applies the inverse scaling and shifting to restore the variables to their original scale and offset. Similarly, objectives and constraints are transformed back to the user domain.</p> <p>These transformations are defined and managed by the <code>OptModelTransforms</code> object, which encapsulates the specific transformations for variables, objectives, and nonlinear constraints.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EnOptConfig</code> <p>The configuration used by the source of the results.</p> required <code>transforms</code> <code>OptModelTransforms</code> <p>The transforms to apply.</p> required <p>Returns:</p> Type Description <code>Results</code> <p>A new <code>FunctionResults</code> object with all relevant data transformed</p> <code>Results</code> <p>back to the user domain.</p>"},{"location":"reference/results/#ropt.results.Results.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(\n    field_name: str,\n    select: Iterable[str],\n    unstack: Iterable[AxisName] | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Export a field to a pandas DataFrame.</p> <p>Exports the values of a single field to a <code>pandas</code> DataFrame. The field to export is selected by the <code>field_name</code> argument. Typically, such a field contains multiple sub-fields. By default, all sub-fields are exported as columns in the DataFrame, but a subset can be selected using the <code>select</code> argument.</p> <p>Sub-fields may be multi-dimensional arrays, which are exported in a stacked manner. Using the axis types found in the metadata, the exporter constructs a multi-index labeled with the corresponding names provided via the <code>names</code> field. If <code>names</code> does not contain a key/value pair for the the axis, numerical indices are used. These multi-indices can optionally be unstacked into multiple columns by providing the axis types to unstack via the <code>unstack</code> argument.</p> The DataFrame Index <p>The index of the resulting DataFrame may be a multi-index constructed from axis indices or labels. In addition, the <code>batch_id</code> (if not <code>None</code>) is prepended to the index.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>The field to export.</p> required <code>select</code> <code>Iterable[str]</code> <p>Select the sub-fields to export. By default, all         sub-fields are exported.</p> required <code>unstack</code> <code>Iterable[AxisName] | None</code> <p>Select axes to unstack. By default, no axes are         unstacked.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the <code>pandas</code> module is not installed.</p> <code>ValueError</code> <p>If the field name is incorrect.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas</code> DataFrame containing the results.</p> Warning <p>This function requires the <code>pandas</code> module to be installed.</p>"},{"location":"reference/results/#ropt.results.ResultField","title":"ropt.results.ResultField  <code>dataclass</code>","text":"<p>Base class for fields within <code>Results</code> objects.</p> <p>The <code>ResultField</code> class serves as a foundation for defining the various data fields that can be stored within <code>Results</code> objects. These fields typically hold multi-dimensional numerical data, such as objective values, constraint values, or gradients.</p> <p>This class provides a standardized way to:</p> <ul> <li>Store metadata about the axes of multi-dimensional arrays.</li> <li>Retrieve the axes associated with a specific field.</li> </ul> <p>Derived classes, such as <code>FunctionEvaluations</code> or <code>Gradients</code>, extend this base class to define specific data structures for different types of optimization results.</p>"},{"location":"reference/results/#ropt.results.ResultField.get_axes","title":"get_axes  <code>classmethod</code>","text":"<pre><code>get_axes(name: str) -&gt; tuple[AxisName, ...]\n</code></pre> <p>Retrieve the axes associated with a specific field.</p> <p>Fields within a <code>ResultField</code> object that store multi-dimensional <code>numpy</code> arrays, contain metadata that describes the meaning of each dimension in the array. This method retrieves the axes of a field within a ResultField object from that meta-data, returning a tuple of <code>AxisName</code>][ropt.enums.AxisName] enums.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field (sub-field) within the   <code>ResultField</code> instance or class.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided field name is not recognized.</p> <p>Returns:</p> Type Description <code>tuple[AxisName, ...]</code> <p>A tuple of <code>AxisName</code> enums, representing the axes of the field.</p>"},{"location":"reference/results/#ropt.results.FunctionResults","title":"ropt.results.FunctionResults  <code>dataclass</code>","text":"<p>               Bases: <code>Results</code></p> <p>Stores results related to function evaluations.</p> <p>The <code>FunctionResults</code> class extends the base <code>Results</code> class to store data specific to function evaluations. This includes:</p> <ol> <li> <p>Evaluations: The results of the function evaluations, including the    variable values, objective values, and constraint values for each    realization. See    <code>FunctionEvaluations</code>.</p> </li> <li> <p>Realizations: Information about the realizations, such as weights for    objectives and constraints, and whether each realization was successful.    See <code>Realizations</code>.</p> </li> <li> <p>Functions: The calculated objective and constraint function values,    typically aggregated across realizations. See    <code>Functions</code>.</p> </li> <li> <p>Constraint Info: Details about constraint differences and violations.    See <code>ConstraintInfo</code>.</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>evaluations</code> <code>FunctionEvaluations</code> <p>Results of the function evaluations.</p> <code>realizations</code> <code>Realizations</code> <p>The calculated parameters of the realizations.</p> <code>functions</code> <code>Functions | None</code> <p>The calculated functions.</p> <code>constraint_info</code> <code>ConstraintInfo | None</code> <p>Information on constraint differences and violations.</p>"},{"location":"reference/results/#ropt.results.GradientResults","title":"ropt.results.GradientResults  <code>dataclass</code>","text":"<p>               Bases: <code>Results</code></p> <p>Stores results related to gradient evaluations.</p> <p>The <code>GradientResults</code> class extends the base <code>Results</code> class to store data specific to gradient evaluations. This includes:</p> <ol> <li> <p>Evaluations: The results of the function evaluations for perturbed    variables, including the perturbed variable values, objective values, and    constraint values for each realization and perturbation. See    <code>GradientEvaluations</code>.</p> </li> <li> <p>Realizations: Information about the realizations, such as weights for    objectives and constraints, and whether each realization was successful.    See <code>Realizations</code>.</p> </li> <li> <p>Gradients: The calculated gradients of the objectives and constraints.    See <code>Gradients</code>.</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>evaluations</code> <code>GradientEvaluations</code> <p>Results of the function evaluations for perturbed           variables.</p> <code>realizations</code> <code>Realizations</code> <p>The calculated parameters of the realizations.</p> <code>gradients</code> <code>Gradients | None</code> <p>The calculated gradients.</p>"},{"location":"reference/results/#ropt.results.Functions","title":"ropt.results.Functions  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores the calculated objective and constraint function values.</p> <p>The <code>Functions</code> class stores the calculated values of the objective and constraint functions. These values are typically derived from the evaluations performed across all realizations, often through a process like averaging. The optimizer may handle multiple objectives and constraints. Multiple objectives are combined into a single weighted sum, which is stored in the <code>weighted_objective</code> field. Multiple constraints are handled individually by the optimizer.</p> <p>Result descriptions</p> Weighted ObjectiveObjectivesConstraints <p><code>weighted_objective</code>: The overall objective calculated as a weighted sum over the objectives. This is a single floating point values. It is defined as a <code>numpy</code> array of dimensions 0, hence it has no axes:</p> <ul> <li>Shape: \\(()\\)</li> <li>Axis type: <code>None</code></li> </ul> <p><code>objectives</code>: The calculated objective function values. This is a one-dimensional array of floating point values:</p> <ul> <li>Shape \\((n_o,)\\), where:<ul> <li>\\(n_o\\) is the number of objectives.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.OBJECTIVE</code></li> </ul> </li> </ul> <p><code>constraints</code>: The calculated constraint function values. This is a one-dimensional array of floating point values:</p> <ul> <li>Shape \\((n_c,)\\), where:<ul> <li>\\(n_c\\) is the number of constraints.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>weighted_objective</code> <code>NDArray[float64]</code> <p>The weighted sum of the objective values.</p> <code>objectives</code> <code>NDArray[float64]</code> <p>The value of each individual objective.</p> <code>constraints</code> <code>NDArray[float64] | None</code> <p>The value of each individual constraint.</p>"},{"location":"reference/results/#ropt.results.Gradients","title":"ropt.results.Gradients  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores the calculated objective and constraint gradients.</p> <p>The <code>Gradients</code> class stores the calculated gradients of the objective and constraint functions. These gradients are typically derived from function evaluations across all realizations, often through a process like averaging. The optimizer may handle multiple objectives and constraints. Multiple objective gradients are combined into a single weighted sum, which is stored in the <code>weighted_objective</code> field. Multiple constraint gradients are handled individually by the optimizer.</p> <p>Result descriptions</p> Weighted Objective GradientObjective  GradientsConstraint Gradients <p><code>weighted_objective</code>: The gradient of the weighted objective with respect to each variable:</p> <ul> <li>Shape: \\((n_v,)\\), where:<ul> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p><code>objectives</code>: The calculated gradients of each objective with respect to each variable. This is a two-dimensional array of floating point values:</p> <ul> <li>Shape \\((n_o, n_v)\\), where:<ul> <li>\\(n_o\\) is the number of objectives.</li> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.OBJECTIVE</code></li> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p><code>constraints</code>: The calculated gradients of each nonlinear constraint with respect to each variable. This is a two-dimensional array of floating point values:</p> <ul> <li>Shape \\((n_c, n_v)\\), where:<ul> <li>\\(n_c\\) is the number of constraints.</li> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>weighted_objective</code> <code>NDArray[float64]</code> <p>The weighted sum of the objective gradients.</p> <code>objectives</code> <code>NDArray[float64]</code> <p>The gradient of each individual objective.</p> <code>constraints</code> <code>NDArray[float64] | None</code> <p>The gradient of each individual constraint.</p>"},{"location":"reference/results/#ropt.results.FunctionEvaluations","title":"ropt.results.FunctionEvaluations  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores the results of function evaluations.</p> <p>The <code>FunctionEvaluations</code> class stores the results of evaluating the objective and constraint functions for a set of variables.</p> <p>Result descriptions</p> VariablesObjectivesConstraintsEvaluation Info <p><code>variables</code>: The vector of variable values at which the functions were evaluated:</p> <ul> <li>Shape: \\((n_v,)\\), where:<ul> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p><code>objectives</code>: The calculated objective function values for each realization. This is a two-dimensional array of floating point values where each row corresponds to a realization and each column corresponds to an objective:</p> <ul> <li>Shape \\((n_r, n_o)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_o\\) is the number of objectives.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.OBJECTIVE</code></li> </ul> </li> </ul> <p><code>constraints</code>: The calculated constraint function values for each realization. Only provided if non-linear constraints are defined. This is a two-dimensional array of floating point values where each row corresponds to a realization and each column corresponds to a constraint:</p> <ul> <li>Shape \\((n_r, n_c)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_c\\) is the number of constraints.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> </ul> </li> </ul> <p><code>evaluation_info</code>: Optional metadata associated with each realization, potentially provided by the evaluator. If provided, each value in the info dictionary must be a one-dimensional array of arbitrary type supported by <code>numpy</code> (including objects):</p> <ul> <li>Shape: \\((n_r,)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.REALIZATION</code></li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>variables</code> <code>NDArray[float64]</code> <p>The variable vector:</p> <code>objectives</code> <code>NDArray[float64]</code> <p>The objective function values for each realization.</p> <code>constraints</code> <code>NDArray[float64] | None</code> <p>The constraint function values for each realization.</p> <code>evaluation_info</code> <code>dict[str, NDArray[Any]]</code> <p>Optional metadata for each evaluated realization.</p>"},{"location":"reference/results/#ropt.results.GradientEvaluations","title":"ropt.results.GradientEvaluations  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores the results of evaluations for gradient calculations.</p> <p>The <code>GradientEvaluations</code> class stores the results of evaluating the objective and constraint functions for perturbed variables, which is necessary for gradient calculations.</p> <p>Result descriptions</p> VariablesPerturbed VariablesPerturbed ObjectivesPerturbed ConstraintsEvaluation Info <p><code>variables</code>: The vector of unperturbed variable values:</p> <ul> <li>Shape: \\((n_v,)\\), where:<ul> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p><code>perturbed_variables</code>: A three-dimensional array of perturbed variable values for each realization and perturbation:</p> <ul> <li>Shape: \\((n_r, n_p, n_v)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_p\\) is the number of perturbations.</li> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.PERTURBATION</code></li> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <p><code>perturbed_objectives</code>: A three-dimensional array of perturbed calculated objective function values for each realization and perturbation:</p> <ul> <li>Shape \\((n_r, n_p, n_o)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_p\\) is the number of perturbations.</li> <li>\\(n_o\\) is the number of objectives.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.PERTURBATION</code></li> <li><code>AxisName.OBJECTIVE</code></li> </ul> </li> </ul> <p><code>perturbed_constraints</code>: A three-dimensional array of perturbed calculated non-linear constraint values for each realization and perturbation:</p> <ul> <li>Shape \\((n_r, n_p, n_c)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_p\\) is the number of perturbations.</li> <li>\\(n_c\\) is the number of constraints.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.PERTURBATION</code></li> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> </ul> </li> </ul> <p><code>evaluation_info</code>: Optional metadata associated with each realization, potentially provided by the evaluator. If provided, each value in the info dictionary must be a two-dimensional array of arbitrary type supported by <code>numpy</code> (including objects):</p> <ul> <li>Shape: \\((n_r, n_p)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> <li>\\(n_p\\) is the number of perturbations.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.REALIZATION</code></li> <li><code>AxisName.PERTURBATION</code></li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>variables</code> <code>NDArray[float64]</code> <p>The unperturbed variable vector.</p> <code>perturbed_variables</code> <code>NDArray[float64]</code> <p>The perturbed variable values for each                    realization and perturbation.</p> <code>perturbed_objectives</code> <code>NDArray[float64]</code> <p>The objective function values for each                    realization and perturbation.</p> <code>perturbed_constraints</code> <code>NDArray[float64] | None</code> <p>The constraint function values for each                    realization and perturbation.</p> <code>evaluation_info</code> <code>dict[str, NDArray[Any]]</code> <p>Optional metadata for each evaluated                    realization and perturbation.</p>"},{"location":"reference/results/#ropt.results.Realizations","title":"ropt.results.Realizations  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores information about the realizations.</p> <p>The <code>Realizations</code> class stores data related to the individual realizations used in the optimization process.</p> <p>Result descriptions</p> Active RealizationsFailed RealizationsObjective WeightsConstraint Weights <p><code>active_realizations</code>: A boolean array indicating whether each realization's evaluation was evaluated. <code>True</code> indicates that a realization was evaluated:</p> <ul> <li>Shape \\((n_r,)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.REALIZATION</code></li> </ul> </li> </ul> <p><code>failed_realizations</code>: A boolean array indicating whether each realization's evaluation was successful. <code>True</code> indicates a failed realization, while <code>False</code> indicates a successful one:</p> <ul> <li>Shape \\((n_r,)\\), where:<ul> <li>\\(n_r\\) is the number of realizations.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.REALIZATION</code></li> </ul> </li> </ul> <p><code>objective_weights</code>: A two-dimensional array of weights used for each objective in each realization:</p> <ul> <li>Shape \\((n_o, n_r)\\), where:<ul> <li>\\(n_o\\) is the number of objectives.</li> <li>\\(n_r\\) is the number of realizations.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.OBJECTIVE</code></li> <li><code>AxisName.REALIZATION</code></li> </ul> </li> </ul> <p>These weights may change during optimization, depending on the type of objective calculation</p> <p><code>constraint_weights</code>: A two-dimensional array of weights used for each constraint in each realization:</p> <ul> <li>Shape \\((n_c, n_r)\\), where:<ul> <li>\\(n_c\\) is the number of constraints.</li> <li>\\(n_r\\) is the number of realizations.</li> </ul> </li> <li>Axis types:<ul> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> <li><code>AxisName.REALIZATION</code></li> </ul> </li> </ul> <p>These weights may change during optimization, depending on the type of constraint calculation</p> <p>Attributes:</p> Name Type Description <code>active_realizations</code> <code>NDArray[bool_]</code> <p>Boolean array indicating active realizations.</p> <code>failed_realizations</code> <code>NDArray[bool_]</code> <p>Boolean array indicating failed realizations.</p> <code>objective_weights</code> <code>NDArray[float64] | None</code> <p>Weights for each objective in each realization.</p> <code>constraint_weights</code> <code>NDArray[float64] | None</code> <p>Weights for each constraint in each realization.</p>"},{"location":"reference/results/#ropt.results.ConstraintInfo","title":"ropt.results.ConstraintInfo  <code>dataclass</code>","text":"<p>               Bases: <code>ResultField</code></p> <p>Stores information about constraint differences and violations.</p> <p>The <code>ConstraintInfo</code> class stores the differences between variable or constraint values and their respective bounds. It also calculates and stores constraint violations. This information is useful for assessing how well the optimization process is satisfying the imposed constraints.</p> <p>Constraint differences</p> <p>These represent the difference between a variable or constraint value and its corresponding bound. Whether this difference signifies a violation depends on the bound type:</p> <ul> <li>Lower Bounds: A negative difference means the value is below the lower   bound, thus violating the constraint.</li> <li>Upper Bounds: A positive difference means the value is above the upper   bound, thus violating the constraint.</li> </ul> <p>The class stores the following information on the differences:</p> <p>Constraint Violations</p> <p>Constraint violations are calculated based on the constraint differences. If a bound is violated, the violation value is the absolute value of the difference. If the bound is not violated, the violation value is zero.</p> <p>Result descriptions</p> <p>The class stores the following information for bound, linear constraint, and non-linear constraint differences and violations as one-dimensional vectors:</p> Bound ConstraintsLinear ConstraintsNonlinear Constraints <ul> <li>Differences: <code>bound_lower</code> and <code>bound_upper</code></li> <li>Violations: <code>bound_violation</code></li> <li>Shape: \\((n_v,)\\), where:<ul> <li>\\(n_v\\) is the number of variables.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.VARIABLE</code></li> </ul> </li> </ul> <ul> <li>Differences: <code>linear_lower</code> and <code>linear_upper</code></li> <li>Violations: <code>linear_violation</code></li> <li>Shape: \\((n_l,)\\), where:<ul> <li>\\(n_l\\) is the number of linear constraints.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.LINEAR_CONSTRAINT</code></li> </ul> </li> </ul> <ul> <li>Differences: <code>nonlinear_lower</code> and <code>nonlinear_upper</code></li> <li>Violations: <code>nonlinear_violation</code></li> <li>Shape: \\((n_c,)\\), where:<ul> <li>\\(n_c\\) is the number of non-linear constraints.</li> </ul> </li> <li>Axis type:<ul> <li><code>AxisName.NONLINEAR_CONSTRAINT</code></li> </ul> </li> </ul> <p>Attributes:</p> Name Type Description <code>bound_lower</code> <code>NDArray[float64] | None</code> <p>Difference between variables and their lower bounds.</p> <code>bound_upper</code> <code>NDArray[float64] | None</code> <p>Difference between variables and their upper bounds.</p> <code>linear_lower</code> <code>NDArray[float64] | None</code> <p>Difference between linear constraints and their lower                 bounds.</p> <code>linear_upper</code> <code>NDArray[float64] | None</code> <p>Difference between linear constraints and their upper                 bounds.</p> <code>nonlinear_lower</code> <code>NDArray[float64] | None</code> <p>Difference between nonlinear constraints and their                 lower bounds.</p> <code>nonlinear_upper</code> <code>NDArray[float64] | None</code> <p>Difference between nonlinear constraints and their                 upper bounds.</p> <code>bound_violation</code> <code>NDArray[float64] | None</code> <p>Magnitude of the violation of the variable bounds.</p> <code>linear_violation</code> <code>NDArray[float64] | None</code> <p>Magnitude of the violation of the linear constraints.</p> <code>nonlinear_violation</code> <code>NDArray[float64] | None</code> <p>Magnitude of the violation of the nonlinear constraints.</p>"},{"location":"reference/results/#ropt.results.results_to_dataframe","title":"ropt.results.results_to_dataframe","text":"<pre><code>results_to_dataframe(\n    results: Sequence[Results],\n    fields: set[str],\n    result_type: Literal[\"functions\", \"gradients\"],\n) -&gt; pd.DataFrame\n</code></pre> <p>Combine a sequence of results into a single pandas DataFrame.</p> <p>This function aggregates results from multiple <code>FunctionResults</code> or <code>GradientResults</code> objects into a single <code>pandas</code> DataFrame. It is designed to be used with observers that produce results during the optimization process.</p> <p>The <code>fields</code> argument determines which data fields to include in the DataFrame. These fields can be any of the attributes defined within <code>FunctionResults</code> or <code>GradientResults</code>. Nested fields are specified using dot notation (e.g., <code>evaluations.variables</code> to include the <code>variables</code> field within the <code>evaluations</code> object).</p> <p>The <code>evaluation_info</code> sub-fields, found within the <code>evaluations</code> fields of <code>functions</code> and <code>gradient</code> results, respectively, are dictionaries. To include specific keys from these dictionaries, use the format <code>evaluations.evaluation_info.key</code>, where <code>key</code> is the name of the desired key.</p> <p>Many fields may result in multiple columns in the DataFrame. For example, <code>evaluations.variables</code> will generate a separate column for each variable. If available, variable names will be used as column labels. Multi-dimensional fields, such as those with named realizations and objectives, will have column names that are tuples of the corresponding names.</p> <p>The <code>result_type</code> argument specifies whether to include function evaluation results (<code>functions</code>) or gradient results (<code>gradients</code>).</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Sequence[Results]</code> <p>A sequence of <code>Results</code> objects          to combine.</p> required <code>fields</code> <code>set[str]</code> <p>The names of the fields to include in the DataFrame.</p> required <code>result_type</code> <code>Literal['functions', 'gradients']</code> <p>The type of results to include (\"functions\" or          \"gradients\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas</code> DataFrame containing the combined results.</p>"},{"location":"reference/sampler_plugins/","title":"Samplers","text":""},{"location":"reference/sampler_plugins/#ropt.plugins.sampler","title":"ropt.plugins.sampler","text":"<p>Provides plugin functionality for adding sampler plugins.</p> <p>Samplers are used by the optimization process to generate perturbed variable vectors. This module allows for the extension of <code>ropt</code> with custom samplers.</p> <p>Core Concepts:</p> <ul> <li>Plugin Interface: Sampler plugins must inherit from the   <code>SamplerPlugin</code> base class.   This class acts as a factory, defining a <code>create</code> method to instantiate   sampler objects.</li> <li>Sampler Implementation: The actual sampling logic resides in classes that   inherit from the <code>Sampler</code> abstract base   class. These classes are initialized with the optimization configuration   (<code>EnOptConfig</code>), the index of the specific sampler   configuration to use (<code>sampler_index</code>), an optional variable mask (<code>mask</code>),   and a random number generator (<code>rng</code>). Samples are generated by calling the   sampler's <code>generate_samples</code> method.</li> <li>Discovery: The <code>PluginManager</code> discovers   available <code>SamplerPlugin</code> implementations (typically via entry points) and   uses them to create <code>Sampler</code> instances as needed.</li> </ul> <p>Built-in Sampler Plugins:</p> <p>By default, the <code>SciPySampler</code> sampler is installed, which provides several sampling methods based on the <code>scipy.stats</code> and <code>scipy.stats.qmc</code> packages.</p>"},{"location":"reference/sampler_plugins/#ropt.plugins.sampler.base.SamplerPlugin","title":"ropt.plugins.sampler.base.SamplerPlugin","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base Class for Sampler Plugins (Factories).</p> <p>This class defines the interface for plugins responsible for creating <code>Sampler</code> instances. These plugins act as factories for specific sampling algorithms or strategies.</p>"},{"location":"reference/sampler_plugins/#ropt.plugins.sampler.base.SamplerPlugin.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(\n    enopt_config: EnOptConfig,\n    sampler_index: int,\n    mask: NDArray[bool_] | None,\n    rng: Generator,\n) -&gt; Sampler\n</code></pre> <p>Factory method to create a concrete Sampler instance.</p> <p>This abstract class method serves as a factory for creating concrete <code>Sampler</code> objects. Plugin implementations must override this method to return an instance of their specific <code>Sampler</code> subclass.</p> <p>The <code>PluginManager</code> calls this method when an optimization requires samples generated by this plugin.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The main EnOpt configuration object.</p> required <code>sampler_index</code> <code>int</code> <p>Index into <code>enopt_config.samplers</code> for this sampler.</p> required <code>mask</code> <code>NDArray[bool_] | None</code> <p>Optional boolean mask for variable subset sampling.</p> required <code>rng</code> <code>Generator</code> <p>NumPy random number generator instance.</p> required <p>Returns:</p> Type Description <code>Sampler</code> <p>An initialized Sampler object ready for use.</p>"},{"location":"reference/sampler_plugins/#ropt.plugins.sampler.base.Sampler","title":"ropt.plugins.sampler.base.Sampler","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for Sampler Implementations.</p> <p>This class defines the fundamental interface for all concrete sampler implementations within the <code>ropt</code> framework. Sampler plugins provide classes derived from <code>Sampler</code> that encapsulate the logic of specific sampling algorithms or strategies used to generate perturbed variable vectors for the optimization process.</p> <p>Instances of <code>Sampler</code> subclasses are created by their corresponding <code>SamplerPlugin</code> factories. They are initialized with an <code>EnOptConfig</code> object detailing the optimization setup, the <code>sampler_index</code> identifying the specific sampler configuration to use from the config, an optional variable <code>mask</code> indicating which variables this sampler instance handles, and a NumPy random number generator (<code>rng</code>) for stochastic methods.</p> <p>The core functionality, generating samples, is performed by the <code>generate_samples</code> method, which must be implemented by subclasses.</p> <p>Subclasses must implement:</p> <ul> <li><code>__init__</code>: To accept the configuration, index, mask, and RNG.</li> <li><code>generate_samples</code>: To contain the sample generation logic.</li> </ul>"},{"location":"reference/sampler_plugins/#ropt.plugins.sampler.base.Sampler.__init__","title":"__init__","text":"<pre><code>__init__(\n    enopt_config: EnOptConfig,\n    sampler_index: int,\n    mask: NDArray[bool_] | None,\n    rng: Generator,\n) -&gt; None\n</code></pre> <p>Initialize the sampler object.</p> <p>The <code>samplers</code> field in the <code>enopt_config</code> is a tuple of sampler configurations (<code>SamplerConfig</code>). The <code>sampler_index</code> identifies which configuration from this tuple should be used to initialize this specific sampler instance.</p> <p>If a boolean <code>mask</code> array is provided, it indicates that this sampler instance is responsible for generating samples only for the subset of variables where the mask is <code>True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>enopt_config</code> <code>EnOptConfig</code> <p>The configuration of the optimizer.</p> required <code>sampler_index</code> <code>int</code> <p>The index of the sampler configuration to use.</p> required <code>mask</code> <code>NDArray[bool_] | None</code> <p>Optional mask indicating variables handled by this sampler.</p> required <code>rng</code> <code>Generator</code> <p>A random generator object for stochastic methods.</p> required"},{"location":"reference/sampler_plugins/#ropt.plugins.sampler.base.Sampler.generate_samples","title":"generate_samples  <code>abstractmethod</code>","text":"<pre><code>generate_samples() -&gt; NDArray[np.float64]\n</code></pre> <p>Generate and return an array of sampled perturbation values.</p> <p>This method must return a three-dimensional NumPy array containing the generated perturbation samples. The shape of the array should be <code>(n_realizations, n_perturbations, n_variables)</code>, where:</p> <ul> <li><code>n_realizations</code> is the number of realizations in the ensemble.</li> <li><code>n_perturbations</code> is the number of perturbations requested.</li> <li><code>n_variables</code> is the total number of optimization variables.</li> </ul> <p>If the <code>shared</code> flag is <code>True</code> in the associated <code>SamplerConfig</code>, the first dimension (realizations) should have a size of 1. The framework will broadcast these shared samples across all realizations.</p> <p>If a boolean <code>mask</code> was provided during initialization, this sampler instance is responsible only for a subset of variables (where the mask is <code>True</code>). The returned array must still have the full <code>n_variables</code> size along the last axis. However, values corresponding to variables not handled by this sampler (where the mask is <code>False</code>) must be zero.</p> Sample Scaling and Perturbation Magnitudes <p>The generated samples represent unscaled perturbations. During the gradient estimation process, these samples will be multiplied element-wise by the <code>perturbation_magnitudes</code> defined in the <code>GradientConfig</code>.</p> <p>Therefore, it is generally recommended that sampler implementations produce samples with a characteristic scale of approximately one (e.g., drawn from a distribution with a standard deviation of 1, or uniformly distributed within <code>[-1, 1]</code>). This allows the <code>perturbation_magnitudes</code> to directly control the effective size of the perturbations applied to the variables.</p> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>A 3D NumPy array of sampled perturbation values.</p>"},{"location":"reference/scipy_optimizer_plugin/","title":"SciPy Optimizers","text":""},{"location":"reference/scipy_optimizer_plugin/#ropt.plugins.optimizer.scipy.SciPyOptimizer","title":"ropt.plugins.optimizer.scipy.SciPyOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>SciPy optimization backend for ropt.</p> <p>This class provides an interface to several optimization algorithms from SciPy's <code>scipy.optimize</code> module, enabling their use within <code>ropt</code>.</p> <p>To select an optimizer, set the <code>method</code> field within the <code>optimizer</code> section of the <code>EnOptConfig</code> configuration object to the desired algorithm's name. Most methods support the general options defined in the <code>EnOptConfig</code> object. For algorithm-specific options, use the <code>options</code> dictionary within the <code>optimizer</code> section.</p> <p>The table below lists the included methods together with the method-specific options that are supported. Click on the method name to consult the corresponding <code>scipy.optimize</code> documentation:</p> Method Method Options Nelder-Mead disp, maxiter, maxfev, xatol, fatol, adaptive Powell disp, maxiter, maxfev, xtol, ftol CG disp, maxiter, gtol, norm, eps, finite_diff_rel_step, c1, c2 BFGS disp, maxiter, gtol, norm, eps, finite_diff_rel_step, xrtol, c1, c2 Newton-CG disp, maxiter, xtol, eps, c1, c2 L-BFGS-B disp, maxiter, maxcor, ftol, gtol, eps, maxfun, iprint, maxls, finite_diff_rel_step TNC disp, maxfun, eps, scale, offset, maxCGit, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, finite_diff_rel_step COBYLA disp, maxiter, rhobeg, tol, catol SLSQP disp, maxiter, ftol, eps, finite_diff_rel_step differential_evolution disp, maxiter, strategy, popsize, tol, mutation, recombination, rng, polish, init, atol, updating"},{"location":"reference/scipy_sampler_plugin/","title":"SciPy Samplers","text":""},{"location":"reference/scipy_sampler_plugin/#ropt.plugins.sampler.scipy.SciPySampler","title":"ropt.plugins.sampler.scipy.SciPySampler","text":"<p>               Bases: <code>Sampler</code></p> <p>A sampler implementation utilizing SciPy's statistical functions.</p> <p>This sampler leverages functions from the <code>scipy.stats</code> and <code>scipy.stats.qmc</code> modules to generate perturbation samples for optimization.</p> <p>Supported Sampling Methods:</p> <ul> <li> <p>From Probability Distributions   (<code>scipy.stats</code>):</p> <ul> <li><code>norm</code>: Samples from a standard normal distribution (mean 0,   standard deviation 1). This is the default method if none is   specified or if \"default\" is requested.</li> <li><code>truncnorm</code>: Samples from a truncated normal distribution (mean 0,   std dev 1), truncated to the range <code>[-1, 1]</code> by default.</li> <li><code>uniform</code>: Samples from a uniform distribution. Defaults to the   range <code>[-1, 1]</code>.</li> </ul> </li> <li> <p>From Quasi-Monte Carlo Sequences   (<code>scipy.stats.qmc</code>):</p> <ul> <li><code>sobol</code>: Uses Sobol' sequences.</li> <li><code>halton</code>: Uses Halton sequences.</li> <li><code>lhs</code>: Uses Latin Hypercube Sampling. (Note: QMC samples are generated in the unit hypercube <code>[0, 1]^d</code> and then scaled to the hypercube <code>[-1, 1]^d</code>)</li> </ul> </li> </ul> <p>Configuration:</p> <p>The specific sampling method is chosen via the <code>method</code> field in the <code>SamplerConfig</code>. Additional method-specific parameters (e.g., distribution parameters like <code>loc</code>, <code>scale</code>, <code>a</code>, <code>b</code> for <code>stats</code> methods, or engine parameters for <code>qmc</code> methods) can be passed through the <code>options</code> dictionary within the <code>SamplerConfig</code>. Refer to the <code>scipy.stats</code> and documentation for available options.</p>"},{"location":"reference/utilities/","title":"Utilities","text":""},{"location":"reference/utilities/#ropt.config.utils","title":"ropt.config.utils","text":"<p>Utilities for checking and converting configuration values.</p> <p>This module provides helper functions primarily designed for use within Pydantic model validation logic. These functions facilitate the conversion of configuration inputs into standardized, immutable NumPy arrays and handle common validation tasks like checking enum values or broadcasting arrays to required dimensions.</p>"},{"location":"reference/utilities/#ropt.config.utils.normalize","title":"normalize","text":"<pre><code>normalize(array: NDArray[float64]) -&gt; NDArray[np.float64]\n</code></pre> <p>Normalize a NumPy array so its elements sum to one.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray[float64]</code> <p>The input NumPy array (1D).</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>A new immutable NumPy array with the same shape as the input, where</p> <code>NDArray[float64]</code> <p>the elements have been scaled to sum to 1.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of the input array elements is not positive         (i.e., less than or equal to machine epsilon).</p>"},{"location":"reference/utilities/#ropt.config.utils.immutable_array","title":"immutable_array","text":"<pre><code>immutable_array(\n    array_like: ArrayLike, **kwargs: Any\n) -&gt; NDArray[Any]\n</code></pre> <p>Convert input to an immutable NumPy array.</p> <p>This function takes various array-like inputs (e.g., lists, tuples, other NumPy arrays) and converts them into a NumPy array. It then sets the <code>writeable</code> flag of the resulting array to <code>False</code>, making it immutable.</p> <p>Parameters:</p> Name Type Description Default <code>array_like</code> <code>ArrayLike</code> <p>The input data to convert (e.g., list, tuple, NumPy array).</p> required <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments passed directly to <code>numpy.array</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>A new NumPy array, with its <code>writeable</code> flag set to <code>False</code>.</p>"},{"location":"reference/utilities/#ropt.config.utils.broadcast_arrays","title":"broadcast_arrays","text":"<pre><code>broadcast_arrays(*args: Any) -&gt; tuple[NDArray[Any], ...]\n</code></pre> <p>Broadcast arrays to a common shape and make them immutable.</p> <p>This function takes multiple NumPy arrays (or array-like objects) and uses <code>numpy.broadcast_arrays</code> to make them conform to a common shape according to NumPy's broadcasting rules. Each resulting array is then made immutable by setting its <code>writeable</code> flag to <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Any</code> <p>A variable number of NumPy arrays or array-like objects.</p> <code>()</code> <p>Returns:</p> Type Description <code>tuple[NDArray[Any], ...]</code> <p>A tuple containing the broadcasted, immutable NumPy arrays.</p>"},{"location":"reference/utilities/#ropt.config.utils.broadcast_1d_array","title":"broadcast_1d_array","text":"<pre><code>broadcast_1d_array(\n    array: NDArray[Any], name: str, size: int\n) -&gt; NDArray[Any]\n</code></pre> <p>Broadcast an array to a 1D array of a specific size and make it immutable.</p> <p>This function takes an input array and attempts to broadcast it to a one-dimensional array of the specified <code>size</code> using NumPy's broadcasting rules. If successful, the resulting array is made immutable.</p> <p>This is useful for ensuring configuration parameters (like weights or magnitudes) have the correct dimension corresponding to the number of variables, objectives, etc., allowing users to provide a single scalar value that applies to all elements.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray[Any]</code> <p>The input NumPy array or array-like object.</p> required <code>name</code> <code>str</code> <p>A descriptive name for the array (used in error messages).</p> required <code>size</code> <code>int</code> <p>The target size (number of elements) for the 1D array.</p> required <p>Returns:</p> Type Description <code>NDArray[Any]</code> <p>A new, immutable 1D NumPy array of the specified <code>size</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input <code>array</code> cannot be broadcast to the target <code>size</code>.</p>"},{"location":"reference/utilities/#ropt.config.utils.check_enum_values","title":"check_enum_values","text":"<pre><code>check_enum_values(\n    value: NDArray[ubyte], enum_type: type[IntEnum]\n) -&gt; None\n</code></pre> <p>Check if enum values in a NumPy array are valid members of an IntEnum.</p> <p>This function verifies that all integer values within the input NumPy array correspond to valid members of the specified <code>IntEnum</code> type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>NDArray[ubyte]</code> <p>A NumPy array containing integer values (typically <code>np.ubyte</code>)        representing potential enum members.</p> required <code>enum_type</code> <code>type[IntEnum]</code> <p>The <code>IntEnum</code> class to validate against.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any value in the <code>value</code> array does not correspond to a         member of the <code>enum_type</code>.</p>"},{"location":"reference/utilities/#ropt.config.validated_types","title":"ropt.config.validated_types","text":"<p>Annotated types for Pydantic models providing input conversion and validation.</p> <p>These types leverage Pydantic's <code>BeforeValidator</code> to automatically convert input values (like lists or scalars) into standardized, immutable NumPy arrays or Python collections (sets, tuples) during model initialization.</p> <p>NumPy Array Types:</p> <ul> <li><code>Array1D</code>: Converts input to an   immutable 1D <code>np.float64</code> array.</li> <li><code>Array2D</code>: Converts input to an   immutable 2D <code>np.float64</code> array.</li> <li><code>ArrayEnum</code>: Converts input to an   immutable 1D <code>np.ubyte</code> array (suitable for integer enum values).</li> <li><code>Array1DInt</code>: Converts input to an   immutable 1D <code>np.intc</code> array.</li> <li><code>Array1DBool</code>: Converts input to an   immutable 1D <code>np.bool_</code> array.</li> </ul> <p>Collection Types:</p> <ul> <li><code>ItemOrSet[T]</code>: Ensures the value is a   <code>set[T]</code>, converting single items or sequences.</li> <li><code>ItemOrTuple[T]</code>: Ensures the value is a   <code>tuple[T, ...]</code>, converting single items or sequences.</li> </ul>"},{"location":"reference/utilities/#ropt.config.validated_types.Array1D","title":"Array1D  <code>module-attribute</code>","text":"<pre><code>Array1D = Annotated[\n    NDArray[float64], BeforeValidator(_convert_1d_array)\n]\n</code></pre> <p>Convert to an immutable 1D numpy array of floating point values.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.Array2D","title":"Array2D  <code>module-attribute</code>","text":"<pre><code>Array2D = Annotated[\n    NDArray[float64], BeforeValidator(_convert_2d_array)\n]\n</code></pre> <p>Convert to an immutable 2D numpy array of floating point values.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.ArrayEnum","title":"ArrayEnum  <code>module-attribute</code>","text":"<pre><code>ArrayEnum = Annotated[\n    NDArray[ubyte], BeforeValidator(_convert_enum_array)\n]\n</code></pre> <p>Convert to an immutable numpy array of numerical enumeration values.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.Array1DInt","title":"Array1DInt  <code>module-attribute</code>","text":"<pre><code>Array1DInt = Annotated[\n    NDArray[intc], BeforeValidator(_convert_1d_array_intc)\n]\n</code></pre> <p>Convert to an immutable 1D numpy array of integer values.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.Array1DBool","title":"Array1DBool  <code>module-attribute</code>","text":"<pre><code>Array1DBool = Annotated[\n    NDArray[bool_], BeforeValidator(_convert_1d_array_bool)\n]\n</code></pre> <p>Convert to an immutable 1D numpy array of boolean values.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.ItemOrSet","title":"ItemOrSet  <code>module-attribute</code>","text":"<pre><code>ItemOrSet = Annotated[set[T], BeforeValidator(_convert_set)]\n</code></pre> <p>Convert to single value to a set containing that value, passes sets unchanged.</p>"},{"location":"reference/utilities/#ropt.config.validated_types.ItemOrTuple","title":"ItemOrTuple  <code>module-attribute</code>","text":"<pre><code>ItemOrTuple = Annotated[\n    tuple[T, ...], BeforeValidator(_convert_tuple)\n]\n</code></pre> <p>Convert to single value to a tuple containing that value, passes sets unchanged.</p>"},{"location":"reference/workflow/","title":"Optimization Workflows","text":""},{"location":"reference/workflow/#ropt.workflow","title":"ropt.workflow","text":"<p>Optimization workflow functionality.</p> <p>The <code>ropt.workflow</code> package provides a powerful and flexible framework for constructing and executing optimization workflows. It is designed to handle both simple, single-run optimizations and more complex, customized scenarios. The framework is built upon three key components:</p> <ul> <li><code>ComputeStep</code>: Defines a   distinct action within the workflow, such as running an optimization algorithm.</li> <li><code>EventHandler</code>: Responds to   events emitted by <code>ComputeStep</code> instances. This allows for real-time   monitoring, storing results, or triggering custom logic during the workflow.</li> <li><code>Evaluator</code>: Provides a mechanism   for <code>ComputeStep</code> objects to perform function evaluations, such as running   simulations on a high-performance computing (HPC) cluster.</li> </ul> <p>Compute steps, event handlers, and evaluators are implemented using the <code>plugin</code> system. These objects are typically created via helper fuctions:</p> <ul> <li>create_compute_step: Create   compute steps.</li> <li>create_event_handler:   Create event handlers.</li> <li>create_evaluator: Create   evaluators.</li> </ul> <p>After creation, compute steps are executed by calling their <code>run</code> method. During execution, compute steps may emit <code>events</code> to communicate intermediate results. Event handlers can be added to compute steps using the <code>add_event_handler</code> method. Many compute steps, such as those performing optimizations, will require the repeated evaluation of a function, which is performed by evaluator objects passed to the step upon creation.</p> <p>The following example demonstrates how to construct a workflow from these components. It finds the optimum of the Rosenbrock function by combining an optimizer <code>ComputeStep</code> with a <code>tracker</code> to store the best result.</p> Example <pre><code>import numpy as np\nfrom numpy.typing import NDArray\n\nfrom ropt.config import EnOptConfig\nfrom ropt.evaluator import EvaluatorContext, EvaluatorResult\nfrom ropt.workflow import (\n    create_compute_step,\n    create_evaluator,\n    create_event_handler,\n)\n\nDIM = 5\nCONFIG = {\n    \"variables\": {\n        \"variable_count\": DIM,\n        \"perturbation_magnitudes\": 1e-6,\n    },\n}\ninitial_values = 2 * np.arange(DIM) / DIM + 0.5\n\n\ndef rosenbrock(variables: NDArray[np.float64], _: EvaluatorContext) -&gt; EvaluatorResult:\n    objectives = np.zeros((variables.shape[0], 1), dtype=np.float64)\n    for v_idx in range(variables.shape[0]):\n        for d_idx in range(DIM - 1):\n            x, y = variables[v_idx, d_idx : d_idx + 2]\n            objectives[v_idx, 0] += (1.0 - x) ** 2 + 100 * (y - x * x) ** 2\n    return EvaluatorResult(objectives=objectives)\n\n\nevaluator = create_evaluator(\"function_evaluator\", callback=rosenbrock)\nstep = create_compute_step(\"optimizer\", evaluator=evaluator)\ntracker = create_event_handler(\"tracker\")\nstep.add_event_handler(tracker)\nstep.run(variables=initial_values, config=EnOptConfig.model_validate(CONFIG))\n\nprint(f\"Optimal variables: {tracker['results'].evaluations.variables}\")\nprint(f\"Optimal objective: {tracker['results'].functions.weighted_objective}\")\n</code></pre> Note <p>This example demonstrates the manual construction of a workflow, which is ideal for complex, multi-step processes. For straightforward, single-run optimizations, the <code>BasicOptimizer</code> class offers a simpler, high-level interface.</p>"},{"location":"reference/workflow/#ropt.workflow.create_compute_step","title":"ropt.workflow.create_compute_step","text":"<pre><code>create_compute_step(\n    method: str, **kwargs: Any\n) -&gt; ComputeStep\n</code></pre> <p>Create a new compute step.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method string to find the compute step.</p> required <code>kwargs</code> <code>Any</code> <p>Optional keyword arguments passed to the compute step init.</p> <code>{}</code>"},{"location":"reference/workflow/#ropt.workflow.create_event_handler","title":"ropt.workflow.create_event_handler","text":"<pre><code>create_event_handler(\n    method: str, **kwargs: Any\n) -&gt; EventHandler\n</code></pre> <p>Create a new event handler.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method string to find the handler.</p> required <code>kwargs</code> <code>Any</code> <p>Optional keyword arguments passed to the handler init.</p> <code>{}</code>"},{"location":"reference/workflow/#ropt.workflow.create_evaluator","title":"ropt.workflow.create_evaluator","text":"<pre><code>create_evaluator(method: str, **kwargs: Any) -&gt; Evaluator\n</code></pre> <p>Create a new evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method string to find the evaluator.</p> required <code>kwargs</code> <code>Any</code> <p>Optional keyword arguments passed to the evaluator init.</p> <code>{}</code>"},{"location":"reference/workflow/#ropt.workflow.Event","title":"ropt.workflow.Event  <code>dataclass</code>","text":"<p>Stores data related to an optimization event.</p> <p>During the execution of an optimization workflow, events are triggered to signal specific occurrences. Callbacks can be registered to react to these events and will receive an <code>Event</code> object containing relevant information.</p> <p>The specific data within the <code>Event</code> object varies depending on the event type. See the <code>EventType</code> documentation for details.</p> <p>Attributes:</p> Name Type Description <code>event_type</code> <code>EventType</code> <p>The type of event that occurred.</p> <code>data</code> <code>dict[str, Any]</code> <p>A dictionary containing additional event-specific data.</p>"},{"location":"usage/background/","title":"Ensemble-based robust optimization","text":"<p>Constraint optimization is the process of optimizing an objective function \\(f(\\mathbf{x})\\) with respect to a vector of variables \\(\\mathbf{x}\\) in the presence of one or more inequality constraints \\(g_j(\\mathbf{x})\\) and/or equality constraints \\(h_k(\\mathbf{x})\\).</p> \\[ \\begin{align*} \\textrm{minimize} \\quad &amp; f(\\mathbf{x}) \\\\ \\textrm{subject to} \\quad &amp; g_j(\\mathbf{x}) \\le 0, \\quad \\quad j=1, \\ldots, J \\\\ &amp; h_k(\\mathbf{x}) = 0, \\quad \\quad k=1, \\ldots, K \\\\ &amp; \\mathbf{x}^L \\le \\mathbf{x} \\le \\mathbf{x}^U \\end{align*} \\] <p>In this context, the function \\(f(\\mathbf{x})\\) is assumed to have a deterministic nature, meaning it is well-defined for given parameters. However, in realistic scenarios, \\(f(\\mathbf{x})\\) may be part of a larger set of functions, especially if it depends on uncertain parameters drawn from some, possibly unknown, probability distribution.</p> <p>Ensemble-based robust optimization aims to optimize an ensemble of functions \\(f_i(\\mathbf{x})\\) with respect to \\(\\mathbf{x}\\). The set of realizations \\(f_i\\) captures the uncertainty that may exist in the model, which can be, for instance, constructed by varying some parameters according to a given probability distribution. When given a set of realizations, ensemble-based optimization proceeds by combining the functions \\(f_i(\\mathbf{x})\\) into a single objective function. For example, using a weighted sum, the problem becomes (ignoring constraints):</p> \\[ \\textrm{minimize} \\quad \\sum_i w_i f_i(\\mathbf{x}), \\] <p>where \\(w_i\\) represents the weights assigned to the different realizations. In more complex settings, the realizations may also be combined in different ways, and the set of realizations may be modified during optimization. For instance, risk-aware objectives may be constructed by minimizing the standard deviation of the functions or by selecting some of the worst-performing realizations at each iteration.</p> <p>In practice, the optimization task often becomes complex due to additional factors. The evaluation of functions might be computationally expensive, and calculating their gradients analytically can be challenging or even impossible. For example, the functions may involve lengthy simulations of a physical process with numerous variables, utilizing numerical calculations that preclude straightforward analytical differentiation.</p> <p><code>ropt</code> leverages standard optimization algorithms, such as those available in the SciPy package. These methods typically follow an iterative approach, necessitating repeated assessments of the objective function and, in many cases, its gradient. Currently, it is assumed that the functions are not easily differentiated analytically. One of the core functions of <code>ropt</code> is to calculate gradients efficiently using stochastic methods.</p> <p><code>ropt</code> is responsible for configuring and executing the optimization algorithm, building the overall function and gradient values from individual realizations, and monitoring both intermediate and final optimization results. It delegates the actual calculations of functions to external code that is provided by the user.</p> <p>While many optimization scenarios involve a single run of a particular method, there are cases where it proves beneficial to conduct multiple runs using the same or different algorithms. For example, when dealing with a mix of continuous and discrete variables, it might be advantageous to employ different methods for each variable type. <code>ropt</code> facilitates this by offering a mechanism to run a workflow containing multiple optimizers, potentially of different types, in an alternating or nested fashion.</p>"},{"location":"usage/basic/","title":"Running a basic optimization task","text":"<p>The <code>ropt</code> library provides a <code>BasicOptimizer</code> class that simplifies running an optimization task.</p> <p>This section walks you through an example of how to use <code>BasicOptimizer</code> to solve a simple optimization problem. We minimize the multi-dimensional Rosenbrock function, where we introduce some uncertainty in its parameters across an ensemble of realizations.</p>"},{"location":"usage/basic/#the-complete-example","title":"The complete example","text":"<p>Below is the full Python script for this example. We will go through each part of the script in the following sections.</p> <pre><code>\"\"\"Example of optimization of a multi-dimensional Rosenbrock test function.\n\nThis example demonstrates optimization of the a modified multi-dimensional\nRosenbrock function that exhibits uncertainty in its parameters. It shows how to\nwrite a minimal configuration and how to run and monitor the optimization.\n\"\"\"\n\nfrom functools import partial\nfrom typing import Any\n\nimport numpy as np\nfrom numpy.random import default_rng\nfrom numpy.typing import NDArray\n\nfrom ropt.evaluator import EvaluatorContext, EvaluatorResult\nfrom ropt.results import FunctionResults, Results\nfrom ropt.workflow import BasicOptimizer\n\nDIM = 5\nUNCERTAINTY = 0.1\nCONFIG: dict[str, Any] = {\n    \"variables\": {\n        \"variable_count\": DIM,\n        \"perturbation_magnitudes\": 1e-6,\n    },\n    \"realizations\": {\n        \"weights\": [1.0] * 10,\n    },\n    \"gradient\": {\n        \"number_of_perturbations\": 5,\n    },\n}\ninitial_values = 2 * np.arange(DIM) / DIM + 0.5\n\n\ndef rosenbrock(\n    variables: NDArray[np.float64],\n    context: EvaluatorContext,\n    a: NDArray[np.float64],\n    b: NDArray[np.float64],\n) -&gt; EvaluatorResult:\n    \"\"\"Function evaluator for the multi-dimensional rosenbrock function.\n\n    This function returns a tuple containing the calculated objectives and\n    `None`, the latter because no constraints are calculated.\n\n    Args:\n        variables: The variables to evaluate.\n        context:   Evaluator context.\n        a:         The 'a' parameters.\n        b:         The 'b' parameters.\n\n    Returns:\n        The calculated objective, and `None`\n    \"\"\"\n    objectives = np.zeros((variables.shape[0], 1), dtype=np.float64)\n    for v_idx, r in enumerate(context.realizations):\n        for d_idx in range(DIM - 1):\n            x, y = variables[v_idx, d_idx : d_idx + 2]\n            objectives[v_idx, 0] += (a[r] - x) ** 2 + b[r] * (y - x * x) ** 2\n    return EvaluatorResult(objectives=objectives)\n\n\ndef report(results: tuple[Results, ...]) -&gt; None:\n    \"\"\"Report results of an evaluation.\n\n    Args:\n        results: The results.\n    \"\"\"\n    for item in results:\n        if isinstance(item, FunctionResults) and item.functions is not None:\n            print(f\"  variables: {item.evaluations.variables}\")\n            print(f\"  objective: {item.functions.weighted_objective}\\n\")\n\n\ndef run_optimization(config: dict[str, Any]) -&gt; FunctionResults:\n    \"\"\"Run the optimization.\n\n    Args:\n        config: The configuration of the optimizer.\n\n    Returns:\n        The optimal results.\n    \"\"\"\n    rng = default_rng(seed=123)\n\n    realizations = len(config[\"realizations\"][\"weights\"])\n    a = rng.normal(loc=1.0, scale=UNCERTAINTY, size=realizations)\n    b = rng.normal(loc=100.0, scale=100 * UNCERTAINTY, size=realizations)\n\n    optimizer = BasicOptimizer(CONFIG, partial(rosenbrock, a=a, b=b))\n    optimizer.set_results_callback(report)\n    optimizer.run(initial_values)\n    assert optimizer.results is not None\n    assert optimizer.results.functions is not None\n\n    print(f\"Optimal variables: {optimizer.results.evaluations.variables}\")\n    print(f\"Optimal objective: {optimizer.results.functions.weighted_objective}\\n\")\n\n    return optimizer.results\n\n\ndef main() -&gt; None:\n    \"\"\"Run the example and check the result.\"\"\"\n    optimal_result = run_optimization(CONFIG)\n    assert optimal_result is not None\n    assert optimal_result.functions is not None\n    assert np.allclose(optimal_result.functions.weighted_objective, 0, atol=1e-1)\n    assert np.allclose(optimal_result.evaluations.variables, 1, atol=1e-1)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"usage/basic/#configuration","title":"Configuration","text":"<p>The <code>BasicOptimizer</code> requires a configuration dictionary that is parsed into an <code>EnOptConfing</code> object. Most of the optimization parameters are set to their defaults when parsing the dictionary, here we override only the most essential ones:</p> <ul> <li><code>variables</code>: Specifies details about the optimization variables.   <code>variable_count</code> is the number of variables, and <code>perturbation_magnitudes</code> is   used for generating the perturbations that are used for the gradient   calculations.</li> <li><code>realizations</code>: Defines the ensemble. <code>weights</code> is a list where each entry   corresponds to a realization. Here, we have 10 realizations with equal   weights.</li> <li><code>gradient</code>: Configures the stochastic gradient approximation.   <code>number_of_perturbations</code> controls how many perturbations are used to estimate   the gradient at each iteration.</li> </ul>"},{"location":"usage/basic/#the-objective-function","title":"The objective function","text":"<p>You must provide a Python function (<code>rosenbrock()</code> in our example) that <code>ropt</code> can call to evaluate your objective function for a given set of variables. The evaluator function receives the <code>variables</code> to be evaluated and an <code>EvaluatorContext</code> object. The context provides information such as which realizations to compute. The function must return an <code>EvaluatorResult</code> containing the calculated objective values.</p>"},{"location":"usage/basic/#running-the-optimization","title":"Running the optimization","text":"<p>The <code>run_optimization</code> function shows the main steps for setting up and running the optimizer:</p> <ol> <li> <p>Initialize uncertain parameters:     Before instantiating the optimizer, the <code>run_optimization</code> function     initializes a random number generator (<code>rng = default_rng(seed=123)</code>). This     generator is then used to create the uncertain parameters <code>a</code> and <code>b</code> for     the Rosenbrock function, simulating variability across realizations.</p> </li> <li> <p>Instantiate <code>BasicOptimizer</code>:     We create an instance of <code>BasicOptimizer</code>, passing the configuration     dictionary and the objective function. We use <code>functools.partial</code> to pass     the uncertain parameters <code>a</code> and <code>b</code> to our <code>rosenbrock</code> function, ensuring     they are available during objective function evaluations.</p> </li> <li> <p>Set a results callback:     The <code>set_results_callback</code> method allows you to register a function that     will be called after each optimization iteration with the current results.     In this example, the <code>report</code> function is used to print the current     variables and the weighted objective value, providing real-time feedback on     the optimization progress.</p> </li> <li> <p>Run the optimizer:     The <code>run</code> method starts the optimization process, beginning from the     <code>initial_values</code>. <code>ropt</code> will now iteratively call your objective function     to find the optimal variable values that minimize the weighted average of     the objective across all realizations.</p> </li> <li> <p>Retrieve results:     After the optimization completes, the final results are accessible via the     <code>optimizer.results</code> attribute. The optimal variables and the corresponding     weighted objective are printed to the console, and finally function then     returns the optimal results,  an instance of     <code>FunctionResults</code>.</p> </li> </ol>"}]}